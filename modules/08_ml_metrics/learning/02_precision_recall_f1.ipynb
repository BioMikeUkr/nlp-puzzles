{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision, Recall, and F1-Score\n",
    "\n",
    "In this notebook, you'll learn:\n",
    "- How to calculate precision, recall, and F1-score from confusion matrix\n",
    "- The trade-off between precision and recall\n",
    "- When to optimize for precision vs recall\n",
    "- How to find the optimal threshold for your use case\n",
    "- Using sklearn's classification_report\n",
    "\n",
    "## The Problem with Accuracy\n",
    "\n",
    "On imbalanced datasets, accuracy can be misleading. We need metrics that tell us:\n",
    "1. **Precision**: When we predict positive, how often are we correct?\n",
    "2. **Recall**: Of all actual positives, how many did we catch?\n",
    "3. **F1-Score**: A balanced metric combining precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score, f1_score,\n",
    "    accuracy_score, classification_report\n",
    ")\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('default')\n",
    "sns.set_palette('colorblind')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Calculate Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classification data\n",
    "df = pd.read_csv('../../fixtures/input/classification_data.csv')\n",
    "\n",
    "# Extract labels\n",
    "y_true = df['true_label'].values\n",
    "y_pred = df['predicted_label'].values\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(f\"TN: {tn}, FP: {fp}\")\n",
    "print(f\"FN: {fn}, TP: {tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision: Quality of Positive Predictions\n",
    "\n",
    "**Precision** measures: \"Of all samples we predicted as positive, how many are actually positive?\"\n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "**Use cases where precision is critical:**\n",
    "- Spam detection (don't want to mark legitimate emails as spam)\n",
    "- Medical diagnosis when false alarms are costly\n",
    "- Recommendation systems (don't want to recommend irrelevant items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision manually\n",
    "precision_manual = tp / (tp + fp)\n",
    "print(f\"Precision (manual): {precision_manual:.4f}\")\n",
    "\n",
    "# Calculate precision with sklearn\n",
    "precision_sklearn = precision_score(y_true, y_pred)\n",
    "print(f\"Precision (sklearn): {precision_sklearn:.4f}\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"When our model predicts class 1, it's correct {precision_manual*100:.2f}% of the time\")\n",
    "print(f\"Out of {tp + fp} positive predictions, {tp} were correct and {fp} were wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall (Sensitivity): Coverage of Actual Positives\n",
    "\n",
    "**Recall** measures: \"Of all actual positive samples, how many did we correctly identify?\"\n",
    "\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "**Use cases where recall is critical:**\n",
    "- Disease detection (don't want to miss sick patients)\n",
    "- Fraud detection (want to catch all fraud cases)\n",
    "- Search engines (want to find all relevant documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate recall manually\n",
    "recall_manual = tp / (tp + fn)\n",
    "print(f\"Recall (manual): {recall_manual:.4f}\")\n",
    "\n",
    "# Calculate recall with sklearn\n",
    "recall_sklearn = recall_score(y_true, y_pred)\n",
    "print(f\"Recall (sklearn): {recall_sklearn:.4f}\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"Our model found {recall_manual*100:.2f}% of all actual class 1 samples\")\n",
    "print(f\"Out of {tp + fn} actual positives, we correctly identified {tp} and missed {fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1-Score: Harmonic Mean of Precision and Recall\n",
    "\n",
    "**F1-Score** is the harmonic mean of precision and recall, giving a single score that balances both:\n",
    "\n",
    "$$F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} = \\frac{2 \\times TP}{2 \\times TP + FP + FN}$$\n",
    "\n",
    "**Why harmonic mean?** It penalizes extreme values. If either precision or recall is low, F1 will be low.\n",
    "\n",
    "**When to use F1:**\n",
    "- When you need a balance between precision and recall\n",
    "- When you have imbalanced classes\n",
    "- When false positives and false negatives have similar costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 manually (from precision and recall)\n",
    "f1_from_pr = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual)\n",
    "print(f\"F1 (from P and R): {f1_from_pr:.4f}\")\n",
    "\n",
    "# Calculate F1 manually (from confusion matrix)\n",
    "f1_from_cm = (2 * tp) / (2 * tp + fp + fn)\n",
    "print(f\"F1 (from confusion matrix): {f1_from_cm:.4f}\")\n",
    "\n",
    "# Calculate F1 with sklearn\n",
    "f1_sklearn = f1_score(y_true, y_pred)\n",
    "print(f\"F1 (sklearn): {f1_sklearn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing All Metrics Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# Create bar plot\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "values = [accuracy, precision, recall, f1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(metrics, values, color=['skyblue', 'lightgreen', 'lightcoral', 'gold'])\n",
    "plt.ylim(0, 1.0)\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.title('Model Performance Metrics', fontsize=14, pad=20)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{value:.3f}',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMetric Values:\")\n",
    "for metric, value in zip(metrics, values):\n",
    "    print(f\"{metric:12s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Precision-Recall Trade-off\n",
    "\n",
    "There's usually a trade-off between precision and recall:\n",
    "- **Increase threshold** → Higher precision, Lower recall (fewer but more confident predictions)\n",
    "- **Decrease threshold** → Lower precision, Higher recall (more predictions but less confident)\n",
    "\n",
    "Let's demonstrate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different thresholds\n",
    "thresholds = np.arange(0.1, 1.0, 0.05)\n",
    "y_prob = df['predicted_probability'].values\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresh = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics (handle edge cases where precision/recall might be undefined)\n",
    "    try:\n",
    "        prec = precision_score(y_true, y_pred_thresh, zero_division=0)\n",
    "        rec = recall_score(y_true, y_pred_thresh, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred_thresh, zero_division=0)\n",
    "    except:\n",
    "        prec = 0\n",
    "        rec = 0\n",
    "        f1 = 0\n",
    "    \n",
    "    precisions.append(prec)\n",
    "    recalls.append(rec)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Plot precision-recall trade-off\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Precision and Recall vs Threshold\n",
    "axes[0].plot(thresholds, precisions, 'g-', label='Precision', linewidth=2)\n",
    "axes[0].plot(thresholds, recalls, 'r-', label='Recall', linewidth=2)\n",
    "axes[0].plot(thresholds, f1_scores, 'b--', label='F1-Score', linewidth=2)\n",
    "axes[0].set_xlabel('Threshold', fontsize=12)\n",
    "axes[0].set_ylabel('Score', fontsize=12)\n",
    "axes[0].set_title('Precision-Recall Trade-off', fontsize=14)\n",
    "axes[0].legend(loc='best')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Precision vs Recall curve\n",
    "axes[1].plot(recalls, precisions, 'b-', linewidth=2)\n",
    "axes[1].set_xlabel('Recall', fontsize=12)\n",
    "axes[1].set_ylabel('Precision', fontsize=12)\n",
    "axes[1].set_title('Precision vs Recall', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Mark some key points\n",
    "for i, thresh in enumerate([0.3, 0.5, 0.7]):\n",
    "    idx = np.argmin(np.abs(thresholds - thresh))\n",
    "    axes[1].plot(recalls[idx], precisions[idx], 'ro', markersize=8)\n",
    "    axes[1].annotate(f'T={thresh}', (recalls[idx], precisions[idx]),\n",
    "                    xytext=(10, 10), textcoords='offset points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Optimal Threshold\n",
    "\n",
    "The \"optimal\" threshold depends on your use case:\n",
    "- **Maximize F1**: Balance between precision and recall\n",
    "- **Maximize Precision**: When false positives are costly\n",
    "- **Maximize Recall**: When false negatives are costly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find threshold that maximizes F1-score\n",
    "best_f1_idx = np.argmax(f1_scores)\n",
    "best_f1_threshold = thresholds[best_f1_idx]\n",
    "best_f1 = f1_scores[best_f1_idx]\n",
    "\n",
    "print(f\"Threshold that maximizes F1: {best_f1_threshold:.3f}\")\n",
    "print(f\"  Precision: {precisions[best_f1_idx]:.4f}\")\n",
    "print(f\"  Recall: {recalls[best_f1_idx]:.4f}\")\n",
    "print(f\"  F1-Score: {best_f1:.4f}\")\n",
    "print()\n",
    "\n",
    "# Find threshold for high precision (>0.90)\n",
    "high_prec_indices = [i for i, p in enumerate(precisions) if p >= 0.90]\n",
    "if high_prec_indices:\n",
    "    best_high_prec_idx = high_prec_indices[np.argmax([recalls[i] for i in high_prec_indices])]\n",
    "    print(f\"Threshold for precision >= 0.90 (with max recall):\")\n",
    "    print(f\"  Threshold: {thresholds[best_high_prec_idx]:.3f}\")\n",
    "    print(f\"  Precision: {precisions[best_high_prec_idx]:.4f}\")\n",
    "    print(f\"  Recall: {recalls[best_high_prec_idx]:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Find threshold for high recall (>0.90)\n",
    "high_rec_indices = [i for i, r in enumerate(recalls) if r >= 0.90]\n",
    "if high_rec_indices:\n",
    "    best_high_rec_idx = high_rec_indices[np.argmax([precisions[i] for i in high_rec_indices])]\n",
    "    print(f\"Threshold for recall >= 0.90 (with max precision):\")\n",
    "    print(f\"  Threshold: {thresholds[best_high_rec_idx]:.3f}\")\n",
    "    print(f\"  Precision: {precisions[best_high_rec_idx]:.4f}\")\n",
    "    print(f\"  Recall: {recalls[best_high_rec_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sklearn's classification_report\n",
    "\n",
    "sklearn provides a convenient function to calculate all metrics at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "report = classification_report(y_true, y_pred, target_names=['Class 0', 'Class 1'])\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Get report as dictionary for easier access\n",
    "report_dict = classification_report(y_true, y_pred, target_names=['Class 0', 'Class 1'], output_dict=True)\n",
    "\n",
    "# Visualize per-class metrics\n",
    "classes = ['Class 0', 'Class 1']\n",
    "metrics_names = ['precision', 'recall', 'f1-score']\n",
    "\n",
    "class_metrics = np.array([\n",
    "    [report_dict['Class 0']['precision'], report_dict['Class 0']['recall'], report_dict['Class 0']['f1-score']],\n",
    "    [report_dict['Class 1']['precision'], report_dict['Class 1']['recall'], report_dict['Class 1']['f1-score']]\n",
    "])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, class_metrics[0], width, label='Class 0', color='skyblue')\n",
    "bars2 = ax.bar(x + width/2, class_metrics[1], width, label='Class 1', color='lightcoral')\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Per-Class Performance Metrics', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics_names)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "def autolabel(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                   xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                   xytext=(0, 3),\n",
    "                   textcoords=\"offset points\",\n",
    "                   ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "autolabel(bars1)\n",
    "autolabel(bars2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Support and Weighted Averages\n",
    "\n",
    "The classification report includes:\n",
    "- **Support**: Number of samples in each class\n",
    "- **Macro avg**: Simple average across classes (treats all classes equally)\n",
    "- **Weighted avg**: Average weighted by support (accounts for class imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract averaging methods\n",
    "macro_avg = report_dict['macro avg']\n",
    "weighted_avg = report_dict['weighted avg']\n",
    "\n",
    "print(\"Averaging Methods:\")\n",
    "print(f\"\\nMacro Average (treats all classes equally):\")\n",
    "print(f\"  Precision: {macro_avg['precision']:.4f}\")\n",
    "print(f\"  Recall: {macro_avg['recall']:.4f}\")\n",
    "print(f\"  F1-Score: {macro_avg['f1-score']:.4f}\")\n",
    "\n",
    "print(f\"\\nWeighted Average (accounts for class imbalance):\")\n",
    "print(f\"  Precision: {weighted_avg['precision']:.4f}\")\n",
    "print(f\"  Recall: {weighted_avg['recall']:.4f}\")\n",
    "print(f\"  F1-Score: {weighted_avg['f1-score']:.4f}\")\n",
    "\n",
    "print(f\"\\nClass supports:\")\n",
    "print(f\"  Class 0: {report_dict['Class 0']['support']} samples\")\n",
    "print(f\"  Class 1: {report_dict['Class 1']['support']} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Calculate Metrics for Different Thresholds\n",
    "\n",
    "Calculate precision, recall, and F1 for threshold=0.6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Create predictions with threshold 0.6\n",
    "# 2. Calculate confusion matrix\n",
    "# 3. Calculate precision, recall, F1 manually\n",
    "# 4. Compare with sklearn results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Optimize for Your Use Case\n",
    "\n",
    "Imagine this is a medical test. Which threshold would you choose and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Consider: What's worse - missing a sick patient (FN) or false alarm (FP)?\n",
    "# Find an appropriate threshold and justify your choice\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Compare with Baseline\n",
    "\n",
    "Calculate precision, recall, and F1 for a naive baseline (always predict class 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Create a naive predictor (all 0s)\n",
    "# 2. Calculate its metrics\n",
    "# 3. Compare with your model\n",
    "# What happens to recall? Why?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Precision**: Quality of positive predictions (TP / (TP + FP))\n",
    "2. **Recall**: Coverage of actual positives (TP / (TP + FN))\n",
    "3. **F1-Score**: Harmonic mean of precision and recall\n",
    "4. **Precision-Recall Trade-off**: Adjusting threshold affects both metrics\n",
    "5. **Threshold Optimization**: Choose based on your use case\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- On imbalanced data, precision/recall/F1 are more informative than accuracy\n",
    "- There's always a trade-off between precision and recall\n",
    "- The \"optimal\" threshold depends on the relative costs of FP vs FN\n",
    "- F1-score provides a single metric that balances both\n",
    "- Use classification_report for a comprehensive view\n",
    "\n",
    "### Decision Guide:\n",
    "\n",
    "- **Optimize for Precision**: When false positives are costly (spam detection, recommendations)\n",
    "- **Optimize for Recall**: When false negatives are costly (disease detection, fraud)\n",
    "- **Optimize for F1**: When you need balance or costs are similar\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "In the next notebook, we'll explore:\n",
    "- ROC curves and ROC-AUC\n",
    "- Precision-Recall curves and PR-AUC\n",
    "- When to use which curve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
