{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix Basics\n",
    "\n",
    "In this notebook, you'll learn:\n",
    "- What a confusion matrix is and why it's important\n",
    "- How to calculate a confusion matrix manually and with sklearn\n",
    "- How to visualize confusion matrices\n",
    "- How to extract True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN)\n",
    "\n",
    "## What is a Confusion Matrix?\n",
    "\n",
    "A **confusion matrix** is a table that summarizes the performance of a classification model. It shows the counts of:\n",
    "- **True Positives (TP)**: Correctly predicted positive cases\n",
    "- **True Negatives (TN)**: Correctly predicted negative cases\n",
    "- **False Positives (FP)**: Incorrectly predicted as positive (Type I error)\n",
    "- **False Negatives (FN)**: Incorrectly predicted as negative (Type II error)\n",
    "\n",
    "```\n",
    "                    Predicted\n",
    "                 Negative  Positive\n",
    "Actual Negative     TN       FP\n",
    "       Positive     FN       TP\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('default')\n",
    "sns.set_palette('colorblind')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "We'll use our binary classification dataset with 1000 samples. This dataset is imbalanced (80% class 0, 20% class 1), which is common in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classification data\n",
    "df = pd.read_csv('../../fixtures/input/classification_data.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the data distribution\n",
    "print(\"Class distribution (true labels):\")\n",
    "print(df['true_label'].value_counts())\n",
    "print(f\"\\nClass 0: {(df['true_label'] == 0).sum() / len(df) * 100:.1f}%\")\n",
    "print(f\"Class 1: {(df['true_label'] == 1).sum() / len(df) * 100:.1f}%\")\n",
    "\n",
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "df['true_label'].value_counts().plot(kind='bar', ax=axes[0], color=['skyblue', 'coral'])\n",
    "axes[0].set_title('True Label Distribution')\n",
    "axes[0].set_xlabel('Class')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(['Class 0', 'Class 1'], rotation=0)\n",
    "\n",
    "df['predicted_label'].value_counts().plot(kind='bar', ax=axes[1], color=['skyblue', 'coral'])\n",
    "axes[1].set_title('Predicted Label Distribution')\n",
    "axes[1].set_xlabel('Class')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_xticklabels(['Class 0', 'Class 1'], rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Confusion Matrix Manually\n",
    "\n",
    "Let's calculate the confusion matrix components manually to understand what's happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract true and predicted labels\n",
    "y_true = df['true_label'].values\n",
    "y_pred = df['predicted_label'].values\n",
    "\n",
    "# Calculate confusion matrix components manually\n",
    "tp = ((y_true == 1) & (y_pred == 1)).sum()\n",
    "tn = ((y_true == 0) & (y_pred == 0)).sum()\n",
    "fp = ((y_true == 0) & (y_pred == 1)).sum()\n",
    "fn = ((y_true == 1) & (y_pred == 0)).sum()\n",
    "\n",
    "print(\"Confusion Matrix Components:\")\n",
    "print(f\"True Positives (TP):  {tp}\")\n",
    "print(f\"True Negatives (TN):  {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "print(f\"\\nTotal: {tp + tn + fp + fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "\n",
    "- **TP (True Positives)**: Number of class 1 samples correctly classified as class 1\n",
    "- **TN (True Negatives)**: Number of class 0 samples correctly classified as class 0\n",
    "- **FP (False Positives)**: Number of class 0 samples incorrectly classified as class 1 (Type I error)\n",
    "- **FN (False Negatives)**: Number of class 1 samples incorrectly classified as class 0 (Type II error)\n",
    "\n",
    "**Clinical Example**: In disease detection:\n",
    "- TP: Sick patients correctly diagnosed as sick\n",
    "- TN: Healthy patients correctly diagnosed as healthy\n",
    "- FP: Healthy patients incorrectly diagnosed as sick (false alarm)\n",
    "- FN: Sick patients incorrectly diagnosed as healthy (missed diagnosis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Confusion Matrix with sklearn\n",
    "\n",
    "sklearn provides a convenient function to calculate the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix using sklearn\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nLayout:\")\n",
    "print(\"[[TN, FP],\")\n",
    "print(\" [FN, TP]]\")\n",
    "\n",
    "# Verify it matches our manual calculation\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"TN from sklearn: {cm[0, 0]} vs manual: {tn}\")\n",
    "print(f\"FP from sklearn: {cm[0, 1]} vs manual: {fp}\")\n",
    "print(f\"FN from sklearn: {cm[1, 0]} vs manual: {fn}\")\n",
    "print(f\"TP from sklearn: {cm[1, 1]} vs manual: {tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Confusion Matrix\n",
    "\n",
    "Let's create several visualizations to understand the confusion matrix better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Using sklearn's ConfusionMatrixDisplay\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Display with counts\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])\n",
    "disp.plot(ax=axes[0], cmap='Blues', values_format='d')\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14)\n",
    "\n",
    "# Display with percentages (normalized)\n",
    "cm_normalized = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "disp_norm = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=['Class 0', 'Class 1'])\n",
    "disp_norm.plot(ax=axes[1], cmap='Blues', values_format='.2%')\n",
    "axes[1].set_title('Confusion Matrix (Percentages)', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using seaborn heatmap (more customizable)\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create annotations with both counts and percentages\n",
    "group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in cm.flatten() / np.sum(cm)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names, group_counts, group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2, 2)\n",
    "\n",
    "sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', square=True, linewidths=2,\n",
    "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "            yticklabels=['Actual 0', 'Actual 1'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Detailed Confusion Matrix', fontsize=16, pad=20)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Results\n",
    "\n",
    "Let's analyze what the confusion matrix tells us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate basic statistics from confusion matrix\n",
    "total_samples = tp + tn + fp + fn\n",
    "correct_predictions = tp + tn\n",
    "incorrect_predictions = fp + fn\n",
    "\n",
    "print(\"Overall Statistics:\")\n",
    "print(f\"Total samples: {total_samples}\")\n",
    "print(f\"Correct predictions: {correct_predictions} ({correct_predictions/total_samples*100:.2f}%)\")\n",
    "print(f\"Incorrect predictions: {incorrect_predictions} ({incorrect_predictions/total_samples*100:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# Performance by class\n",
    "class_0_correct = tn\n",
    "class_0_total = tn + fp\n",
    "class_1_correct = tp\n",
    "class_1_total = tp + fn\n",
    "\n",
    "print(\"Per-Class Performance:\")\n",
    "print(f\"Class 0 - Correct: {class_0_correct}/{class_0_total} ({class_0_correct/class_0_total*100:.2f}%)\")\n",
    "print(f\"Class 1 - Correct: {class_1_correct}/{class_1_total} ({class_1_correct/class_1_total*100:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# Error analysis\n",
    "print(\"Error Analysis:\")\n",
    "print(f\"False Positive Rate: {fp}/{tn+fp} = {fp/(tn+fp)*100:.2f}%\")\n",
    "print(f\"False Negative Rate: {fn}/{tp+fn} = {fn/(tp+fn)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights from Imbalanced Data\n",
    "\n",
    "Notice how the confusion matrix reveals important information that overall accuracy might hide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall accuracy\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(f\"Overall Accuracy: {accuracy*100:.2f}%\")\n",
    "print()\n",
    "\n",
    "# What if we always predicted class 0?\n",
    "naive_accuracy = (df['true_label'] == 0).sum() / len(df)\n",
    "print(f\"Naive baseline (always predict class 0): {naive_accuracy*100:.2f}%\")\n",
    "print()\n",
    "\n",
    "print(\"Key Observation:\")\n",
    "print(f\"Our model achieves {accuracy*100:.2f}% accuracy\")\n",
    "print(f\"But a naive model (always predict 0) would get {naive_accuracy*100:.2f}%!\")\n",
    "print(f\"The improvement is only {(accuracy - naive_accuracy)*100:.2f}%\")\n",
    "print()\n",
    "print(\"This is why we need more sophisticated metrics like precision, recall, and F1-score!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Calculate Confusion Matrix for a Subset\n",
    "\n",
    "Select the first 100 samples and calculate the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Extract first 100 samples\n",
    "# 2. Calculate confusion matrix\n",
    "# 3. Extract TP, TN, FP, FN\n",
    "# 4. Print the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Visualize Confusion Matrix\n",
    "\n",
    "Create a custom heatmap visualization for the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create a seaborn heatmap with custom colors and annotations\n",
    "# Include percentages in the annotations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Compare Multiple Thresholds\n",
    "\n",
    "Create confusion matrices for different probability thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Try thresholds: 0.3, 0.5, 0.7\n",
    "# 2. For each threshold, create new predictions from predicted_probability\n",
    "# 3. Calculate and visualize confusion matrix for each\n",
    "# 4. Compare the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **What a confusion matrix is**: A table showing TP, TN, FP, FN counts\n",
    "2. **How to calculate it**: Both manually and using sklearn\n",
    "3. **How to visualize it**: Using sklearn's built-in tools and seaborn\n",
    "4. **Why it's important**: It reveals model performance on each class, especially important for imbalanced datasets\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- The confusion matrix is the foundation for all classification metrics\n",
    "- On imbalanced datasets, overall accuracy can be misleading\n",
    "- Different types of errors (FP vs FN) may have different costs in real applications\n",
    "- Visualizing the confusion matrix helps quickly identify model weaknesses\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "In the next notebook, we'll learn how to derive important metrics from the confusion matrix:\n",
    "- Precision: How many predicted positives are actually positive?\n",
    "- Recall: How many actual positives did we catch?\n",
    "- F1-score: The harmonic mean of precision and recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
