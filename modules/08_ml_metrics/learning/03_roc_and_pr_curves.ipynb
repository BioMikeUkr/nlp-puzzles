{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC and Precision-Recall Curves\n",
    "\n",
    "In this notebook, you'll learn:\n",
    "- How to plot and interpret ROC (Receiver Operating Characteristic) curves\n",
    "- How to calculate ROC-AUC (Area Under the Curve)\n",
    "- How to plot and interpret Precision-Recall curves\n",
    "- When to use ROC-AUC vs PR-AUC\n",
    "- How to find the optimal threshold from these curves\n",
    "\n",
    "## Why Curves?\n",
    "\n",
    "So far we've looked at metrics for a single threshold. But curves show us:\n",
    "- Model performance across **all possible thresholds**\n",
    "- The trade-offs between different metrics\n",
    "- A single summary metric (AUC) independent of threshold choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, roc_auc_score, auc,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    RocCurveDisplay, PrecisionRecallDisplay\n",
    ")\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('default')\n",
    "sns.set_palette('colorblind')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classification data\n",
    "df = pd.read_csv('../../fixtures/input/classification_data.csv')\n",
    "\n",
    "# Extract labels and probabilities\n",
    "y_true = df['true_label'].values\n",
    "y_pred = df['predicted_label'].values\n",
    "y_prob = df['predicted_probability'].values  # Probability of class 1\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_true)}\")\n",
    "print(f\"Probability range: [{y_prob.min():.3f}, {y_prob.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve (Receiver Operating Characteristic)\n",
    "\n",
    "The **ROC curve** plots:\n",
    "- **X-axis**: False Positive Rate (FPR) = FP / (FP + TN)\n",
    "- **Y-axis**: True Positive Rate (TPR) = TP / (TP + FN) = **Recall**\n",
    "\n",
    "Each point on the curve represents a different threshold.\n",
    "\n",
    "### Interpretation:\n",
    "- **Top-left corner** (0, 1): Perfect classifier\n",
    "- **Diagonal line**: Random classifier (no better than guessing)\n",
    "- **Below diagonal**: Worse than random (predictions are inverted)\n",
    "- **Higher curve**: Better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_true, y_prob)\n",
    "\n",
    "# Calculate ROC-AUC\n",
    "roc_auc = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"Number of thresholds evaluated: {len(roc_thresholds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "\n",
    "# Plot diagonal (random classifier)\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random classifier (AUC = 0.5)')\n",
    "\n",
    "# Mark some interesting points\n",
    "# Find threshold closest to 0.5\n",
    "idx_05 = np.argmin(np.abs(roc_thresholds - 0.5))\n",
    "plt.plot(fpr[idx_05], tpr[idx_05], 'ro', markersize=10, label=f'Threshold = 0.5')\n",
    "\n",
    "# Find optimal threshold (Youden's J statistic)\n",
    "j_scores = tpr - fpr\n",
    "optimal_idx = np.argmax(j_scores)\n",
    "optimal_threshold = roc_thresholds[optimal_idx]\n",
    "plt.plot(fpr[optimal_idx], tpr[optimal_idx], 'go', markersize=10, \n",
    "         label=f'Optimal (T = {optimal_threshold:.3f})')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (TPR / Recall)', fontsize=12)\n",
    "plt.title('ROC Curve', fontsize=14, pad=20)\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOptimal threshold (Youden's J): {optimal_threshold:.4f}\")\n",
    "print(f\"  TPR (Recall): {tpr[optimal_idx]:.4f}\")\n",
    "print(f\"  FPR: {fpr[optimal_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding ROC-AUC\n",
    "\n",
    "**ROC-AUC** (Area Under the ROC Curve) summarizes the entire curve into a single number:\n",
    "- **1.0**: Perfect classifier\n",
    "- **0.5**: Random classifier\n",
    "- **< 0.5**: Worse than random\n",
    "\n",
    "**Interpretation**: Probability that the model ranks a random positive example higher than a random negative example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate ROC-AUC interpretation\n",
    "# Take 10000 random pairs of (positive, negative) samples\n",
    "np.random.seed(42)\n",
    "n_pairs = 10000\n",
    "\n",
    "pos_indices = np.where(y_true == 1)[0]\n",
    "neg_indices = np.where(y_true == 0)[0]\n",
    "\n",
    "correct_ranking = 0\n",
    "for _ in range(n_pairs):\n",
    "    pos_idx = np.random.choice(pos_indices)\n",
    "    neg_idx = np.random.choice(neg_indices)\n",
    "    \n",
    "    if y_prob[pos_idx] > y_prob[neg_idx]:\n",
    "        correct_ranking += 1\n",
    "\n",
    "empirical_auc = correct_ranking / n_pairs\n",
    "\n",
    "print(f\"ROC-AUC from sklearn: {roc_auc:.4f}\")\n",
    "print(f\"Empirical AUC (from random pairs): {empirical_auc:.4f}\")\n",
    "print(f\"\\nInterpretation: In {empirical_auc*100:.1f}% of random (positive, negative) pairs,\")\n",
    "print(f\"the model assigns a higher probability to the positive sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curve\n",
    "\n",
    "The **Precision-Recall (PR) curve** plots:\n",
    "- **X-axis**: Recall = TP / (TP + FN)\n",
    "- **Y-axis**: Precision = TP / (TP + FP)\n",
    "\n",
    "### When to use PR curves:\n",
    "- **Imbalanced datasets**: PR curves are more informative than ROC curves\n",
    "- **When you care about positive class**: PR focuses on positive predictions\n",
    "\n",
    "### Interpretation:\n",
    "- **Top-right corner** (1, 1): Perfect classifier\n",
    "- **Horizontal line at y = (positive ratio)**: Random classifier\n",
    "- **Higher curve**: Better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Precision-Recall curve\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_true, y_prob)\n",
    "\n",
    "# Calculate PR-AUC (Average Precision)\n",
    "pr_auc = average_precision_score(y_true, y_prob)\n",
    "\n",
    "# Calculate baseline (random classifier)\n",
    "baseline_precision = (y_true == 1).sum() / len(y_true)\n",
    "\n",
    "print(f\"PR-AUC (Average Precision): {pr_auc:.4f}\")\n",
    "print(f\"Baseline (random classifier): {baseline_precision:.4f}\")\n",
    "print(f\"Number of thresholds evaluated: {len(pr_thresholds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot PR curve\n",
    "plt.plot(recall, precision, 'b-', linewidth=2, label=f'PR curve (AP = {pr_auc:.3f})')\n",
    "\n",
    "# Plot baseline (random classifier)\n",
    "plt.plot([0, 1], [baseline_precision, baseline_precision], 'k--', linewidth=1, \n",
    "         label=f'Random classifier (AP = {baseline_precision:.3f})')\n",
    "\n",
    "# Mark threshold = 0.5\n",
    "idx_05 = np.argmin(np.abs(pr_thresholds - 0.5))\n",
    "plt.plot(recall[idx_05], precision[idx_05], 'ro', markersize=10, label=f'Threshold = 0.5')\n",
    "\n",
    "# Find optimal threshold (maximize F1)\n",
    "f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1] + 1e-10)\n",
    "optimal_f1_idx = np.argmax(f1_scores)\n",
    "optimal_f1_threshold = pr_thresholds[optimal_f1_idx]\n",
    "plt.plot(recall[optimal_f1_idx], precision[optimal_f1_idx], 'go', markersize=10,\n",
    "         label=f'Max F1 (T = {optimal_f1_threshold:.3f})')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Precision-Recall Curve', fontsize=14, pad=20)\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nThreshold that maximizes F1: {optimal_f1_threshold:.4f}\")\n",
    "print(f\"  Precision: {precision[optimal_f1_idx]:.4f}\")\n",
    "print(f\"  Recall: {recall[optimal_f1_idx]:.4f}\")\n",
    "print(f\"  F1-Score: {f1_scores[optimal_f1_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC vs PR Curves: Side by Side\n",
    "\n",
    "Let's compare both curves to understand when each is most useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot both curves side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# ROC Curve\n",
    "axes[0].plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random (AUC = 0.5)')\n",
    "axes[0].plot(fpr[optimal_idx], tpr[optimal_idx], 'go', markersize=10, label='Optimal point')\n",
    "axes[0].set_xlim([0.0, 1.0])\n",
    "axes[0].set_ylim([0.0, 1.05])\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=12)\n",
    "axes[0].set_title('ROC Curve', fontsize=14)\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# PR Curve\n",
    "axes[1].plot(recall, precision, 'b-', linewidth=2, label=f'PR curve (AP = {pr_auc:.3f})')\n",
    "axes[1].plot([0, 1], [baseline_precision, baseline_precision], 'k--', linewidth=1, \n",
    "             label=f'Random (AP = {baseline_precision:.3f})')\n",
    "axes[1].plot(recall[optimal_f1_idx], precision[optimal_f1_idx], 'go', markersize=10, label='Max F1')\n",
    "axes[1].set_xlim([0.0, 1.0])\n",
    "axes[1].set_ylim([0.0, 1.05])\n",
    "axes[1].set_xlabel('Recall', fontsize=12)\n",
    "axes[1].set_ylabel('Precision', fontsize=12)\n",
    "axes[1].set_title('Precision-Recall Curve', fontsize=14)\n",
    "axes[1].legend(loc='best')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why PR Curves Matter for Imbalanced Data\n",
    "\n",
    "Let's see what happens with a very imbalanced dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a poor classifier that achieves high ROC-AUC but poor PR-AUC on imbalanced data\n",
    "# Simulate a classifier that predicts mostly negatives\n",
    "poor_prob = y_prob.copy()\n",
    "poor_prob = poor_prob * 0.3  # Reduce all probabilities\n",
    "\n",
    "# Calculate metrics for poor classifier\n",
    "poor_roc_auc = roc_auc_score(y_true, poor_prob)\n",
    "poor_pr_auc = average_precision_score(y_true, poor_prob)\n",
    "\n",
    "print(\"Original Model:\")\n",
    "print(f\"  ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"  PR-AUC:  {pr_auc:.4f}\")\n",
    "print()\n",
    "print(\"Poor Model (reduced probabilities):\")\n",
    "print(f\"  ROC-AUC: {poor_roc_auc:.4f}\")\n",
    "print(f\"  PR-AUC:  {poor_pr_auc:.4f}\")\n",
    "print()\n",
    "print(\"Key Insight:\")\n",
    "print(f\"ROC-AUC is still high ({poor_roc_auc:.3f}) because the model still ranks positives\")\n",
    "print(f\"higher than negatives, but PR-AUC ({poor_pr_auc:.3f}) reveals the model\")\n",
    "print(\"is much less useful for finding actual positives!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Different Models\n",
    "\n",
    "Let's create a few hypothetical models and compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different \"models\" (just transformations of probabilities)\n",
    "models = {\n",
    "    'Original': y_prob,\n",
    "    'Conservative': y_prob * 0.7,  # Lower probabilities\n",
    "    'Aggressive': np.minimum(y_prob * 1.3, 1.0),  # Higher probabilities\n",
    "    'Random': np.random.rand(len(y_true))  # Random predictions\n",
    "}\n",
    "\n",
    "# Calculate metrics for each model\n",
    "results = []\n",
    "for name, probs in models.items():\n",
    "    roc_auc = roc_auc_score(y_true, probs)\n",
    "    pr_auc = average_precision_score(y_true, probs)\n",
    "    results.append({'Model': name, 'ROC-AUC': roc_auc, 'PR-AUC': pr_auc})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Model Comparison:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# ROC Curves\n",
    "for name, probs in models.items():\n",
    "    fpr_temp, tpr_temp, _ = roc_curve(y_true, probs)\n",
    "    roc_auc_temp = roc_auc_score(y_true, probs)\n",
    "    axes[0].plot(fpr_temp, tpr_temp, linewidth=2, label=f'{name} (AUC={roc_auc_temp:.3f})')\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.3)\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=12)\n",
    "axes[0].set_title('ROC Curves Comparison', fontsize=14)\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# PR Curves\n",
    "for name, probs in models.items():\n",
    "    prec_temp, rec_temp, _ = precision_recall_curve(y_true, probs)\n",
    "    pr_auc_temp = average_precision_score(y_true, probs)\n",
    "    axes[1].plot(rec_temp, prec_temp, linewidth=2, label=f'{name} (AP={pr_auc_temp:.3f})')\n",
    "\n",
    "axes[1].plot([0, 1], [baseline_precision, baseline_precision], 'k--', linewidth=1, alpha=0.3)\n",
    "axes[1].set_xlabel('Recall', fontsize=12)\n",
    "axes[1].set_ylabel('Precision', fontsize=12)\n",
    "axes[1].set_title('PR Curves Comparison', fontsize=14)\n",
    "axes[1].legend(loc='best')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Find Optimal Threshold for Your Use Case\n",
    "\n",
    "Find the threshold that achieves at least 95% recall while maximizing precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Find all thresholds where recall >= 0.95\n",
    "# 2. Among those, find the one with highest precision\n",
    "# 3. Report the threshold and resulting metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Calculate AUC Manually\n",
    "\n",
    "Calculate the ROC-AUC using the trapezoidal rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Use np.trapz or manually implement trapezoidal rule\n",
    "# Compare with sklearn's roc_auc_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Understand Model Ranking\n",
    "\n",
    "Verify the ROC-AUC interpretation: check what percentage of (positive, negative) pairs are correctly ranked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Sample many pairs of (positive, negative) examples\n",
    "# 2. Count how often the positive has higher probability\n",
    "# 3. Compare with ROC-AUC score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **ROC Curve**: Plots TPR vs FPR across all thresholds\n",
    "2. **ROC-AUC**: Probability of ranking a positive higher than a negative\n",
    "3. **PR Curve**: Plots Precision vs Recall across all thresholds\n",
    "4. **PR-AUC (AP)**: More informative for imbalanced datasets\n",
    "5. **Threshold Selection**: Choose based on business requirements\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **ROC curves** are good for balanced datasets and when both classes matter equally\n",
    "- **PR curves** are better for imbalanced datasets and when you care about the positive class\n",
    "- ROC-AUC can be misleadingly high on imbalanced data\n",
    "- Both curves help you choose the optimal threshold for your use case\n",
    "- AUC provides a threshold-independent measure of model quality\n",
    "\n",
    "### Decision Guide:\n",
    "\n",
    "| Scenario | Use ROC | Use PR |\n",
    "|----------|---------|--------|\n",
    "| Balanced classes | ✓ | ✓ |\n",
    "| Imbalanced classes | | ✓ |\n",
    "| Both classes equally important | ✓ | |\n",
    "| Care about positive class | | ✓ |\n",
    "| False positives and negatives equal cost | ✓ | |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "In the next notebook, we'll explore:\n",
    "- Multi-class classification metrics\n",
    "- Macro, micro, and weighted averaging\n",
    "- Per-class metrics and confusion matrices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
