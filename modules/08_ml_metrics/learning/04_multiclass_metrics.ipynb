{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Classification Metrics\n",
    "\n",
    "In this notebook, you'll learn:\n",
    "- How to evaluate multi-class classification problems\n",
    "- Confusion matrix for 3+ classes\n",
    "- Macro vs Micro vs Weighted averaging\n",
    "- Per-class metrics analysis\n",
    "- When to use which averaging strategy\n",
    "\n",
    "## From Binary to Multi-Class\n",
    "\n",
    "Binary classification (2 classes) is simpler:\n",
    "- Clear definition of positive/negative\n",
    "- Single confusion matrix\n",
    "\n",
    "Multi-class classification (3+ classes) requires:\n",
    "- One-vs-rest approach for per-class metrics\n",
    "- Aggregation strategies (macro, micro, weighted)\n",
    "- More complex confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    precision_score, recall_score, f1_score,\n",
    "    accuracy_score, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('default')\n",
    "sns.set_palette('colorblind')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Multi-Class Data\n",
    "\n",
    "We'll use a 3-class classification dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the multi-class data\n",
    "df = pd.read_csv('../../fixtures/input/multiclass_data.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels\n",
    "y_true = df['true_label'].values\n",
    "y_pred = df['predicted_label'].values\n",
    "\n",
    "# Examine class distribution\n",
    "print(\"Class distribution (true labels):\")\n",
    "print(df['true_label'].value_counts().sort_index())\n",
    "print()\n",
    "print(\"Class distribution (predicted labels):\")\n",
    "print(df['predicted_label'].value_counts().sort_index())\n",
    "\n",
    "# Visualize distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "df['true_label'].value_counts().sort_index().plot(kind='bar', ax=axes[0], \n",
    "                                                   color=['skyblue', 'lightgreen', 'lightcoral'])\n",
    "axes[0].set_title('True Label Distribution', fontsize=12)\n",
    "axes[0].set_xlabel('Class')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(['Class 0', 'Class 1', 'Class 2'], rotation=0)\n",
    "\n",
    "df['predicted_label'].value_counts().sort_index().plot(kind='bar', ax=axes[1],\n",
    "                                                        color=['skyblue', 'lightgreen', 'lightcoral'])\n",
    "axes[1].set_title('Predicted Label Distribution', fontsize=12)\n",
    "axes[1].set_xlabel('Class')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_xticklabels(['Class 0', 'Class 1', 'Class 2'], rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Confusion Matrix\n",
    "\n",
    "For 3 classes, the confusion matrix is 3x3:\n",
    "- Rows: Actual classes\n",
    "- Columns: Predicted classes\n",
    "- Diagonal: Correct predictions\n",
    "- Off-diagonal: Misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print()\n",
    "print(\"Layout:\")\n",
    "print(\"        Pred 0  Pred 1  Pred 2\")\n",
    "print(\"Actual 0  [ cm[0,0]  cm[0,1]  cm[0,2] ]\")\n",
    "print(\"Actual 1  [ cm[1,0]  cm[1,1]  cm[1,2] ]\")\n",
    "print(\"Actual 2  [ cm[2,0]  cm[2,1]  cm[2,2] ]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Counts\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                              display_labels=['Class 0', 'Class 1', 'Class 2'])\n",
    "disp.plot(ax=axes[0], cmap='Blues', values_format='d')\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14)\n",
    "\n",
    "# Plot 2: Normalized (by true label)\n",
    "cm_normalized = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "disp_norm = ConfusionMatrixDisplay(confusion_matrix=cm_normalized,\n",
    "                                   display_labels=['Class 0', 'Class 1', 'Class 2'])\n",
    "disp_norm.plot(ax=axes[1], cmap='Blues', values_format='.2f')\n",
    "axes[1].set_title('Confusion Matrix (Normalized by Row)', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the confusion matrix\n",
    "print(\"Confusion Matrix Analysis:\")\n",
    "print()\n",
    "for i in range(3):\n",
    "    total = cm[i, :].sum()\n",
    "    correct = cm[i, i]\n",
    "    print(f\"Class {i}:\")\n",
    "    print(f\"  Total samples: {total}\")\n",
    "    print(f\"  Correctly classified: {correct} ({correct/total*100:.1f}%)\")\n",
    "    print(f\"  Misclassifications:\")\n",
    "    for j in range(3):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            print(f\"    - Predicted as Class {j}: {cm[i, j]} ({cm[i, j]/total*100:.1f}%)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Class Metrics\n",
    "\n",
    "For each class, we can calculate precision, recall, and F1 using **one-vs-rest**:\n",
    "- Treat the class as \"positive\"\n",
    "- All other classes as \"negative\"\n",
    "- Calculate metrics as in binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class metrics manually for Class 1\n",
    "# For Class 1: it's positive, Classes 0 and 2 are negative\n",
    "class_idx = 1\n",
    "\n",
    "# True Positives: predicted as class_idx AND actually class_idx\n",
    "tp = cm[class_idx, class_idx]\n",
    "\n",
    "# False Positives: predicted as class_idx BUT not actually class_idx\n",
    "fp = cm[:, class_idx].sum() - tp\n",
    "\n",
    "# False Negatives: actually class_idx BUT not predicted as class_idx\n",
    "fn = cm[class_idx, :].sum() - tp\n",
    "\n",
    "# True Negatives: not class_idx AND not predicted as class_idx\n",
    "tn = cm.sum() - tp - fp - fn\n",
    "\n",
    "# Calculate metrics\n",
    "precision_class1 = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall_class1 = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1_class1 = 2 * precision_class1 * recall_class1 / (precision_class1 + recall_class1) if (precision_class1 + recall_class1) > 0 else 0\n",
    "\n",
    "print(f\"Manual calculation for Class {class_idx}:\")\n",
    "print(f\"  TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}\")\n",
    "print(f\"  Precision: {precision_class1:.4f}\")\n",
    "print(f\"  Recall: {recall_class1:.4f}\")\n",
    "print(f\"  F1-Score: {f1_class1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging Strategies\n",
    "\n",
    "To get a single metric for all classes, we can use different averaging strategies:\n",
    "\n",
    "### 1. Macro Average\n",
    "- Calculate metric for each class independently\n",
    "- Take the simple average\n",
    "- **Treats all classes equally** (good for balanced classes)\n",
    "\n",
    "$$\\text{Macro} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{metric}_i$$\n",
    "\n",
    "### 2. Micro Average\n",
    "- Aggregate TP, FP, FN across all classes\n",
    "- Calculate metric from aggregated values\n",
    "- **Weights by sample frequency** (good for imbalanced classes)\n",
    "\n",
    "$$\\text{Micro} = \\frac{\\sum TP_i}{\\sum (TP_i + FP_i)}$$\n",
    "\n",
    "### 3. Weighted Average\n",
    "- Calculate metric for each class\n",
    "- Weight by class support (number of samples)\n",
    "- **Accounts for class imbalance**\n",
    "\n",
    "$$\\text{Weighted} = \\frac{\\sum (n_i \\times \\text{metric}_i)}{\\sum n_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics with different averaging strategies\n",
    "print(\"Precision:\")\n",
    "print(f\"  Macro:    {precision_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "print(f\"  Micro:    {precision_score(y_true, y_pred, average='micro'):.4f}\")\n",
    "print(f\"  Weighted: {precision_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"Recall:\")\n",
    "print(f\"  Macro:    {recall_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "print(f\"  Micro:    {recall_score(y_true, y_pred, average='micro'):.4f}\")\n",
    "print(f\"  Weighted: {recall_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"F1-Score:\")\n",
    "print(f\"  Macro:    {f1_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "print(f\"  Micro:    {f1_score(y_true, y_pred, average='micro'):.4f}\")\n",
    "print(f\"  Weighted: {f1_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"Accuracy:\")\n",
    "print(f\"  {accuracy_score(y_true, y_pred):.4f}\")\n",
    "print()\n",
    "print(\"Note: For multi-class, micro-averaged precision/recall/F1 all equal accuracy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Macro Average Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate macro average manually\n",
    "per_class_precision = []\n",
    "per_class_recall = []\n",
    "per_class_f1 = []\n",
    "\n",
    "for class_idx in range(3):\n",
    "    # Calculate TP, FP, FN for this class\n",
    "    tp = cm[class_idx, class_idx]\n",
    "    fp = cm[:, class_idx].sum() - tp\n",
    "    fn = cm[class_idx, :].sum() - tp\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    per_class_precision.append(precision)\n",
    "    per_class_recall.append(recall)\n",
    "    per_class_f1.append(f1)\n",
    "    \n",
    "    print(f\"Class {class_idx}: Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "# Calculate macro averages\n",
    "macro_precision = np.mean(per_class_precision)\n",
    "macro_recall = np.mean(per_class_recall)\n",
    "macro_f1 = np.mean(per_class_f1)\n",
    "\n",
    "print()\n",
    "print(\"Manual Macro Averages:\")\n",
    "print(f\"  Precision: {macro_precision:.4f}\")\n",
    "print(f\"  Recall: {macro_recall:.4f}\")\n",
    "print(f\"  F1: {macro_f1:.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"sklearn Macro Averages:\")\n",
    "print(f\"  Precision: {precision_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "print(f\"  Recall: {recall_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "print(f\"  F1: {f1_score(y_true, y_pred, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report\n",
    "\n",
    "The classification_report provides a comprehensive view of all metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "report = classification_report(y_true, y_pred, \n",
    "                              target_names=['Class 0', 'Class 1', 'Class 2'])\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Per-Class Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get report as dictionary\n",
    "report_dict = classification_report(y_true, y_pred, \n",
    "                                   target_names=['Class 0', 'Class 1', 'Class 2'],\n",
    "                                   output_dict=True)\n",
    "\n",
    "# Extract per-class metrics\n",
    "classes = ['Class 0', 'Class 1', 'Class 2']\n",
    "metrics = ['precision', 'recall', 'f1-score']\n",
    "\n",
    "data = []\n",
    "for class_name in classes:\n",
    "    data.append([\n",
    "        report_dict[class_name]['precision'],\n",
    "        report_dict[class_name]['recall'],\n",
    "        report_dict[class_name]['f1-score']\n",
    "    ])\n",
    "\n",
    "data = np.array(data)\n",
    "\n",
    "# Create grouped bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "colors = ['skyblue', 'lightgreen', 'lightcoral']\n",
    "for i, class_name in enumerate(classes):\n",
    "    offset = width * (i - 1)\n",
    "    bars = ax.bar(x + offset, data[i], width, label=class_name, color=colors[i])\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                   xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                   xytext=(0, 3),\n",
    "                   textcoords=\"offset points\",\n",
    "                   ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Per-Class Performance Metrics', fontsize=14, pad=20)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Averaging Strategies Visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all averaging strategies\n",
    "averaging_methods = ['macro', 'micro', 'weighted']\n",
    "metric_names = ['Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "results = []\n",
    "for avg in averaging_methods:\n",
    "    if avg == 'micro':\n",
    "        # Micro averaging\n",
    "        prec = precision_score(y_true, y_pred, average=avg)\n",
    "        rec = recall_score(y_true, y_pred, average=avg)\n",
    "        f1 = f1_score(y_true, y_pred, average=avg)\n",
    "    else:\n",
    "        prec = precision_score(y_true, y_pred, average=avg)\n",
    "        rec = recall_score(y_true, y_pred, average=avg)\n",
    "        f1 = f1_score(y_true, y_pred, average=avg)\n",
    "    results.append([prec, rec, f1])\n",
    "\n",
    "results = np.array(results)\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(metric_names))\n",
    "width = 0.25\n",
    "\n",
    "colors = ['gold', 'lightblue', 'lightgreen']\n",
    "for i, avg_method in enumerate(averaging_methods):\n",
    "    offset = width * (i - 1)\n",
    "    bars = ax.bar(x + offset, results[i], width, label=avg_method.capitalize(), color=colors[i])\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                   xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                   xytext=(0, 3),\n",
    "                   textcoords=\"offset points\",\n",
    "                   ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Comparison of Averaging Strategies', fontsize=14, pad=20)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metric_names)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use Which Averaging Strategy?\n",
    "\n",
    "Let's create scenarios to understand the differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scenario with one poorly performing class\n",
    "print(\"Scenario Analysis:\")\n",
    "print()\n",
    "print(\"Per-class performance:\")\n",
    "for i, class_name in enumerate(classes):\n",
    "    support = report_dict[class_name]['support']\n",
    "    f1 = report_dict[class_name]['f1-score']\n",
    "    print(f\"{class_name}: F1={f1:.3f}, Support={support}\")\n",
    "\n",
    "print()\n",
    "print(\"Averaged F1-scores:\")\n",
    "print(f\"  Macro:    {f1_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "print(f\"  Micro:    {f1_score(y_true, y_pred, average='micro'):.4f}\")\n",
    "print(f\"  Weighted: {f1_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"- Macro: Simple average, treats all classes equally\")\n",
    "print(\"- Micro: Equivalent to accuracy for multi-class\")\n",
    "print(\"- Weighted: Accounts for class imbalance by weighting by support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Calculate Weighted Average Manually\n",
    "\n",
    "Verify the weighted average calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Get per-class F1 scores and supports\n",
    "# 2. Calculate weighted average: sum(f1_i * support_i) / sum(support_i)\n",
    "# 3. Compare with sklearn result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Identify Confusion Patterns\n",
    "\n",
    "Analyze the confusion matrix to identify which classes are most confused:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Find the most common misclassification (largest off-diagonal element)\n",
    "# 2. Calculate percentage of each class misclassified\n",
    "# 3. Identify which pair of classes is most confused\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Compare with Random Baseline\n",
    "\n",
    "Calculate metrics for a random classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Create random predictions (uniform over 3 classes)\n",
    "# 2. Calculate confusion matrix and metrics\n",
    "# 3. Compare with your model\n",
    "# How much better is your model than random?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Multi-class confusion matrix**: 3x3 matrix showing all classification outcomes\n",
    "2. **Per-class metrics**: Calculate precision/recall/F1 using one-vs-rest\n",
    "3. **Averaging strategies**:\n",
    "   - **Macro**: Simple average (treats all classes equally)\n",
    "   - **Micro**: Aggregate then calculate (equivalent to accuracy)\n",
    "   - **Weighted**: Weight by class support (accounts for imbalance)\n",
    "4. **Classification report**: Comprehensive view of all metrics\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- Multi-class extends binary metrics using one-vs-rest\n",
    "- Confusion matrix reveals class-specific performance and confusion patterns\n",
    "- Different averaging strategies serve different purposes\n",
    "- Macro average is sensitive to poor performance on any class\n",
    "- Weighted average is more representative for imbalanced datasets\n",
    "\n",
    "### Decision Guide for Averaging:\n",
    "\n",
    "| Scenario | Use Macro | Use Micro | Use Weighted |\n",
    "|----------|-----------|-----------|-------------|\n",
    "| Balanced classes | ✓ | ✓ | |\n",
    "| Imbalanced classes | | | ✓ |\n",
    "| All classes equally important | ✓ | | |\n",
    "| Want to penalize poor minority class performance | ✓ | | |\n",
    "| Want overall accuracy-like metric | | ✓ | |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "You now have a complete understanding of classification metrics! Practice by:\n",
    "1. Working through the task notebooks\n",
    "2. Applying these metrics to your own datasets\n",
    "3. Choosing appropriate metrics for your use cases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
