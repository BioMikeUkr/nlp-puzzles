{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Calculate Metrics from Confusion Matrix\n",
    "\n",
    "In this task, you'll practice calculating classification metrics manually from confusion matrix components.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Complete the code cells below to calculate various metrics. Each cell has test assertions that will verify your answers.\n",
    "\n",
    "**Tips:**\n",
    "- Precision = TP / (TP + FP)\n",
    "- Recall = TP / (TP + FN)\n",
    "- F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "- Accuracy = (TP + TN) / (TP + TN + FP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.1: Calculate Precision\n",
    "\n",
    "Given: TP=45, FP=5, FN=10, TN=40\n",
    "\n",
    "Calculate the precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given confusion matrix components\n",
    "TP = 45\n",
    "FP = 5\n",
    "FN = 10\n",
    "TN = 40\n",
    "\n",
    "# YOUR CODE HERE\n",
    "precision = None\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert precision is not None, \"You need to calculate precision\"\n",
    "assert isinstance(precision, (int, float)), \"Precision should be a number\"\n",
    "assert 0 <= precision <= 1, \"Precision should be between 0 and 1\"\n",
    "assert abs(precision - 0.9) < 0.001, f\"Expected 0.9, got {precision}\"\n",
    "print(f\"✓ Precision: {precision:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2: Calculate Recall\n",
    "\n",
    "Using the same confusion matrix, calculate the recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "recall = None\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert recall is not None, \"You need to calculate recall\"\n",
    "assert isinstance(recall, (int, float)), \"Recall should be a number\"\n",
    "assert 0 <= recall <= 1, \"Recall should be between 0 and 1\"\n",
    "expected_recall = TP / (TP + FN)\n",
    "assert abs(recall - expected_recall) < 0.001, f\"Expected {expected_recall:.4f}, got {recall}\"\n",
    "print(f\"✓ Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.3: Calculate F1-Score\n",
    "\n",
    "Using the precision and recall you calculated, compute the F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "f1 = None\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert f1 is not None, \"You need to calculate F1-score\"\n",
    "assert isinstance(f1, (int, float)), \"F1-score should be a number\"\n",
    "assert 0 <= f1 <= 1, \"F1-score should be between 0 and 1\"\n",
    "expected_f1 = 2 * (precision * recall) / (precision + recall)\n",
    "assert abs(f1 - expected_f1) < 0.001, f\"Expected {expected_f1:.4f}, got {f1}\"\n",
    "print(f\"✓ F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.4: Calculate Accuracy\n",
    "\n",
    "Calculate the accuracy from the confusion matrix components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "accuracy = None\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert accuracy is not None, \"You need to calculate accuracy\"\n",
    "assert isinstance(accuracy, (int, float)), \"Accuracy should be a number\"\n",
    "assert 0 <= accuracy <= 1, \"Accuracy should be between 0 and 1\"\n",
    "expected_accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "assert abs(accuracy - expected_accuracy) < 0.001, f\"Expected {expected_accuracy:.4f}, got {accuracy}\"\n",
    "print(f\"✓ Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.5: Load Data and Calculate Metrics\n",
    "\n",
    "Load the classification dataset and calculate all metrics using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../../fixtures/input/classification_data.csv')\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Extract y_true and y_pred from the dataframe\n",
    "y_true = None\n",
    "y_pred = None\n",
    "\n",
    "# Calculate metrics using sklearn\n",
    "sklearn_precision = None\n",
    "sklearn_recall = None\n",
    "sklearn_f1 = None\n",
    "sklearn_accuracy = None\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert y_true is not None, \"You need to extract y_true\"\n",
    "assert y_pred is not None, \"You need to extract y_pred\"\n",
    "assert len(y_true) == 1000, \"y_true should have 1000 samples\"\n",
    "assert len(y_pred) == 1000, \"y_pred should have 1000 samples\"\n",
    "\n",
    "assert sklearn_precision is not None, \"Calculate precision\"\n",
    "assert sklearn_recall is not None, \"Calculate recall\"\n",
    "assert sklearn_f1 is not None, \"Calculate F1-score\"\n",
    "assert sklearn_accuracy is not None, \"Calculate accuracy\"\n",
    "\n",
    "assert 0 <= sklearn_precision <= 1, \"Precision should be between 0 and 1\"\n",
    "assert 0 <= sklearn_recall <= 1, \"Recall should be between 0 and 1\"\n",
    "assert 0 <= sklearn_f1 <= 1, \"F1 should be between 0 and 1\"\n",
    "assert 0 <= sklearn_accuracy <= 1, \"Accuracy should be between 0 and 1\"\n",
    "\n",
    "print(f\"✓ Metrics calculated successfully:\")\n",
    "print(f\"  Precision: {sklearn_precision:.4f}\")\n",
    "print(f\"  Recall: {sklearn_recall:.4f}\")\n",
    "print(f\"  F1-Score: {sklearn_f1:.4f}\")\n",
    "print(f\"  Accuracy: {sklearn_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.6: Verify Manual Calculation\n",
    "\n",
    "Calculate the confusion matrix from the data and verify that your manual calculation matches sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Calculate confusion matrix\n",
    "cm = None\n",
    "\n",
    "# Extract TP, TN, FP, FN\n",
    "tn_data = None\n",
    "fp_data = None\n",
    "fn_data = None\n",
    "tp_data = None\n",
    "\n",
    "# Calculate precision manually\n",
    "manual_precision = None\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert cm is not None, \"Calculate confusion matrix\"\n",
    "assert cm.shape == (2, 2), \"Confusion matrix should be 2x2\"\n",
    "\n",
    "assert tn_data is not None and fp_data is not None, \"Extract TN and FP\"\n",
    "assert fn_data is not None and tp_data is not None, \"Extract FN and TP\"\n",
    "\n",
    "assert manual_precision is not None, \"Calculate precision manually\"\n",
    "assert abs(manual_precision - sklearn_precision) < 0.001, \\\n",
    "    f\"Manual precision {manual_precision:.4f} doesn't match sklearn {sklearn_precision:.4f}\"\n",
    "\n",
    "print(f\"✓ Confusion matrix components:\")\n",
    "print(f\"  TN: {tn_data}, FP: {fp_data}\")\n",
    "print(f\"  FN: {fn_data}, TP: {tp_data}\")\n",
    "print(f\"✓ Manual precision matches sklearn: {manual_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.7: Calculate Specificity\n",
    "\n",
    "Specificity (True Negative Rate) = TN / (TN + FP)\n",
    "\n",
    "It measures the proportion of actual negatives correctly identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "specificity = None\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert specificity is not None, \"Calculate specificity\"\n",
    "assert isinstance(specificity, (int, float)), \"Specificity should be a number\"\n",
    "assert 0 <= specificity <= 1, \"Specificity should be between 0 and 1\"\n",
    "expected_specificity = tn_data / (tn_data + fp_data)\n",
    "assert abs(specificity - expected_specificity) < 0.001, \\\n",
    "    f\"Expected {expected_specificity:.4f}, got {specificity}\"\n",
    "print(f\"✓ Specificity: {specificity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.8: Compare with Baseline\n",
    "\n",
    "Calculate metrics for a naive baseline that always predicts class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create a baseline predictor (all zeros)\n",
    "y_pred_baseline = None\n",
    "\n",
    "# Calculate baseline metrics\n",
    "baseline_accuracy = None\n",
    "baseline_precision = None  # This will be 0 or undefined\n",
    "baseline_recall = None\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert y_pred_baseline is not None, \"Create baseline predictions\"\n",
    "assert len(y_pred_baseline) == len(y_true), \"Baseline should have same length as y_true\"\n",
    "assert all(y_pred_baseline == 0), \"Baseline should predict all 0s\"\n",
    "\n",
    "assert baseline_accuracy is not None, \"Calculate baseline accuracy\"\n",
    "assert baseline_recall is not None, \"Calculate baseline recall\"\n",
    "\n",
    "# For all-zero predictions, recall should be 0 (no positives predicted)\n",
    "assert baseline_recall == 0, f\"Baseline recall should be 0, got {baseline_recall}\"\n",
    "\n",
    "print(f\"✓ Baseline metrics:\")\n",
    "print(f\"  Accuracy: {baseline_accuracy:.4f}\")\n",
    "print(f\"  Recall: {baseline_recall:.4f}\")\n",
    "print(f\"\\nModel vs Baseline:\")\n",
    "print(f\"  Accuracy improvement: {sklearn_accuracy - baseline_accuracy:.4f}\")\n",
    "print(f\"  Recall improvement: {sklearn_recall - baseline_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've completed Task 1. You've learned to:\n",
    "- Calculate precision, recall, F1-score, and accuracy manually\n",
    "- Extract confusion matrix components\n",
    "- Use sklearn to calculate metrics\n",
    "- Compare model performance with a baseline\n",
    "\n",
    "Key insights:\n",
    "- On imbalanced data, a naive baseline can have high accuracy\n",
    "- Precision, recall, and F1 give more informative metrics\n",
    "- Always compare your model with a reasonable baseline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
