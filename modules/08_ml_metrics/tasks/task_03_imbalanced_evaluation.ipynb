{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Evaluating Models on Imbalanced Data\n",
    "\n",
    "In this task, you'll practice evaluating classification models on imbalanced datasets and understanding why certain metrics are more appropriate than others.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Complete the code cells below to properly evaluate models on imbalanced data.\n",
    "\n",
    "**Key Concepts:**\n",
    "- On imbalanced data, accuracy can be misleading\n",
    "- F1-score, precision, and recall are more informative\n",
    "- PR-AUC is better than ROC-AUC for imbalanced datasets\n",
    "- Always compare with appropriate baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_curve, precision_recall_curve\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the imbalanced classification data (80% class 0, 20% class 1)\n",
    "df = pd.read_csv('../../fixtures/input/classification_data.csv')\n",
    "\n",
    "y_true = df['true_label'].values\n",
    "y_pred = df['predicted_label'].values\n",
    "y_prob = df['predicted_probability'].values\n",
    "\n",
    "print(f\"Dataset size: {len(y_true)}\")\n",
    "print(f\"Class distribution: {np.bincount(y_true)}\")\n",
    "print(f\"Class 0: {(y_true == 0).sum() / len(y_true) * 100:.1f}%\")\n",
    "print(f\"Class 1: {(y_true == 1).sum() / len(y_true) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.1: Calculate Baseline Metrics\n",
    "\n",
    "Create three baseline predictors:\n",
    "1. Always predict class 0 (majority class)\n",
    "2. Always predict class 1 (minority class)\n",
    "3. Random predictions (based on class distribution)\n",
    "\n",
    "Calculate accuracy, precision, recall, and F1 for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create three baseline predictors\n",
    "y_pred_all_0 = None\n",
    "y_pred_all_1 = None\n",
    "y_pred_random = None  # Random based on 80/20 distribution\n",
    "\n",
    "# Calculate metrics for each baseline\n",
    "# Store results in a dictionary or DataFrame\n",
    "baseline_results = None\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert y_pred_all_0 is not None, \"Create all-0 baseline\"\n",
    "assert y_pred_all_1 is not None, \"Create all-1 baseline\"\n",
    "assert y_pred_random is not None, \"Create random baseline\"\n",
    "\n",
    "assert all(y_pred_all_0 == 0), \"All-0 baseline should predict all 0s\"\n",
    "assert all(y_pred_all_1 == 1), \"All-1 baseline should predict all 1s\"\n",
    "assert len(np.unique(y_pred_random)) > 1, \"Random baseline should have both classes\"\n",
    "\n",
    "assert baseline_results is not None, \"Calculate baseline metrics\"\n",
    "\n",
    "# Check that all-0 baseline has high accuracy but 0 recall\n",
    "all_0_accuracy = accuracy_score(y_true, y_pred_all_0)\n",
    "all_0_recall = recall_score(y_true, y_pred_all_0, zero_division=0)\n",
    "assert all_0_accuracy > 0.7, \"All-0 should have high accuracy on imbalanced data\"\n",
    "assert all_0_recall == 0, \"All-0 should have 0 recall\"\n",
    "\n",
    "print(\"✓ Baseline results calculated\")\n",
    "if isinstance(baseline_results, pd.DataFrame):\n",
    "    print(baseline_results)\n",
    "else:\n",
    "    print(baseline_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2: Compare Model with Baselines\n",
    "\n",
    "Calculate metrics for your actual model and compare with baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Calculate model metrics\n",
    "model_accuracy = None\n",
    "model_precision = None\n",
    "model_recall = None\n",
    "model_f1 = None\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = None\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert model_accuracy is not None, \"Calculate model accuracy\"\n",
    "assert model_precision is not None, \"Calculate model precision\"\n",
    "assert model_recall is not None, \"Calculate model recall\"\n",
    "assert model_f1 is not None, \"Calculate model F1\"\n",
    "\n",
    "assert comparison_df is not None, \"Create comparison DataFrame\"\n",
    "assert isinstance(comparison_df, pd.DataFrame), \"Should be a DataFrame\"\n",
    "\n",
    "# Model should be better than all baselines in F1-score\n",
    "assert model_f1 > 0, \"Model should have positive F1-score\"\n",
    "\n",
    "print(\"✓ Model vs Baselines:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.3: Why Accuracy is Misleading\n",
    "\n",
    "Calculate the accuracy improvement of your model over the naive baseline (all-0).\n",
    "Then calculate the F1 improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "accuracy_improvement = None\n",
    "f1_improvement = None\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert accuracy_improvement is not None, \"Calculate accuracy improvement\"\n",
    "assert f1_improvement is not None, \"Calculate F1 improvement\"\n",
    "\n",
    "# F1 improvement should be much larger than accuracy improvement\n",
    "assert f1_improvement > accuracy_improvement, \\\n",
    "    \"F1 improvement should be larger than accuracy improvement\"\n",
    "\n",
    "print(f\"✓ Accuracy improvement: {accuracy_improvement:.4f}\")\n",
    "print(f\"✓ F1 improvement: {f1_improvement:.4f}\")\n",
    "print(f\"\\nInsight: On imbalanced data, accuracy improvement ({accuracy_improvement:.4f}) \")\n",
    "print(f\"is much smaller than F1 improvement ({f1_improvement:.4f}).\")\n",
    "print(f\"This shows why F1 is more informative for imbalanced datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.4: ROC-AUC vs PR-AUC\n",
    "\n",
    "Calculate both ROC-AUC and PR-AUC for:\n",
    "1. Your model\n",
    "2. A poor model (multiply probabilities by 0.5)\n",
    "\n",
    "Compare how much each metric changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Calculate ROC-AUC and PR-AUC for the model\n",
    "model_roc_auc = None\n",
    "model_pr_auc = None\n",
    "\n",
    "# Create a poor model by reducing probabilities\n",
    "y_prob_poor = None\n",
    "\n",
    "# Calculate metrics for poor model\n",
    "poor_roc_auc = None\n",
    "poor_pr_auc = None\n",
    "\n",
    "# Calculate relative drops\n",
    "roc_drop = None\n",
    "pr_drop = None\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert model_roc_auc is not None, \"Calculate model ROC-AUC\"\n",
    "assert model_pr_auc is not None, \"Calculate model PR-AUC\"\n",
    "assert y_prob_poor is not None, \"Create poor model probabilities\"\n",
    "assert poor_roc_auc is not None, \"Calculate poor ROC-AUC\"\n",
    "assert poor_pr_auc is not None, \"Calculate poor PR-AUC\"\n",
    "\n",
    "assert 0 <= model_roc_auc <= 1, \"ROC-AUC should be between 0 and 1\"\n",
    "assert 0 <= model_pr_auc <= 1, \"PR-AUC should be between 0 and 1\"\n",
    "\n",
    "# ROC-AUC should stay relatively high even for poor model\n",
    "# PR-AUC should drop more significantly\n",
    "assert roc_drop is not None and pr_drop is not None, \"Calculate drops\"\n",
    "assert pr_drop > roc_drop, \"PR-AUC should drop more than ROC-AUC for poor model\"\n",
    "\n",
    "print(f\"Model Performance:\")\n",
    "print(f\"  ROC-AUC: {model_roc_auc:.4f}\")\n",
    "print(f\"  PR-AUC:  {model_pr_auc:.4f}\")\n",
    "print()\n",
    "print(f\"Poor Model Performance:\")\n",
    "print(f\"  ROC-AUC: {poor_roc_auc:.4f} (drop: {roc_drop:.4f})\")\n",
    "print(f\"  PR-AUC:  {poor_pr_auc:.4f} (drop: {pr_drop:.4f})\")\n",
    "print()\n",
    "print(f\"✓ PR-AUC is more sensitive to poor performance on imbalanced data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.5: Plot ROC and PR Curves\n",
    "\n",
    "Plot both ROC and PR curves for the model and poor model side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create 1x2 subplot\n",
    "# Left: ROC curves for both models\n",
    "# Right: PR curves for both models\n",
    "# Include baseline lines\n",
    "\n",
    "# Your plotting code here\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert len(plt.gcf().axes) >= 2, \"Create 2 subplots (ROC and PR)\"\n",
    "print(\"✓ Curves plotted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.6: Calculate Class-wise Metrics\n",
    "\n",
    "For the minority class (class 1), calculate:\n",
    "- How many samples exist\n",
    "- How many were correctly predicted\n",
    "- How many were missed (FN)\n",
    "- The recall for this class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "minority_total = None\n",
    "minority_correct = None\n",
    "minority_missed = None\n",
    "minority_recall = None\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert minority_total is not None, \"Count minority class samples\"\n",
    "assert minority_correct is not None, \"Count correctly predicted minority samples\"\n",
    "assert minority_missed is not None, \"Count missed minority samples\"\n",
    "assert minority_recall is not None, \"Calculate minority recall\"\n",
    "\n",
    "assert minority_total == (y_true == 1).sum(), \"Incorrect minority total\"\n",
    "assert minority_correct + minority_missed == minority_total, \"Counts don't add up\"\n",
    "assert abs(minority_recall - (minority_correct / minority_total)) < 0.001, \\\n",
    "    \"Recall calculation incorrect\"\n",
    "\n",
    "print(f\"✓ Minority Class (Class 1) Analysis:\")\n",
    "print(f\"  Total samples: {minority_total}\")\n",
    "print(f\"  Correctly predicted: {minority_correct} ({minority_correct/minority_total*100:.1f}%)\")\n",
    "print(f\"  Missed (FN): {minority_missed} ({minority_missed/minority_total*100:.1f}%)\")\n",
    "print(f\"  Recall: {minority_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.7: Confusion Matrix Analysis\n",
    "\n",
    "Create a normalized confusion matrix (normalized by true labels) and identify:\n",
    "- What percentage of class 0 is correctly classified\n",
    "- What percentage of class 1 is correctly classified\n",
    "- Which class is harder to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Calculate normalized confusion matrix\n",
    "cm_normalized = None\n",
    "\n",
    "# Extract per-class accuracy\n",
    "class_0_accuracy = None\n",
    "class_1_accuracy = None\n",
    "\n",
    "# Determine harder class\n",
    "harder_class = None  # Should be 0 or 1\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert cm_normalized is not None, \"Calculate normalized confusion matrix\"\n",
    "assert cm_normalized.shape == (2, 2), \"Confusion matrix should be 2x2\"\n",
    "\n",
    "assert class_0_accuracy is not None, \"Calculate class 0 accuracy\"\n",
    "assert class_1_accuracy is not None, \"Calculate class 1 accuracy\"\n",
    "\n",
    "assert 0 <= class_0_accuracy <= 1, \"Class 0 accuracy should be between 0 and 1\"\n",
    "assert 0 <= class_1_accuracy <= 1, \"Class 1 accuracy should be between 0 and 1\"\n",
    "\n",
    "assert harder_class is not None, \"Identify harder class\"\n",
    "assert harder_class in [0, 1], \"Harder class should be 0 or 1\"\n",
    "\n",
    "if class_0_accuracy < class_1_accuracy:\n",
    "    assert harder_class == 0, \"Class 0 is harder (lower accuracy)\"\n",
    "else:\n",
    "    assert harder_class == 1, \"Class 1 is harder (lower accuracy)\"\n",
    "\n",
    "print(f\"✓ Per-Class Performance:\")\n",
    "print(f\"  Class 0 accuracy: {class_0_accuracy:.4f}\")\n",
    "print(f\"  Class 1 accuracy: {class_1_accuracy:.4f}\")\n",
    "print(f\"  Harder class: {harder_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.8: Stratified Performance\n",
    "\n",
    "Split the data into two groups based on probability:\n",
    "- High confidence predictions (prob > 0.7 or prob < 0.3)\n",
    "- Low confidence predictions (0.3 <= prob <= 0.7)\n",
    "\n",
    "Calculate accuracy for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create masks for high and low confidence\n",
    "high_confidence_mask = None\n",
    "low_confidence_mask = None\n",
    "\n",
    "# Calculate accuracy for each group\n",
    "high_confidence_accuracy = None\n",
    "low_confidence_accuracy = None\n",
    "\n",
    "# Count samples in each group\n",
    "n_high_confidence = None\n",
    "n_low_confidence = None\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert high_confidence_mask is not None, \"Create high confidence mask\"\n",
    "assert low_confidence_mask is not None, \"Create low confidence mask\"\n",
    "\n",
    "assert high_confidence_accuracy is not None, \"Calculate high confidence accuracy\"\n",
    "assert low_confidence_accuracy is not None, \"Calculate low confidence accuracy\"\n",
    "\n",
    "assert n_high_confidence is not None, \"Count high confidence samples\"\n",
    "assert n_low_confidence is not None, \"Count low confidence samples\"\n",
    "\n",
    "assert n_high_confidence + n_low_confidence == len(y_true), \"Masks should cover all samples\"\n",
    "\n",
    "# High confidence should have higher accuracy\n",
    "assert high_confidence_accuracy > low_confidence_accuracy, \\\n",
    "    \"High confidence predictions should be more accurate\"\n",
    "\n",
    "print(f\"✓ Stratified Performance:\")\n",
    "print(f\"  High confidence ({n_high_confidence} samples): {high_confidence_accuracy:.4f}\")\n",
    "print(f\"  Low confidence ({n_low_confidence} samples): {low_confidence_accuracy:.4f}\")\n",
    "print(f\"\\nInsight: Model is more accurate when confident in its predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.9: Recommend Best Metric\n",
    "\n",
    "Based on the imbalanced nature of the data, create a summary recommending which metrics to use and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create a dictionary with recommendations\n",
    "# Keys: metric names\n",
    "# Values: \"Recommended\" or \"Not Recommended\" with brief reason\n",
    "\n",
    "recommendations = None\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert recommendations is not None, \"Create recommendations dictionary\"\n",
    "assert isinstance(recommendations, dict), \"Should be a dictionary\"\n",
    "assert 'Accuracy' in recommendations, \"Include Accuracy\"\n",
    "assert 'F1-Score' in recommendations, \"Include F1-Score\"\n",
    "assert 'PR-AUC' in recommendations, \"Include PR-AUC\"\n",
    "\n",
    "print(\"✓ Metric Recommendations for Imbalanced Data:\")\n",
    "for metric, recommendation in recommendations.items():\n",
    "    print(f\"  {metric}: {recommendation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've completed Task 3. You've learned:\n",
    "- How to properly evaluate models on imbalanced datasets\n",
    "- Why accuracy is misleading on imbalanced data\n",
    "- Why PR-AUC is more informative than ROC-AUC for imbalanced data\n",
    "- How to analyze class-wise performance\n",
    "- How to stratify performance by confidence\n",
    "\n",
    "Key insights:\n",
    "- A naive baseline (always predict majority) can achieve high accuracy\n",
    "- F1-score better captures performance on minority class\n",
    "- PR-AUC is more sensitive to poor performance than ROC-AUC\n",
    "- Always report multiple metrics, not just accuracy\n",
    "- Consider per-class metrics to understand model behavior\n",
    "\n",
    "### Best Practices for Imbalanced Data:\n",
    "1. **Always compare with baselines** (majority class, random)\n",
    "2. **Use F1-score or PR-AUC** instead of accuracy\n",
    "3. **Report precision and recall separately** to understand trade-offs\n",
    "4. **Analyze per-class performance** to identify weaknesses\n",
    "5. **Consider cost-sensitive metrics** if misclassification costs differ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
