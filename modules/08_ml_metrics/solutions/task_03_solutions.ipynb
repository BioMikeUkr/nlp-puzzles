{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 03 Solutions: Imbalanced Data Evaluation\n",
    "\n",
    "Solutions for evaluating models on imbalanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score,\n",
    "    roc_curve, precision_recall_curve, confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load imbalanced data (80% class 0, 20% class 1)\n",
    "df = pd.read_csv('../fixtures/input/classification_data.csv')\n",
    "y_true = df['true_label'].values\n",
    "y_pred = df['predicted_label'].values\n",
    "y_prob = df['predicted_probability'].values\n",
    "\n",
    "print(f\"Class distribution:\")\n",
    "print(f\"  Class 0: {np.sum(y_true == 0)} ({np.mean(y_true == 0)*100:.1f}%)\")\n",
    "print(f\"  Class 1: {np.sum(y_true == 1)} ({np.mean(y_true == 1)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.1: Why Accuracy is Misleading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Compare with naive baseline\n",
    "y_pred_naive = np.zeros(len(y_true))  # Always predict majority class\n",
    "\n",
    "accuracy_naive = accuracy_score(y_true, y_pred_naive)\n",
    "f1_naive = f1_score(y_true, y_pred_naive, zero_division=0)\n",
    "\n",
    "accuracy_model = accuracy_score(y_true, y_pred)\n",
    "f1_model = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Naive Baseline (always predict 0):\")\n",
    "print(f\"  Accuracy: {accuracy_naive:.4f}\")\n",
    "print(f\"  F1-Score: {f1_naive:.4f}\")\n",
    "\n",
    "print(f\"\\nActual Model:\")\n",
    "print(f\"  Accuracy: {accuracy_model:.4f}\")\n",
    "print(f\"  F1-Score: {f1_model:.4f}\")\n",
    "\n",
    "print(f\"\\nüí° Insight: Naive gets {accuracy_naive:.1%} accuracy by doing nothing!\")\n",
    "print(f\"   But F1={f1_naive:.2f} reveals it catches no positives.\")\n",
    "\n",
    "assert f1_model > f1_naive\n",
    "print(\"\\n‚úÖ Demonstrated why accuracy misleads!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2: ROC-AUC vs PR-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Calculate both\n",
    "roc_auc = roc_auc_score(y_true, y_prob)\n",
    "pr_auc = average_precision_score(y_true, y_prob)\n",
    "\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"PR-AUC:  {pr_auc:.4f}\")\n",
    "\n",
    "# Plot both curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "ax1.plot(fpr, tpr, linewidth=2, label=f'Model (AUC={roc_auc:.3f})')\n",
    "ax1.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('ROC Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# PR Curve\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "baseline_pr = np.mean(y_true)  # Random baseline\n",
    "ax2.plot(recall, precision, linewidth=2, label=f'Model (AP={pr_auc:.3f})')\n",
    "ax2.axhline(baseline_pr, color='red', linestyle='--', label=f'Random (AP={baseline_pr:.3f})')\n",
    "ax2.set_xlabel('Recall')\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.set_title('Precision-Recall Curve')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Insight: ROC-AUC={roc_auc:.3f} looks great, but PR-AUC={pr_auc:.3f}\")\n",
    "print(f\"   is more realistic on imbalanced data (baseline={baseline_pr:.3f})\")\n",
    "\n",
    "assert pr_auc > baseline_pr\n",
    "print(\"\\n‚úÖ Both metrics calculated and plotted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.3: Per-Class Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Analyze each class separately\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Class 0 (negative) metrics\n",
    "class_0_precision = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "class_0_recall = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "# Class 1 (positive) metrics\n",
    "class_1_precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "class_1_recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "print(f\"Class 0 (Negative - Majority):\")\n",
    "print(f\"  Precision: {class_0_precision:.4f}\")\n",
    "print(f\"  Recall (Specificity): {class_0_recall:.4f}\")\n",
    "\n",
    "print(f\"\\nClass 1 (Positive - Minority):\")\n",
    "print(f\"  Precision: {class_1_precision:.4f}\")\n",
    "print(f\"  Recall (Sensitivity): {class_1_recall:.4f}\")\n",
    "\n",
    "print(f\"\\nüí° Insight: Model performs {'better' if class_0_recall > class_1_recall else 'worse'}\")\n",
    "print(f\"   on majority class (common in imbalanced data)\")\n",
    "\n",
    "assert class_1_precision is not None\n",
    "print(\"\\n‚úÖ Per-class analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Best Practices for Imbalanced Data\n",
    "\n",
    "1. ‚ùå **Don't use accuracy** - misleading on imbalanced data\n",
    "2. ‚úÖ **Use F1-score** - balances precision and recall\n",
    "3. ‚úÖ **Use PR-AUC** - more realistic than ROC-AUC\n",
    "4. ‚úÖ **Compare to naive baseline** - always predict majority\n",
    "5. ‚úÖ **Analyze per-class metrics** - find which class struggles\n",
    "6. ‚úÖ **Consider business costs** - FP vs FN trade-off\n",
    "7. ‚úÖ **Use confusion matrix** - see actual error patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
