{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 01 Solutions: Calculate Metrics\n",
    "\n",
    "Complete solutions for all subtasks in task_01_calculate_metrics.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.1: Calculate Precision from Confusion Matrix\n",
    "\n",
    "**Solution:** Precision = TP / (TP + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given confusion matrix values\n",
    "TP = 45\n",
    "FP = 5\n",
    "FN = 10\n",
    "TN = 40\n",
    "\n",
    "# Solution\n",
    "precision = TP / (TP + FP)\n",
    "print(f\"Precision: {precision}\")\n",
    "\n",
    "# Verify\n",
    "assert precision == 0.9, f\"Expected 0.9, got {precision}\"\n",
    "print(\"✅ Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2: Calculate Recall\n",
    "\n",
    "**Solution:** Recall = TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "recall = TP / (TP + FN)\n",
    "print(f\"Recall: {recall}\")\n",
    "\n",
    "# Verify\n",
    "assert abs(recall - 0.8182) < 0.01, f\"Expected ~0.8182, got {recall}\"\n",
    "print(\"✅ Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.3: Calculate F1-Score\n",
    "\n",
    "**Solution:** F1 = 2 × (Precision × Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(f\"F1-Score: {f1}\")\n",
    "\n",
    "# Verify\n",
    "assert abs(f1 - 0.8571) < 0.01, f\"Expected ~0.8571, got {f1}\"\n",
    "print(\"✅ Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.4: Calculate Accuracy\n",
    "\n",
    "**Solution:** Accuracy = (TP + TN) / Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Verify\n",
    "assert accuracy == 0.85, f\"Expected 0.85, got {accuracy}\"\n",
    "print(\"✅ Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.5: Calculate Metrics using sklearn\n",
    "\n",
    "**Solution:** Use sklearn functions on actual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../fixtures/input/classification_data.csv')\n",
    "y_true = df['true_label'].values\n",
    "y_pred = df['predicted_label'].values\n",
    "\n",
    "# Solution\n",
    "accuracy_sklearn = accuracy_score(y_true, y_pred)\n",
    "precision_sklearn = precision_score(y_true, y_pred)\n",
    "recall_sklearn = recall_score(y_true, y_pred)\n",
    "f1_sklearn = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Accuracy:  {accuracy_sklearn:.4f}\")\n",
    "print(f\"Precision: {precision_sklearn:.4f}\")\n",
    "print(f\"Recall:    {recall_sklearn:.4f}\")\n",
    "print(f\"F1-Score:  {f1_sklearn:.4f}\")\n",
    "\n",
    "# Verify all are calculated\n",
    "assert accuracy_sklearn is not None\n",
    "assert precision_sklearn is not None\n",
    "assert recall_sklearn is not None\n",
    "assert f1_sklearn is not None\n",
    "print(\"✅ All metrics calculated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.6: Generate Classification Report\n",
    "\n",
    "**Solution:** Use classification_report for comprehensive summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)\n",
    "\n",
    "# Verify it's a string with metrics\n",
    "assert isinstance(report, str)\n",
    "assert 'precision' in report\n",
    "assert 'recall' in report\n",
    "assert 'f1-score' in report\n",
    "print(\"✅ Report generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.7: Compare with Naive Baseline\n",
    "\n",
    "**Solution:** Naive baseline always predicts majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Predict most frequent class\n",
    "from collections import Counter\n",
    "most_common_class = Counter(y_true).most_common(1)[0][0]\n",
    "y_pred_naive = np.full(len(y_true), most_common_class)\n",
    "\n",
    "# Calculate naive baseline metrics\n",
    "accuracy_naive = accuracy_score(y_true, y_pred_naive)\n",
    "f1_naive = f1_score(y_true, y_pred_naive, zero_division=0)\n",
    "\n",
    "print(f\"Naive Baseline:\")\n",
    "print(f\"  Accuracy: {accuracy_naive:.4f}\")\n",
    "print(f\"  F1-Score: {f1_naive:.4f}\")\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"  Accuracy: {accuracy_sklearn:.4f}\")\n",
    "print(f\"  F1-Score: {f1_sklearn:.4f}\")\n",
    "\n",
    "print(f\"\\nImprovement:\")\n",
    "print(f\"  Accuracy: +{(accuracy_sklearn - accuracy_naive):.4f}\")\n",
    "print(f\"  F1-Score: +{(f1_sklearn - f1_naive):.4f}\")\n",
    "\n",
    "# Verify model is better than naive\n",
    "assert f1_sklearn > f1_naive, \"Model should be better than naive baseline\"\n",
    "print(\"\\n✅ Model beats naive baseline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.8: Calculate Specificity\n",
    "\n",
    "**Solution:** Specificity = TN / (TN + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Get confusion matrix and calculate\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "specificity = tn / (tn + fp)\n",
    "sensitivity = tp / (tp + fn)  # Same as recall\n",
    "\n",
    "print(f\"Specificity (TNR): {specificity:.4f}\")\n",
    "print(f\"Sensitivity (TPR): {sensitivity:.4f}\")\n",
    "\n",
    "# Verify it's reasonable\n",
    "assert 0 <= specificity <= 1\n",
    "assert abs(sensitivity - recall_sklearn) < 0.001  # Should match recall\n",
    "print(\"✅ Specificity calculated correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "All tasks completed! Key takeaways:\n",
    "\n",
    "1. **Precision** focuses on positive predictions quality\n",
    "2. **Recall** focuses on finding all positives  \n",
    "3. **F1** balances both with harmonic mean\n",
    "4. **Accuracy** can be misleading on imbalanced data\n",
    "5. Always compare against **naive baseline**\n",
    "6. **Specificity** matters when FP is costly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
