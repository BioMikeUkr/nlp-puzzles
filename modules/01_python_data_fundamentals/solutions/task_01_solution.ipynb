{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Ticket Processing Pipeline - SOLUTION\n",
    "\n",
    "## Scenario\n",
    "You have a CSV with support ticket data. Each ticket has:\n",
    "- `id`: ticket ID\n",
    "- `category`: ticket category\n",
    "- `description`: ticket description (may contain PII)\n",
    "- `resolution_note`: how the ticket was resolved\n",
    "- `metadata`: JSON string with additional info including status\n",
    "\n",
    "## Your Tasks:\n",
    "1. **Filter**: Keep only tickets with status `resolved` (extract from metadata)\n",
    "2. **Clean**: Remove phone numbers and emails from descriptions\n",
    "3. **Clean**: Remove category prefix from descriptions\n",
    "4. **Aggregate**: Count tickets by category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"../fixtures/input/tickets.csv\")\n",
    "print(f\"Loaded {len(df)} tickets\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data\n",
    "print(\"Sample metadata:\")\n",
    "print(df[\"metadata\"].iloc[0])\n",
    "print(\"\\nParsed metadata:\")\n",
    "print(json.loads(df[\"metadata\"].iloc[0]))\n",
    "print(\"\\nSample description with PII:\")\n",
    "print(df[\"description\"].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1: Extract Status and Filter - SOLUTION\n",
    "\n",
    "**Approach:**\n",
    "1. Parse JSON string to list of dicts\n",
    "2. Find dict that contains 'status' key\n",
    "3. Extract the value\n",
    "4. Filter DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "def extract_status(metadata_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract status from metadata JSON string.\n",
    "    \n",
    "    Metadata format: [{\"source\": \"email\"}, {\"language\": \"EN\"}, {\"status\": \"resolved\"}]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        metadata_list = json.loads(metadata_str)\n",
    "        for item in metadata_list:\n",
    "            if \"status\" in item:\n",
    "                return item[\"status\"]\n",
    "        return None\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return None\n",
    "\n",
    "# Apply to create status column\n",
    "df[\"status\"] = df[\"metadata\"].apply(extract_status)\n",
    "\n",
    "# Check status distribution before filtering\n",
    "print(\"Status distribution:\")\n",
    "print(df[\"status\"].value_counts())\n",
    "\n",
    "# Filter to keep only resolved\n",
    "df = df[df[\"status\"] == \"resolved\"].copy()\n",
    "print(f\"\\nFiltered to {len(df)} resolved tickets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "assert \"status\" in df.columns, \"Column 'status' not found\"\n",
    "assert len(df) == 10, f\"Expected 10 resolved tickets, got {len(df)}\"\n",
    "assert df[\"status\"].unique().tolist() == [\"resolved\"], \"Should only contain resolved tickets\"\n",
    "print(\"Task 1 PASSED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Remove PII from Descriptions - SOLUTION\n",
    "\n",
    "**Approach:**\n",
    "1. Create regex pattern for Lithuanian phone numbers\n",
    "2. Create regex pattern for emails\n",
    "3. Apply substitution to remove matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "def remove_pii(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove PII (phone numbers and emails) from text.\n",
    "    \n",
    "    Phone formats:\n",
    "    - +370 1234567 or +3701234567\n",
    "    - 81234567 (8 followed by 7 digits)\n",
    "    \n",
    "    Email format:\n",
    "    - name.surname@company.com\n",
    "    \"\"\"\n",
    "    # Remove phones with +370 prefix (with optional space, then 7 digits)\n",
    "    text = re.sub(r'\\+370\\s*\\d{7}', '', text)\n",
    "    \n",
    "    # Remove phones starting with 8 (8 followed by 7 digits)\n",
    "    text = re.sub(r'\\b8\\d{7}\\b', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply to description column\n",
    "df[\"description\"] = df[\"description\"].apply(remove_pii)\n",
    "\n",
    "# Debug: check for remaining phones\n",
    "print(\"Checking for remaining phones...\")\n",
    "for idx, row in df.iterrows():\n",
    "    if '+370' in row['description'] or re.search(r'8\\d{7}', row['description']):\n",
    "        print(f\"  Found in: {row['description'][:80]}...\")\n",
    "\n",
    "print(\"\\nSample cleaned description:\")\n",
    "print(df[\"description\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TEST\n# Check for phone patterns (using word boundaries to avoid false positives like error codes)\nhas_phones = df[\"description\"].str.contains(r'\\+370|\\b8\\d{7}\\b', regex=True).any()\nassert not has_phones, \"Phone numbers still present in descriptions\"\n\nhas_emails = df[\"description\"].str.contains(r'@company\\.com', regex=True).any()\nassert not has_emails, \"Emails still present in descriptions\"\n\nprint(\"Task 2 PASSED!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: Remove Category Prefix - SOLUTION\n",
    "\n",
    "**Approach:**\n",
    "1. Build pattern dynamically using category value\n",
    "2. Use re.escape() for safe pattern building\n",
    "3. Handle newlines and whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "def remove_category_prefix(text: str, category: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove category prefix from description.\n",
    "    \n",
    "    Prefix format:\n",
    "    Category:\n",
    "        {category}\n",
    "        Actual text...\n",
    "    \"\"\"\n",
    "    # Build pattern: \"Category:\\n    {category}\\n    \"\n",
    "    # Use re.escape to handle special characters in category name\n",
    "    pattern = rf'Category:\\s*{re.escape(category)}\\s*'\n",
    "    \n",
    "    cleaned = re.sub(pattern, '', text)\n",
    "    return cleaned\n",
    "\n",
    "# Apply using row values (need both description and category)\n",
    "df[\"description\"] = df.apply(\n",
    "    lambda row: remove_category_prefix(row[\"description\"], row[\"category\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Show sample\n",
    "print(\"Sample after prefix removal:\")\n",
    "for i, row in df.head(3).iterrows():\n",
    "    print(f\"\\n[{row['id'][:8]}...] {row['description'][:60]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "ticket_45ea = df[df[\"id\"] == \"45eacbab-ef56-493a-9feb-00acb471fada\"][\"description\"].iloc[0]\n",
    "expected_45ea = \"I am facing issues while trying to install Adobe Photoshop on my Windows 10 PC.\"\n",
    "assert ticket_45ea.strip() == expected_45ea.strip(), f\"Ticket 45ea not cleaned correctly. Got: '{ticket_45ea}'\"\n",
    "\n",
    "has_prefix = df[\"description\"].str.startswith(\"Category:\").any()\n",
    "assert not has_prefix, \"Some descriptions still have Category prefix\"\n",
    "\n",
    "print(\"Task 3 PASSED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4: Aggregate by Category - SOLUTION\n",
    "\n",
    "**Approach:**\n",
    "1. Calculate description length\n",
    "2. Use groupby with named aggregations\n",
    "3. Reset index for clean output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "# First, add description length column\n",
    "df[\"desc_length\"] = df[\"description\"].str.len()\n",
    "\n",
    "# Aggregate by category with named aggregations\n",
    "category_stats = df.groupby(\"category\").agg(\n",
    "    ticket_count=(\"id\", \"count\"),\n",
    "    avg_description_length=(\"desc_length\", \"mean\")\n",
    ").reset_index()\n",
    "\n",
    "# Sort by ticket count descending\n",
    "category_stats = category_stats.sort_values(\"ticket_count\", ascending=False)\n",
    "\n",
    "print(\"Category Statistics:\")\n",
    "print(category_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "assert \"category_stats\" in dir(), \"Variable 'category_stats' not found\"\n",
    "assert set(category_stats.columns) >= {\"category\", \"ticket_count\", \"avg_description_length\"}, \\\n",
    "    f\"Missing columns. Got: {category_stats.columns.tolist()}\"\n",
    "\n",
    "software_count = category_stats[category_stats[\"category\"] == \"Software Installation\"][\"ticket_count\"].iloc[0]\n",
    "assert software_count == 7, f\"Expected 7 Software Installation tickets, got {software_count}\"\n",
    "\n",
    "print(\"Task 4 PASSED!\")\n",
    "print(\"\\nFinal Category Stats:\")\n",
    "print(category_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Bonus: Full Pipeline Function - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS SOLUTION\n",
    "\n",
    "def process_tickets(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Complete ticket processing pipeline.\n",
    "    \n",
    "    Steps:\n",
    "    1. Extract status from metadata JSON\n",
    "    2. Filter to resolved tickets only\n",
    "    3. Remove PII (phones, emails) from descriptions\n",
    "    4. Remove category prefix from descriptions\n",
    "    \n",
    "    Args:\n",
    "        df: Raw ticket DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned and filtered DataFrame\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Step 1: Extract status\n",
    "    def extract_status(metadata_str):\n",
    "        try:\n",
    "            for item in json.loads(metadata_str):\n",
    "                if \"status\" in item:\n",
    "                    return item[\"status\"]\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    df[\"status\"] = df[\"metadata\"].apply(extract_status)\n",
    "    \n",
    "    # Step 2: Filter resolved\n",
    "    df = df[df[\"status\"] == \"resolved\"].copy()\n",
    "    \n",
    "    # Step 3: Remove PII (apply patterns separately for reliability)\n",
    "    def remove_pii(text):\n",
    "        text = re.sub(r'\\+370\\s*\\d{7}', '', text)  # +370 phones\n",
    "        text = re.sub(r'\\b8\\d{7}\\b', '', text)      # 8XXXXXXX phones\n",
    "        text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', '', text)  # emails\n",
    "        return text\n",
    "    \n",
    "    df[\"description\"] = df[\"description\"].apply(remove_pii)\n",
    "    \n",
    "    # Step 4: Remove category prefix\n",
    "    def remove_prefix(row):\n",
    "        pattern = rf'Category:\\s*{re.escape(row[\"category\"])}\\s*'\n",
    "        return re.sub(pattern, '', row[\"description\"])\n",
    "    \n",
    "    df[\"description\"] = df.apply(remove_prefix, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Test the pipeline\n",
    "df_raw = pd.read_csv(\"../fixtures/input/tickets.csv\")\n",
    "df_processed = process_tickets(df_raw)\n",
    "\n",
    "print(f\"Input: {len(df_raw)} tickets\")\n",
    "print(f\"Output: {len(df_processed)} resolved tickets\")\n",
    "print(f\"\\nSample processed:\")\n",
    "print(df_processed[[\"id\", \"category\", \"description\"]].head(3).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "**Key techniques used:**\n",
    "\n",
    "1. **JSON Parsing**: `json.loads()` to parse string, iterate over list of dicts\n",
    "2. **Regex Patterns** (applied separately for reliability):\n",
    "   - Phone +370: `r'\\+370\\s*\\d{7}'`\n",
    "   - Phone 8X: `r'\\b8\\d{7}\\b'`\n",
    "   - Email: `r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'`\n",
    "3. **Dynamic Patterns**: `re.escape()` for safe pattern building\n",
    "4. **Aggregation**: `groupby().agg()` with named aggregations\n",
    "5. **Pipeline Pattern**: Combine all steps into reusable function\n",
    "\n",
    "**Common pitfalls:**\n",
    "- Forgetting `.copy()` when filtering (SettingWithCopyWarning)\n",
    "- Not escaping special characters in regex patterns\n",
    "- Not handling JSON parse errors\n",
    "- Using `axis=0` instead of `axis=1` in apply with row access\n",
    "- Combining regex patterns with `|` can cause issues - apply separately for reliability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}