{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model Serving with FastAPI\n",
    "\n",
    "This notebook covers how to serve machine learning models with FastAPI:\n",
    "- Loading sentence-transformers at startup (lifespan events)\n",
    "- Creating embedding endpoints\n",
    "- Batch prediction endpoint\n",
    "- Model versioning\n",
    "- Error handling for model failures\n",
    "\n",
    "We'll build a production-ready embedding service using sentence-transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from fastapi import FastAPI, HTTPException, status\nfrom fastapi.testclient import TestClient\nfrom pydantic import BaseModel, Field, validator\nfrom typing import List, Optional, Dict, Any\nfrom contextlib import asynccontextmanager\nimport time\nfrom datetime import datetime\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nimport traceback"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lifespan Events - Loading Models at Startup\n",
    "\n",
    "The lifespan context manager allows us to load models once when the app starts and clean up when it shuts down. This is more efficient than loading models on each request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global state for storing models\n",
    "class ModelStore:\n",
    "    \"\"\"Store for ML models loaded at startup\"\"\"\n",
    "    def __init__(self):\n",
    "        self.models: Dict[str, Any] = {}\n",
    "        self.model_metadata: Dict[str, Dict] = {}\n",
    "\n",
    "model_store = ModelStore()\n",
    "\n",
    "# Lifespan context manager\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    \"\"\"Load models on startup, cleanup on shutdown\"\"\"\n",
    "    # Startup: Load models\n",
    "    print(\"ðŸš€ Loading ML models...\")\n",
    "    \n",
    "    try:\n",
    "        # Load small model for demos\n",
    "        model_name = \"all-MiniLM-L6-v2\"\n",
    "        print(f\"Loading model: {model_name}...\")\n",
    "        model = SentenceTransformer(model_name)\n",
    "        \n",
    "        # Store model and metadata\n",
    "        model_store.models[\"default\"] = model\n",
    "        model_store.model_metadata[\"default\"] = {\n",
    "            \"name\": model_name,\n",
    "            \"version\": \"1.0\",\n",
    "            \"embedding_dim\": model.get_sentence_embedding_dimension(),\n",
    "            \"max_seq_length\": model.max_seq_length,\n",
    "            \"loaded_at\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Load a second model for demonstration\n",
    "        model_name_v2 = \"paraphrase-MiniLM-L3-v2\"\n",
    "        print(f\"Loading model: {model_name_v2}...\")\n",
    "        model_v2 = SentenceTransformer(model_name_v2)\n",
    "        \n",
    "        model_store.models[\"v2\"] = model_v2\n",
    "        model_store.model_metadata[\"v2\"] = {\n",
    "            \"name\": model_name_v2,\n",
    "            \"version\": \"2.0\",\n",
    "            \"embedding_dim\": model_v2.get_sentence_embedding_dimension(),\n",
    "            \"max_seq_length\": model_v2.max_seq_length,\n",
    "            \"loaded_at\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… Successfully loaded {len(model_store.models)} models\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading models: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Yield control to the application\n",
    "    yield\n",
    "    \n",
    "    # Shutdown: Cleanup\n",
    "    print(\"ðŸ›‘ Shutting down, cleaning up models...\")\n",
    "    model_store.models.clear()\n",
    "    model_store.model_metadata.clear()\n",
    "    print(\"âœ… Cleanup complete\")\n",
    "\n",
    "# Create app with lifespan\n",
    "app = FastAPI(\n",
    "    title=\"ML Model Serving API\",\n",
    "    description=\"Serve sentence-transformer embeddings\",\n",
    "    version=\"1.0.0\",\n",
    "    lifespan=lifespan\n",
    ")\n",
    "\n",
    "print(\"FastAPI app with lifespan created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pydantic Models for Requests and Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request models\n",
    "class EmbeddingRequest(BaseModel):\n",
    "    \"\"\"Request for single text embedding\"\"\"\n",
    "    text: str = Field(..., min_length=1, max_length=5000, description=\"Text to embed\")\n",
    "    model_version: Optional[str] = Field(\"default\", description=\"Model version to use\")\n",
    "    normalize: bool = Field(True, description=\"Normalize embeddings to unit length\")\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"text\": \"FastAPI is a modern web framework for building APIs\",\n",
    "                \"model_version\": \"default\",\n",
    "                \"normalize\": True\n",
    "            }\n",
    "        }\n",
    "\n",
    "class BatchEmbeddingRequest(BaseModel):\n",
    "    \"\"\"Request for batch text embeddings\"\"\"\n",
    "    texts: List[str] = Field(..., min_items=1, max_items=100, description=\"Texts to embed\")\n",
    "    model_version: Optional[str] = Field(\"default\", description=\"Model version to use\")\n",
    "    normalize: bool = Field(True, description=\"Normalize embeddings\")\n",
    "    batch_size: int = Field(32, ge=1, le=128, description=\"Batch size for encoding\")\n",
    "    \n",
    "    @validator('texts')\n",
    "    def validate_texts(cls, v):\n",
    "        if any(len(text.strip()) == 0 for text in v):\n",
    "            raise ValueError(\"All texts must be non-empty\")\n",
    "        return v\n",
    "\n",
    "class SimilarityRequest(BaseModel):\n",
    "    \"\"\"Request for computing similarity between two texts\"\"\"\n",
    "    text1: str = Field(..., min_length=1, max_length=5000)\n",
    "    text2: str = Field(..., min_length=1, max_length=5000)\n",
    "    model_version: Optional[str] = Field(\"default\")\n",
    "\n",
    "# Response models\n",
    "class EmbeddingResponse(BaseModel):\n",
    "    \"\"\"Response with embedding vector\"\"\"\n",
    "    embedding: List[float] = Field(..., description=\"Embedding vector\")\n",
    "    dimension: int = Field(..., description=\"Embedding dimension\")\n",
    "    model_version: str = Field(..., description=\"Model version used\")\n",
    "    processing_time_ms: float = Field(..., description=\"Processing time in milliseconds\")\n",
    "\n",
    "class BatchEmbeddingResponse(BaseModel):\n",
    "    \"\"\"Response with batch embeddings\"\"\"\n",
    "    embeddings: List[List[float]] = Field(..., description=\"List of embedding vectors\")\n",
    "    count: int = Field(..., description=\"Number of embeddings\")\n",
    "    dimension: int = Field(..., description=\"Embedding dimension\")\n",
    "    model_version: str = Field(..., description=\"Model version used\")\n",
    "    processing_time_ms: float = Field(..., description=\"Processing time in milliseconds\")\n",
    "\n",
    "class SimilarityResponse(BaseModel):\n",
    "    \"\"\"Response with similarity score\"\"\"\n",
    "    similarity: float = Field(..., ge=-1.0, le=1.0, description=\"Cosine similarity (-1 to 1)\")\n",
    "    text1_preview: str = Field(..., description=\"Preview of first text\")\n",
    "    text2_preview: str = Field(..., description=\"Preview of second text\")\n",
    "    model_version: str\n",
    "\n",
    "class ModelInfo(BaseModel):\n",
    "    \"\"\"Model metadata\"\"\"\n",
    "    name: str\n",
    "    version: str\n",
    "    embedding_dim: int\n",
    "    max_seq_length: int\n",
    "    loaded_at: str\n",
    "\n",
    "print(\"Pydantic models defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Information Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    \"\"\"Root endpoint with service info\"\"\"\n",
    "    return {\n",
    "        \"service\": \"ML Model Serving API\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"models_loaded\": len(model_store.models),\n",
    "        \"available_models\": list(model_store.models.keys()),\n",
    "        \"endpoints\": [\n",
    "            \"/models\",\n",
    "            \"/embed\",\n",
    "            \"/embed/batch\",\n",
    "            \"/similarity\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    models_loaded = len(model_store.models) > 0\n",
    "    return {\n",
    "        \"status\": \"healthy\" if models_loaded else \"unhealthy\",\n",
    "        \"models_loaded\": len(model_store.models),\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "@app.get(\"/models\", response_model=Dict[str, ModelInfo])\n",
    "async def list_models():\n",
    "    \"\"\"List all available models and their metadata\"\"\"\n",
    "    if not model_store.models:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n",
    "            detail=\"No models loaded\"\n",
    "        )\n",
    "    return model_store.model_metadata\n",
    "\n",
    "@app.get(\"/models/{model_version}\", response_model=ModelInfo)\n",
    "async def get_model_info(model_version: str):\n",
    "    \"\"\"Get information about a specific model\"\"\"\n",
    "    if model_version not in model_store.models:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_404_NOT_FOUND,\n",
    "            detail=f\"Model '{model_version}' not found. Available: {list(model_store.models.keys())}\"\n",
    "        )\n",
    "    return model_store.model_metadata[model_version]\n",
    "\n",
    "print(\"Model info endpoints added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embedding Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/embed\", response_model=EmbeddingResponse)\n",
    "async def create_embedding(request: EmbeddingRequest):\n",
    "    \"\"\"Create embedding for a single text\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Validate model version\n",
    "        if request.model_version not in model_store.models:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_404_NOT_FOUND,\n",
    "                detail=f\"Model '{request.model_version}' not found. Available: {list(model_store.models.keys())}\"\n",
    "            )\n",
    "        \n",
    "        # Get model\n",
    "        model = model_store.models[request.model_version]\n",
    "        \n",
    "        # Create embedding\n",
    "        embedding = model.encode(\n",
    "            request.text,\n",
    "            normalize_embeddings=request.normalize,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        \n",
    "        # Convert to list\n",
    "        embedding_list = embedding.tolist()\n",
    "        \n",
    "        processing_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        return {\n",
    "            \"embedding\": embedding_list,\n",
    "            \"dimension\": len(embedding_list),\n",
    "            \"model_version\": request.model_version,\n",
    "            \"processing_time_ms\": round(processing_time, 2)\n",
    "        }\n",
    "        \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating embedding: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Error creating embedding: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@app.post(\"/embed/batch\", response_model=BatchEmbeddingResponse)\n",
    "async def create_batch_embeddings(request: BatchEmbeddingRequest):\n",
    "    \"\"\"Create embeddings for multiple texts in batch\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Validate model version\n",
    "        if request.model_version not in model_store.models:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_404_NOT_FOUND,\n",
    "                detail=f\"Model '{request.model_version}' not found\"\n",
    "            )\n",
    "        \n",
    "        # Get model\n",
    "        model = model_store.models[request.model_version]\n",
    "        \n",
    "        # Create embeddings in batch (much faster than one-by-one)\n",
    "        embeddings = model.encode(\n",
    "            request.texts,\n",
    "            batch_size=request.batch_size,\n",
    "            normalize_embeddings=request.normalize,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        \n",
    "        # Convert to list of lists\n",
    "        embeddings_list = embeddings.tolist()\n",
    "        \n",
    "        processing_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        return {\n",
    "            \"embeddings\": embeddings_list,\n",
    "            \"count\": len(embeddings_list),\n",
    "            \"dimension\": len(embeddings_list[0]) if embeddings_list else 0,\n",
    "            \"model_version\": request.model_version,\n",
    "            \"processing_time_ms\": round(processing_time, 2)\n",
    "        }\n",
    "        \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating batch embeddings: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Error creating batch embeddings: {str(e)}\"\n",
    "        )\n",
    "\n",
    "print(\"Embedding endpoints added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Similarity Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/similarity\", response_model=SimilarityResponse)\n",
    "async def compute_similarity(request: SimilarityRequest):\n",
    "    \"\"\"Compute cosine similarity between two texts\"\"\"\n",
    "    try:\n",
    "        # Validate model version\n",
    "        if request.model_version not in model_store.models:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_404_NOT_FOUND,\n",
    "                detail=f\"Model '{request.model_version}' not found\"\n",
    "            )\n",
    "        \n",
    "        # Get model\n",
    "        model = model_store.models[request.model_version]\n",
    "        \n",
    "        # Create embeddings for both texts\n",
    "        embeddings = model.encode(\n",
    "            [request.text1, request.text2],\n",
    "            normalize_embeddings=True,  # Normalized for cosine similarity\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        \n",
    "        # Compute cosine similarity (dot product of normalized vectors)\n",
    "        similarity = float(np.dot(embeddings[0], embeddings[1]))\n",
    "        \n",
    "        return {\n",
    "            \"similarity\": similarity,\n",
    "            \"text1_preview\": request.text1[:100] + (\"...\" if len(request.text1) > 100 else \"\"),\n",
    "            \"text2_preview\": request.text2[:100] + (\"...\" if len(request.text2) > 100 else \"\"),\n",
    "            \"model_version\": request.model_version\n",
    "        }\n",
    "        \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing similarity: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Error computing similarity: {str(e)}\"\n",
    "        )\n",
    "\n",
    "print(\"Similarity endpoint added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Running the Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create TestClient with context manager to trigger lifespan events\nprint(\"Creating TestClient and triggering model loading...\")\nprint(\"(This may take a moment to download and load models)\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test 1: Root endpoint\n# Use 'with' statement to ensure lifespan events run\nwith TestClient(app) as client:\n    response = client.get(\"/\")\n    print(\"Test 1 - Service info:\")\n    print(f\"Response: {response.json()}\")\n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test 2: List available models\nwith TestClient(app) as client:\n    response = client.get(\"/models\")\n    print(\"Test 2 - Available models:\")\n    models = response.json()\n    for version, info in models.items():\n        print(f\"\\nModel: {version}\")\n        print(f\"  Name: {info['name']}\")\n        print(f\"  Embedding dimension: {info['embedding_dim']}\")\n        print(f\"  Max sequence length: {info['max_seq_length']}\")\n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test 3: Create single embedding\nwith TestClient(app) as client:\n    request_data = {\n        \"text\": \"FastAPI is a modern web framework for building APIs with Python\",\n        \"model_version\": \"default\",\n        \"normalize\": True\n    }\n    response = client.post(\"/embed\", json=request_data)\n    result = response.json()\n\n    print(\"Test 3 - Single embedding:\")\n    print(f\"Model version: {result['model_version']}\")\n    print(f\"Embedding dimension: {result['dimension']}\")\n    print(f\"Processing time: {result['processing_time_ms']:.2f}ms\")\n    print(f\"First 5 values: {result['embedding'][:5]}\")\n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test 4: Batch embeddings\nwith TestClient(app) as client:\n    texts = [\n        \"Machine learning is a subset of artificial intelligence\",\n        \"FastAPI is built on Starlette and Pydantic\",\n        \"Python is a popular programming language\",\n        \"Natural language processing analyzes text data\",\n        \"Deep learning uses neural networks\"\n    ]\n\n    request_data = {\n        \"texts\": texts,\n        \"model_version\": \"default\",\n        \"normalize\": True,\n        \"batch_size\": 32\n    }\n\n    response = client.post(\"/embed/batch\", json=request_data)\n    result = response.json()\n\n    print(\"Test 4 - Batch embeddings:\")\n    print(f\"Number of texts: {result['count']}\")\n    print(f\"Embedding dimension: {result['dimension']}\")\n    print(f\"Processing time: {result['processing_time_ms']:.2f}ms\")\n    print(f\"Time per text: {result['processing_time_ms'] / result['count']:.2f}ms\")\n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test 5: Similarity computation\nwith TestClient(app) as client:\n    # Similar texts\n    request_data = {\n        \"text1\": \"Machine learning is a type of artificial intelligence\",\n        \"text2\": \"Artificial intelligence includes machine learning\",\n        \"model_version\": \"default\"\n    }\n    response = client.post(\"/similarity\", json=request_data)\n    result = response.json()\n\n    print(\"Test 5a - Similar texts:\")\n    print(f\"Similarity: {result['similarity']:.4f}\")\n    print(f\"Text 1: {result['text1_preview']}\")\n    print(f\"Text 2: {result['text2_preview']}\")\n    print()\n\n    # Dissimilar texts\n    request_data = {\n        \"text1\": \"I love pizza and pasta\",\n        \"text2\": \"Python is a programming language\",\n        \"model_version\": \"default\"\n    }\n    response = client.post(\"/similarity\", json=request_data)\n    result = response.json()\n\n    print(\"Test 5b - Dissimilar texts:\")\n    print(f\"Similarity: {result['similarity']:.4f}\")\n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test 6: Different model versions\nwith TestClient(app) as client:\n    text = \"FastAPI makes building APIs easy\"\n\n    for model_version in [\"default\", \"v2\"]:\n        request_data = {\n            \"text\": text,\n            \"model_version\": model_version\n        }\n        response = client.post(\"/embed\", json=request_data)\n        result = response.json()\n        \n        print(f\"Model: {model_version}\")\n        print(f\"  Dimension: {result['dimension']}\")\n        print(f\"  Processing time: {result['processing_time_ms']:.2f}ms\")\n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test 7: Error handling - invalid model\nwith TestClient(app) as client:\n    request_data = {\n        \"text\": \"Test text\",\n        \"model_version\": \"nonexistent\"\n    }\n    response = client.post(\"/embed\", json=request_data)\n\n    print(\"Test 7 - Error handling (invalid model):\")\n    print(f\"Status code: {response.status_code}\")\n    print(f\"Error: {response.json()['detail']}\")\n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test 8: Validation error - empty text\nwith TestClient(app) as client:\n    request_data = {\n        \"text\": \"\",  # Empty text\n        \"model_version\": \"default\"\n    }\n    response = client.post(\"/embed\", json=request_data)\n\n    print(\"Test 8 - Validation error (empty text):\")\n    print(f\"Status code: {response.status_code}\")\n    print(f\"Error details: {response.json()}\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Comparison: Single vs Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "with TestClient(app) as client:\n    texts = [\n        \"Machine learning models can be served via APIs\",\n        \"FastAPI provides automatic documentation\",\n        \"Batch processing is more efficient than single requests\",\n        \"Sentence transformers create text embeddings\",\n        \"Python is widely used in machine learning\",\n        \"Deep learning requires substantial computational resources\",\n        \"Natural language processing analyzes human language\",\n        \"Vector embeddings capture semantic meaning\",\n        \"API endpoints can handle batch operations\",\n        \"Model versioning enables gradual rollouts\"\n    ]\n\n    # Method 1: Individual requests\n    start_time = time.time()\n    for text in texts:\n        client.post(\"/embed\", json={\"text\": text})\n    individual_time = (time.time() - start_time) * 1000\n\n    # Method 2: Batch request\n    start_time = time.time()\n    client.post(\"/embed/batch\", json={\"texts\": texts})\n    batch_time = (time.time() - start_time) * 1000\n\n    print(\"Performance comparison (10 texts):\")\n    print(f\"Individual requests: {individual_time:.2f}ms\")\n    print(f\"Batch request: {batch_time:.2f}ms\")\n    print(f\"Speedup: {individual_time / batch_time:.2f}x faster with batching\")\n    print(f\"\\nðŸ’¡ Key insight: Batch processing is much more efficient!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "### What we learned:\n",
    "\n",
    "1. **Lifespan Events**:\n",
    "   - Use `@asynccontextmanager` for startup/shutdown logic\n",
    "   - Load models once at startup (not on each request)\n",
    "   - Store models in a global state object\n",
    "   - Clean up resources on shutdown\n",
    "\n",
    "2. **Model Serving Best Practices**:\n",
    "   - Load models at startup for better performance\n",
    "   - Support multiple model versions\n",
    "   - Provide model metadata endpoints\n",
    "   - Return processing time in responses\n",
    "\n",
    "3. **Batch Processing**:\n",
    "   - Batch endpoints are much more efficient\n",
    "   - Use appropriate batch sizes\n",
    "   - Set reasonable limits (max 100 texts in example)\n",
    "   - Track processing time to monitor performance\n",
    "\n",
    "4. **Error Handling**:\n",
    "   - Validate model versions exist\n",
    "   - Catch and handle model errors gracefully\n",
    "   - Return appropriate HTTP status codes\n",
    "   - Provide helpful error messages\n",
    "\n",
    "5. **Model Versioning**:\n",
    "   - Support multiple model versions simultaneously\n",
    "   - Allow clients to specify which version to use\n",
    "   - Track metadata for each model\n",
    "   - Enable A/B testing and gradual rollouts\n",
    "\n",
    "### Production Considerations:\n",
    "- Add request rate limiting\n",
    "- Implement request queuing for high load\n",
    "- Monitor model performance metrics\n",
    "- Add model caching strategies\n",
    "- Implement health checks for models\n",
    "- Add logging and tracing\n",
    "\n",
    "### Next steps:\n",
    "- In the next notebook, we'll explore async operations and background tasks\n",
    "- We'll cover file uploads, streaming responses, and concurrent request handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"\\nðŸŽ‰ Congratulations! You've completed ML Model Serving!\\n\")\nprint(f\"Your embedding service is ready for testing!\")\nprint(f\"In production, you would run: uvicorn app:app --host 0.0.0.0 --port 8000\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}