{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Task 2: Build a File Upload and Processing API - SOLUTION\n",
    "\n",
    "## Scenario\n",
    "Build a FastAPI service that handles file uploads with background processing:\n",
    "1. Accept file uploads with validation\n",
    "2. Process files in background tasks\n",
    "3. Track processing status\n",
    "4. Use async file operations for efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks, status\n",
    "from fastapi.testclient import TestClient\n",
    "from pydantic import BaseModel, Field\n",
    "import aiofiles\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "### Create temporary directory for file storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Create temp directory for this session\n",
    "TEMP_DIR = Path(tempfile.mkdtemp())\n",
    "UPLOAD_DIR = TEMP_DIR / \"uploads\"\n",
    "RESULTS_DIR = TEMP_DIR / \"results\"\n",
    "\n",
    "UPLOAD_DIR.mkdir(exist_ok=True)\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Upload directory: {UPLOAD_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1: Setup Job Tracking and App - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "# Global dict to track jobs\n",
    "# Structure: {job_id: {\"status\": str, \"filename\": str, \"result\": dict}}\n",
    "jobs: Dict[str, Dict] = {}\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    \"\"\"Lifespan context manager for cleanup.\"\"\"\n",
    "    # Startup\n",
    "    print(\"File processing API starting up...\")\n",
    "    print(f\"Upload directory: {UPLOAD_DIR}\")\n",
    "    \n",
    "    yield  # App runs here\n",
    "    \n",
    "    # Shutdown: Cleanup\n",
    "    print(\"Cleaning up temporary files...\")\n",
    "    # In production, you'd want to clean up old files\n",
    "    jobs.clear()\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"File Upload and Processing API\",\n",
    "    description=\"Upload files for background processing with status tracking\",\n",
    "    version=\"1.0.0\",\n",
    "    lifespan=lifespan\n",
    ")\n",
    "\n",
    "print(\"FastAPI app created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "assert 'jobs' in dir(), \"jobs dict not found\"\n",
    "assert 'lifespan' in dir(), \"lifespan function not found\"\n",
    "assert 'app' in dir(), \"app not found\"\n",
    "assert isinstance(jobs, dict), \"jobs should be a dict\"\n",
    "\n",
    "client = TestClient(app)\n",
    "\n",
    "print(\"✓ Task 1 PASSED!\")\n",
    "print(\"  Job tracking and app initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Define Pydantic Models - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "class UploadResponse(BaseModel):\n",
    "    \"\"\"Response model for file upload.\"\"\"\n",
    "    job_id: str = Field(..., description=\"Unique job identifier\")\n",
    "    filename: str = Field(..., description=\"Uploaded filename\")\n",
    "    status: str = Field(..., description=\"Current job status\")\n",
    "    message: str = Field(..., description=\"Status message\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n",
    "                \"filename\": \"document.txt\",\n",
    "                \"status\": \"pending\",\n",
    "                \"message\": \"File uploaded successfully and queued for processing\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "class JobStatus(BaseModel):\n",
    "    \"\"\"Model for job status.\"\"\"\n",
    "    job_id: str = Field(..., description=\"Job identifier\")\n",
    "    status: str = Field(..., description=\"Current status\")\n",
    "    filename: str = Field(..., description=\"Filename being processed\")\n",
    "    result: Optional[Dict] = Field(None, description=\"Processing result (if completed)\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n",
    "                \"status\": \"completed\",\n",
    "                \"filename\": \"document.txt\",\n",
    "                \"result\": {\n",
    "                    \"line_count\": 100,\n",
    "                    \"word_count\": 500,\n",
    "                    \"char_count\": 3000\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "class ProcessingResult(BaseModel):\n",
    "    \"\"\"Model for file processing results.\"\"\"\n",
    "    line_count: int = Field(..., description=\"Number of lines\")\n",
    "    word_count: int = Field(..., description=\"Number of words\")\n",
    "    char_count: int = Field(..., description=\"Number of characters\")\n",
    "    processing_time: float = Field(..., description=\"Processing time in seconds\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"line_count\": 100,\n",
    "                \"word_count\": 500,\n",
    "                \"char_count\": 3000,\n",
    "                \"processing_time\": 0.15\n",
    "            }\n",
    "        }\n",
    "\n",
    "print(\"Pydantic models defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "assert 'UploadResponse' in dir(), \"UploadResponse not found\"\n",
    "assert 'JobStatus' in dir(), \"JobStatus not found\"\n",
    "assert 'ProcessingResult' in dir(), \"ProcessingResult not found\"\n",
    "\n",
    "# Test models\n",
    "upload_resp = UploadResponse(\n",
    "    job_id=\"test-123\",\n",
    "    filename=\"test.txt\",\n",
    "    status=\"pending\",\n",
    "    message=\"File uploaded\"\n",
    ")\n",
    "assert upload_resp.job_id == \"test-123\"\n",
    "\n",
    "job_status = JobStatus(\n",
    "    job_id=\"test-123\",\n",
    "    status=\"completed\",\n",
    "    filename=\"test.txt\",\n",
    "    result={\"lines\": 10}\n",
    ")\n",
    "assert job_status.result is not None\n",
    "\n",
    "result = ProcessingResult(\n",
    "    line_count=10,\n",
    "    word_count=50,\n",
    "    char_count=300,\n",
    "    processing_time=0.5\n",
    ")\n",
    "assert result.line_count == 10\n",
    "\n",
    "print(\"✓ Task 2 PASSED!\")\n",
    "print(\"  All Pydantic models defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: Implement Background Processing Function - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "async def process_file(job_id: str, file_path: Path):\n",
    "    \"\"\"\n",
    "    Process uploaded file asynchronously.\n",
    "    \n",
    "    Args:\n",
    "        job_id: Job identifier\n",
    "        file_path: Path to uploaded file\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Update status to processing\n",
    "        jobs[job_id][\"status\"] = \"processing\"\n",
    "        \n",
    "        # Read file asynchronously\n",
    "        async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = await f.read()\n",
    "        \n",
    "        # Process content\n",
    "        lines = content.split('\\n')\n",
    "        line_count = len(lines)\n",
    "        \n",
    "        # Count words (split on whitespace)\n",
    "        words = content.split()\n",
    "        word_count = len(words)\n",
    "        \n",
    "        # Count characters\n",
    "        char_count = len(content)\n",
    "        \n",
    "        # Calculate processing time\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Create result\n",
    "        result = {\n",
    "            \"line_count\": line_count,\n",
    "            \"word_count\": word_count,\n",
    "            \"char_count\": char_count,\n",
    "            \"processing_time\": processing_time\n",
    "        }\n",
    "        \n",
    "        # Update job with result\n",
    "        jobs[job_id][\"status\"] = \"completed\"\n",
    "        jobs[job_id][\"result\"] = result\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Handle errors\n",
    "        jobs[job_id][\"status\"] = \"failed\"\n",
    "        jobs[job_id][\"error\"] = str(e)\n",
    "\n",
    "print(\"process_file function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "# TEST\nimport asyncio\n\nassert 'process_file' in dir(), \"process_file function not found\"\n\n# Create test file\ntest_file = UPLOAD_DIR / \"test_processing.txt\"\ntest_file.write_text(\"Hello world\\nThis is a test\\nThree lines total\")\n\n# Test processing\ntest_job_id = \"test-job-123\"\njobs[test_job_id] = {\"status\": \"pending\", \"filename\": \"test_processing.txt\"}\n\n# Run async function (use await in Jupyter since event loop is already running)\nawait process_file(test_job_id, test_file)\n\n# Check results\nassert jobs[test_job_id]['status'] == 'completed', f\"Expected completed, got {jobs[test_job_id]['status']}\"\nassert 'result' in jobs[test_job_id], \"Result not found in job\"\nresult = jobs[test_job_id]['result']\nassert result['line_count'] == 3, f\"Expected 3 lines, got {result['line_count']}\"\nassert result['word_count'] > 0, \"Word count should be > 0\"\nassert result['char_count'] > 0, \"Char count should be > 0\"\n\nprint(\"✓ Task 3 PASSED!\")\nprint(f\"  Processing result: {result}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4: Implement File Upload Endpoint - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB\n",
    "ALLOWED_EXTENSIONS = {\".txt\", \".csv\", \".json\"}\n",
    "\n",
    "@app.post(\"/upload\", response_model=UploadResponse, status_code=status.HTTP_200_OK)\n",
    "async def upload_file(\n",
    "    file: UploadFile = File(...),\n",
    "    background_tasks: BackgroundTasks = BackgroundTasks()\n",
    "):\n",
    "    \"\"\"\n",
    "    Upload a file for processing.\n",
    "    \n",
    "    Args:\n",
    "        file: File to upload\n",
    "        background_tasks: FastAPI background tasks\n",
    "        \n",
    "    Returns:\n",
    "        UploadResponse with job information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate file extension\n",
    "        file_ext = Path(file.filename).suffix.lower()\n",
    "        if file_ext not in ALLOWED_EXTENSIONS:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_400_BAD_REQUEST,\n",
    "                detail=f\"File type {file_ext} not allowed. Allowed: {ALLOWED_EXTENSIONS}\"\n",
    "            )\n",
    "        \n",
    "        # Generate unique job ID\n",
    "        job_id = str(uuid.uuid4())\n",
    "        \n",
    "        # Create unique filename\n",
    "        safe_filename = f\"{job_id}_{file.filename}\"\n",
    "        file_path = UPLOAD_DIR / safe_filename\n",
    "        \n",
    "        # Save file asynchronously\n",
    "        async with aiofiles.open(file_path, 'wb') as f:\n",
    "            content = await file.read()\n",
    "            \n",
    "            # Check file size\n",
    "            if len(content) > MAX_FILE_SIZE:\n",
    "                raise HTTPException(\n",
    "                    status_code=status.HTTP_400_BAD_REQUEST,\n",
    "                    detail=f\"File too large. Max size: {MAX_FILE_SIZE / 1024 / 1024}MB\"\n",
    "                )\n",
    "            \n",
    "            await f.write(content)\n",
    "        \n",
    "        # Create job entry\n",
    "        jobs[job_id] = {\n",
    "            \"status\": \"pending\",\n",
    "            \"filename\": file.filename,\n",
    "            \"file_path\": str(file_path)\n",
    "        }\n",
    "        \n",
    "        # Schedule background processing\n",
    "        background_tasks.add_task(process_file, job_id, file_path)\n",
    "        \n",
    "        return UploadResponse(\n",
    "            job_id=job_id,\n",
    "            filename=file.filename,\n",
    "            status=\"pending\",\n",
    "            message=\"File uploaded successfully and queued for processing\"\n",
    "        )\n",
    "        \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Upload failed: {str(e)}\"\n",
    "        )\n",
    "\n",
    "print(\"/upload endpoint implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "from io import BytesIO\n",
    "\n",
    "client = TestClient(app)\n",
    "\n",
    "# Create test file content\n",
    "test_content = b\"Line 1\\nLine 2\\nLine 3\"\n",
    "test_file = (\"test.txt\", BytesIO(test_content), \"text/plain\")\n",
    "\n",
    "# Upload file\n",
    "response = client.post(\n",
    "    \"/upload\",\n",
    "    files={\"file\": test_file}\n",
    ")\n",
    "\n",
    "assert response.status_code == 200, f\"Expected 200, got {response.status_code}\"\n",
    "data = response.json()\n",
    "assert 'job_id' in data, \"Response missing job_id\"\n",
    "assert 'filename' in data, \"Response missing filename\"\n",
    "assert 'status' in data, \"Response missing status\"\n",
    "assert data['status'] == 'pending', f\"Expected pending, got {data['status']}\"\n",
    "\n",
    "job_id = data['job_id']\n",
    "assert job_id in jobs, \"Job not found in jobs dict\"\n",
    "\n",
    "# Test invalid file type\n",
    "invalid_file = (\"test.exe\", BytesIO(b\"fake exe\"), \"application/x-msdownload\")\n",
    "response = client.post(\n",
    "    \"/upload\",\n",
    "    files={\"file\": invalid_file}\n",
    ")\n",
    "assert response.status_code == 400, \"Invalid file type should return 400\"\n",
    "\n",
    "# Wait a bit for background task\n",
    "time.sleep(1)\n",
    "\n",
    "print(\"✓ Task 4 PASSED!\")\n",
    "print(f\"  File uploaded with job_id: {job_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 5: Implement Status and Results Endpoints - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "@app.get(\"/jobs/{job_id}\", response_model=JobStatus, status_code=status.HTTP_200_OK)\n",
    "async def get_job_status(job_id: str):\n",
    "    \"\"\"\n",
    "    Get status of a specific job.\n",
    "    \n",
    "    Args:\n",
    "        job_id: Job identifier\n",
    "        \n",
    "    Returns:\n",
    "        JobStatus with current status and result\n",
    "    \"\"\"\n",
    "    if job_id not in jobs:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_404_NOT_FOUND,\n",
    "            detail=f\"Job {job_id} not found\"\n",
    "        )\n",
    "    \n",
    "    job = jobs[job_id]\n",
    "    \n",
    "    return JobStatus(\n",
    "        job_id=job_id,\n",
    "        status=job[\"status\"],\n",
    "        filename=job[\"filename\"],\n",
    "        result=job.get(\"result\")\n",
    "    )\n",
    "\n",
    "@app.get(\"/jobs\", response_model=List[JobStatus], status_code=status.HTTP_200_OK)\n",
    "async def list_jobs():\n",
    "    \"\"\"\n",
    "    List all jobs.\n",
    "    \n",
    "    Returns:\n",
    "        List of JobStatus for all jobs\n",
    "    \"\"\"\n",
    "    return [\n",
    "        JobStatus(\n",
    "            job_id=job_id,\n",
    "            status=job[\"status\"],\n",
    "            filename=job[\"filename\"],\n",
    "            result=job.get(\"result\")\n",
    "        )\n",
    "        for job_id, job in jobs.items()\n",
    "    ]\n",
    "\n",
    "@app.delete(\"/jobs/{job_id}\", status_code=status.HTTP_200_OK)\n",
    "async def delete_job(job_id: str):\n",
    "    \"\"\"\n",
    "    Delete a job and associated files.\n",
    "    \n",
    "    Args:\n",
    "        job_id: Job identifier\n",
    "        \n",
    "    Returns:\n",
    "        Success message\n",
    "    \"\"\"\n",
    "    if job_id not in jobs:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_404_NOT_FOUND,\n",
    "            detail=f\"Job {job_id} not found\"\n",
    "        )\n",
    "    \n",
    "    job = jobs[job_id]\n",
    "    \n",
    "    # Delete uploaded file if exists\n",
    "    if \"file_path\" in job:\n",
    "        file_path = Path(job[\"file_path\"])\n",
    "        if file_path.exists():\n",
    "            file_path.unlink()\n",
    "    \n",
    "    # Remove job from tracking\n",
    "    del jobs[job_id]\n",
    "    \n",
    "    return {\n",
    "        \"message\": f\"Job {job_id} deleted successfully\",\n",
    "        \"job_id\": job_id\n",
    "    }\n",
    "\n",
    "print(\"Status and management endpoints implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "client = TestClient(app)\n",
    "\n",
    "# First upload a file\n",
    "test_content = b\"Test line 1\\nTest line 2\"\n",
    "response = client.post(\n",
    "    \"/upload\",\n",
    "    files={\"file\": (\"status_test.txt\", BytesIO(test_content), \"text/plain\")}\n",
    ")\n",
    "job_id = response.json()['job_id']\n",
    "\n",
    "# Wait for processing\n",
    "time.sleep(1)\n",
    "\n",
    "# Test get specific job\n",
    "response = client.get(f\"/jobs/{job_id}\")\n",
    "assert response.status_code == 200, f\"Expected 200, got {response.status_code}\"\n",
    "data = response.json()\n",
    "assert data['job_id'] == job_id\n",
    "assert data['status'] in ['pending', 'processing', 'completed'], f\"Unexpected status: {data['status']}\"\n",
    "\n",
    "# Test get all jobs\n",
    "response = client.get(\"/jobs\")\n",
    "assert response.status_code == 200\n",
    "jobs_list = response.json()\n",
    "assert isinstance(jobs_list, list), \"Expected list of jobs\"\n",
    "assert len(jobs_list) > 0, \"Should have at least one job\"\n",
    "\n",
    "# Test delete job\n",
    "response = client.delete(f\"/jobs/{job_id}\")\n",
    "assert response.status_code == 200\n",
    "\n",
    "# Verify job deleted\n",
    "response = client.get(f\"/jobs/{job_id}\")\n",
    "assert response.status_code == 404, \"Deleted job should return 404\"\n",
    "\n",
    "# Test non-existent job\n",
    "response = client.get(\"/jobs/fake-job-id\")\n",
    "assert response.status_code == 404, \"Non-existent job should return 404\"\n",
    "\n",
    "print(\"✓ Task 5 PASSED!\")\n",
    "print(\"  Status and management endpoints working\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "## Bonus: Test Complete Upload Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real test file\n",
    "with open('../fixtures/input/test_file.txt', 'rb') as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "print(\"=== Complete File Upload Flow ===\")\n",
    "print()\n",
    "\n",
    "# 1. Upload file\n",
    "response = client.post(\n",
    "    \"/upload\",\n",
    "    files={\"file\": (\"test_file.txt\", BytesIO(file_content), \"text/plain\")}\n",
    ")\n",
    "print(\"1. File Upload:\")\n",
    "upload_data = response.json()\n",
    "print(f\"   Job ID: {upload_data['job_id']}\")\n",
    "print(f\"   Status: {upload_data['status']}\")\n",
    "print()\n",
    "\n",
    "job_id = upload_data['job_id']\n",
    "\n",
    "# 2. Check status immediately\n",
    "response = client.get(f\"/jobs/{job_id}\")\n",
    "print(\"2. Initial Status:\")\n",
    "status_data = response.json()\n",
    "print(f\"   Status: {status_data['status']}\")\n",
    "print()\n",
    "\n",
    "# 3. Wait for processing\n",
    "print(\"3. Waiting for processing...\")\n",
    "time.sleep(2)\n",
    "\n",
    "# 4. Check final status\n",
    "response = client.get(f\"/jobs/{job_id}\")\n",
    "print(\"\\n4. Final Status:\")\n",
    "final_data = response.json()\n",
    "print(f\"   Status: {final_data['status']}\")\n",
    "if final_data.get('result'):\n",
    "    print(\"   Results:\")\n",
    "    for key, value in final_data['result'].items():\n",
    "        print(f\"     - {key}: {value}\")\n",
    "print()\n",
    "\n",
    "# 5. List all jobs\n",
    "response = client.get(\"/jobs\")\n",
    "all_jobs = response.json()\n",
    "print(f\"5. Total Jobs: {len(all_jobs)}\")\n",
    "print()\n",
    "\n",
    "print(\"✓ Complete flow test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup temp directory\n",
    "shutil.rmtree(TEMP_DIR)\n",
    "print(f\"Cleaned up: {TEMP_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "**Key techniques used:**\n",
    "\n",
    "1. **File uploads:**\n",
    "   - Use `UploadFile` for efficient streaming\n",
    "   - Validate file type and size before saving\n",
    "   - Generate unique filenames to avoid collisions\n",
    "\n",
    "2. **Background tasks:**\n",
    "   - Use `BackgroundTasks` to process without blocking\n",
    "   - Update job status as processing progresses\n",
    "   - Handle errors gracefully\n",
    "\n",
    "3. **Async file I/O:**\n",
    "   - Use `aiofiles` for non-blocking file operations\n",
    "   - Read and write files asynchronously\n",
    "   - Better performance under load\n",
    "\n",
    "4. **Job tracking:**\n",
    "   - Use UUID for unique job identifiers\n",
    "   - Store job state in memory (use database in production)\n",
    "   - Provide status endpoints for monitoring\n",
    "\n",
    "5. **Validation:**\n",
    "   - Check file extensions\n",
    "   - Enforce size limits\n",
    "   - Return appropriate error codes\n",
    "\n",
    "6. **Cleanup:**\n",
    "   - Delete files when jobs are removed\n",
    "   - Use lifespan for application cleanup\n",
    "   - Clear job tracking on shutdown\n",
    "\n",
    "**Production considerations:**\n",
    "- Use database for job tracking (Redis, PostgreSQL)\n",
    "- Implement job expiration and cleanup\n",
    "- Add authentication and authorization\n",
    "- Use object storage (S3) for files\n",
    "- Add rate limiting\n",
    "- Implement progress tracking for long operations\n",
    "- Add webhooks for job completion\n",
    "\n",
    "**Common pitfalls avoided:**\n",
    "- Reading entire file into memory (use streaming)\n",
    "- Not validating file size before reading\n",
    "- Blocking operations in async context\n",
    "- Not handling file encoding errors\n",
    "- Filename collision issues\n",
    "- Not cleaning up uploaded files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}