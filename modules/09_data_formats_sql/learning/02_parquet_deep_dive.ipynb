{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet Deep Dive: Advanced Usage Patterns\n",
    "\n",
    "This notebook covers advanced Parquet features that are essential for production ML systems:\n",
    "\n",
    "1. **Chunked reading/writing** - Process large files without loading everything into memory\n",
    "2. **Partitioning** - Organize data by columns (e.g., by date, region) for faster queries\n",
    "3. **Predicate pushdown** - Filter data during read (at the file level, not in Python)\n",
    "4. **Reading multiple files** - Work with partitioned datasets\n",
    "5. **Row group optimization** - Fine-tune internal structure for query performance\n",
    "6. **Statistics and metadata** - Inspect file contents without reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "from pathlib import Path\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "# Create output directories\n",
    "output_dir = Path('../fixtures/output')\n",
    "partitioned_dir = output_dir / 'partitioned_data'\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"PyArrow version: {pa.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Large Sample Dataset\n",
    "\n",
    "We'll create a realistic time-series dataset with multiple dimensions for partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Create 1 million rows of e-commerce transaction data\n",
    "n_rows = 1_000_000\n",
    "\n",
    "# Generate date range (1 year of data)\n",
    "dates = pd.date_range('2023-01-01', '2023-12-31', freq='30s')[:n_rows]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'transaction_id': range(1, n_rows + 1),\n",
    "    'timestamp': dates,\n",
    "    'customer_id': np.random.randint(1000, 50000, n_rows),\n",
    "    'product_category': np.random.choice(['Electronics', 'Clothing', 'Food', 'Books', 'Sports'], n_rows),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], n_rows),\n",
    "    'amount': np.round(np.random.uniform(5, 500, n_rows), 2),\n",
    "    'quantity': np.random.randint(1, 10, n_rows),\n",
    "    'payment_method': np.random.choice(['Credit Card', 'Debit Card', 'PayPal', 'Cash'], n_rows),\n",
    "})\n",
    "\n",
    "# Extract date components for partitioning\n",
    "df['year'] = df['timestamp'].dt.year\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['day'] = df['timestamp'].dt.day\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chunked Writing (Streaming Large DataFrames)\n",
    "\n",
    "When writing large datasets, you may not be able to fit everything in memory. PyArrow supports **chunked writing** using `ParquetWriter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data in chunks\n",
    "chunk_size = 100_000\n",
    "output_file = output_dir / 'large_data.parquet'\n",
    "\n",
    "# Convert pandas DataFrame to PyArrow Table for the first chunk\n",
    "table = pa.Table.from_pandas(df[:chunk_size])\n",
    "\n",
    "# Initialize writer with schema from first chunk\n",
    "writer = pq.ParquetWriter(output_file, table.schema, compression='snappy')\n",
    "\n",
    "# Write chunks\n",
    "print(f\"Writing {len(df):,} rows in chunks of {chunk_size:,}...\")\n",
    "for i in range(0, len(df), chunk_size):\n",
    "    chunk = df[i:i+chunk_size]\n",
    "    table = pa.Table.from_pandas(chunk)\n",
    "    writer.write_table(table)\n",
    "    print(f\"  Wrote chunk {i//chunk_size + 1}: rows {i:,} to {min(i+chunk_size, len(df)):,}\")\n",
    "\n",
    "writer.close()\n",
    "print(f\"\\nFile size: {output_file.stat().st_size / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chunked Reading (Processing Large Files)\n",
    "\n",
    "Read large Parquet files in chunks without loading everything into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file in batches\n",
    "parquet_file = pq.ParquetFile(output_file)\n",
    "\n",
    "print(f\"File metadata:\")\n",
    "print(f\"  Total rows: {parquet_file.metadata.num_rows:,}\")\n",
    "print(f\"  Number of row groups: {parquet_file.num_row_groups}\")\n",
    "print(f\"  Columns: {len(parquet_file.schema)}\")\n",
    "\n",
    "# Process data in batches\n",
    "batch_size = 250_000\n",
    "total_amount = 0\n",
    "\n",
    "print(f\"\\nProcessing in batches of {batch_size:,} rows...\")\n",
    "for batch in parquet_file.iter_batches(batch_size=batch_size):\n",
    "    # Convert to pandas for processing (or use PyArrow compute)\n",
    "    batch_df = batch.to_pandas()\n",
    "    total_amount += batch_df['amount'].sum()\n",
    "    print(f\"  Processed batch: {len(batch_df):,} rows\")\n",
    "\n",
    "print(f\"\\nTotal amount across all transactions: ${total_amount:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Partitioned Datasets\n",
    "\n",
    "**Partitioning** organizes data into subdirectories based on column values. This is critical for query performance on large datasets.\n",
    "\n",
    "Benefits:\n",
    "- Skip entire partitions when filtering\n",
    "- Parallel processing of partitions\n",
    "- Better organization for time-series data\n",
    "\n",
    "Common partitioning strategies:\n",
    "- **Time-based**: year/month/day\n",
    "- **Geographic**: region/country/city\n",
    "- **Categorical**: product_type/category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up previous partitioned data\n",
    "if partitioned_dir.exists():\n",
    "    shutil.rmtree(partitioned_dir)\n",
    "\n",
    "# Write partitioned dataset (by year and month)\n",
    "print(\"Writing partitioned dataset (partitioned by year and month)...\")\n",
    "df.to_parquet(\n",
    "    partitioned_dir,\n",
    "    partition_cols=['year', 'month'],\n",
    "    compression='snappy',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"\\nPartitioned directory structure:\")\n",
    "# Show directory structure\n",
    "for year_dir in sorted(partitioned_dir.glob('year=*')):\n",
    "    print(f\"  {year_dir.name}/\")\n",
    "    for month_dir in sorted(year_dir.glob('month=*')):\n",
    "        files = list(month_dir.glob('*.parquet'))\n",
    "        total_size = sum(f.stat().st_size for f in files)\n",
    "        print(f\"    {month_dir.name}/ ({len(files)} files, {total_size/1024**2:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Partitioned Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read entire partitioned dataset\n",
    "print(\"Reading entire partitioned dataset...\")\n",
    "start = time.time()\n",
    "df_all = pd.read_parquet(partitioned_dir)\n",
    "print(f\"Read {len(df_all):,} rows in {time.time()-start:.3f}s\")\n",
    "print(f\"Shape: {df_all.shape}\")\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read only specific partitions (using filters)\n",
    "print(\"Reading data only for year=2023, month=6...\")\n",
    "start = time.time()\n",
    "df_june = pd.read_parquet(\n",
    "    partitioned_dir,\n",
    "    filters=[('year', '=', 2023), ('month', '=', 6)]\n",
    ")\n",
    "print(f\"Read {len(df_june):,} rows in {time.time()-start:.3f}s\")\n",
    "print(f\"Date range: {df_june['timestamp'].min()} to {df_june['timestamp'].max()}\")\n",
    "df_june.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predicate Pushdown (Filtering During Read)\n",
    "\n",
    "**Predicate pushdown** applies filters at the file/row-group level, avoiding reading unnecessary data.\n",
    "\n",
    "This is much faster than:\n",
    "1. Reading entire file\n",
    "2. Then filtering in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHOUT predicate pushdown (read then filter)\n",
    "print(\"Method 1: Read entire file, then filter in pandas\")\n",
    "start = time.time()\n",
    "df_full = pd.read_parquet(output_file)\n",
    "df_filtered_python = df_full[df_full['region'] == 'North']\n",
    "time_without_pushdown = time.time() - start\n",
    "print(f\"  Time: {time_without_pushdown:.3f}s\")\n",
    "print(f\"  Rows: {len(df_filtered_python):,}\")\n",
    "\n",
    "# WITH predicate pushdown (filter during read)\n",
    "print(\"\\nMethod 2: Filter during read (predicate pushdown)\")\n",
    "start = time.time()\n",
    "df_filtered_pushdown = pd.read_parquet(\n",
    "    output_file,\n",
    "    filters=[('region', '=', 'North')]\n",
    ")\n",
    "time_with_pushdown = time.time() - start\n",
    "print(f\"  Time: {time_with_pushdown:.3f}s\")\n",
    "print(f\"  Rows: {len(df_filtered_pushdown):,}\")\n",
    "\n",
    "print(f\"\\nSpeedup: {time_without_pushdown/time_with_pushdown:.2f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Filters\n",
    "\n",
    "Parquet supports various filter operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple conditions (AND)\n",
    "df_complex = pd.read_parquet(\n",
    "    output_file,\n",
    "    filters=[\n",
    "        ('region', '=', 'North'),\n",
    "        ('amount', '>', 100),\n",
    "        ('product_category', 'in', ['Electronics', 'Books'])\n",
    "    ]\n",
    ")\n",
    "print(f\"Complex filter (North region, amount > 100, Electronics or Books):\")\n",
    "print(f\"  Rows: {len(df_complex):,}\")\n",
    "print(f\"  Average amount: ${df_complex['amount'].mean():.2f}\")\n",
    "\n",
    "# Show sample\n",
    "df_complex.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reading Multiple Parquet Files\n",
    "\n",
    "Common pattern: Read all Parquet files from a directory as a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple small files\n",
    "multi_file_dir = output_dir / 'multi_files'\n",
    "multi_file_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Split data into 4 files by region\n",
    "for region in df['region'].unique():\n",
    "    region_df = df[df['region'] == region]\n",
    "    region_df.to_parquet(multi_file_dir / f'data_{region}.parquet', index=False)\n",
    "    print(f\"Wrote {region}: {len(region_df):,} rows\")\n",
    "\n",
    "print(f\"\\nFiles created:\")\n",
    "for f in sorted(multi_file_dir.glob('*.parquet')):\n",
    "    print(f\"  {f.name} ({f.stat().st_size/1024**2:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all files in directory\n",
    "print(\"Reading all Parquet files in directory...\")\n",
    "df_combined = pd.read_parquet(multi_file_dir)\n",
    "print(f\"Total rows: {len(df_combined):,}\")\n",
    "print(f\"Regions: {df_combined['region'].unique()}\")\n",
    "print(f\"\\nRows per region:\")\n",
    "print(df_combined['region'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Specific Files with Glob Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read only North and South regions\n",
    "import glob\n",
    "\n",
    "north_south_files = [\n",
    "    str(multi_file_dir / 'data_North.parquet'),\n",
    "    str(multi_file_dir / 'data_South.parquet')\n",
    "]\n",
    "\n",
    "df_north_south = pd.concat([\n",
    "    pd.read_parquet(f) for f in north_south_files\n",
    "])\n",
    "\n",
    "print(f\"Read {len(df_north_south):,} rows from North and South\")\n",
    "print(df_north_south['region'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Row Groups and File Structure\n",
    "\n",
    "Parquet files are organized into **row groups**:\n",
    "- Each row group contains a chunk of rows\n",
    "- Stores min/max statistics per column\n",
    "- Enables predicate pushdown at row group level\n",
    "\n",
    "Smaller row groups = better filtering, larger overhead  \n",
    "Larger row groups = less overhead, coarser filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write with custom row group size\n",
    "file_small_groups = output_dir / 'small_row_groups.parquet'\n",
    "file_large_groups = output_dir / 'large_row_groups.parquet'\n",
    "\n",
    "# Small row groups (50k rows each)\n",
    "table = pa.Table.from_pandas(df)\n",
    "pq.write_table(\n",
    "    table,\n",
    "    file_small_groups,\n",
    "    row_group_size=50_000,\n",
    "    compression='snappy'\n",
    ")\n",
    "\n",
    "# Large row groups (500k rows each)\n",
    "pq.write_table(\n",
    "    table,\n",
    "    file_large_groups,\n",
    "    row_group_size=500_000,\n",
    "    compression='snappy'\n",
    ")\n",
    "\n",
    "# Compare metadata\n",
    "print(\"Small row groups file:\")\n",
    "pf_small = pq.ParquetFile(file_small_groups)\n",
    "print(f\"  Total rows: {pf_small.metadata.num_rows:,}\")\n",
    "print(f\"  Row groups: {pf_small.num_row_groups}\")\n",
    "print(f\"  File size: {file_small_groups.stat().st_size/1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\nLarge row groups file:\")\n",
    "pf_large = pq.ParquetFile(file_large_groups)\n",
    "print(f\"  Total rows: {pf_large.metadata.num_rows:,}\")\n",
    "print(f\"  Row groups: {pf_large.num_row_groups}\")\n",
    "print(f\"  File size: {file_large_groups.stat().st_size/1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Metadata and Statistics\n",
    "\n",
    "Parquet stores rich metadata without reading the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect file metadata\n",
    "parquet_file = pq.ParquetFile(output_file)\n",
    "\n",
    "print(\"File Metadata:\")\n",
    "print(f\"  Created by: {parquet_file.metadata.created_by}\")\n",
    "print(f\"  Total rows: {parquet_file.metadata.num_rows:,}\")\n",
    "print(f\"  Number of row groups: {parquet_file.num_row_groups}\")\n",
    "print(f\"  Number of columns: {parquet_file.metadata.num_columns}\")\n",
    "\n",
    "print(\"\\nSchema:\")\n",
    "print(parquet_file.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row group statistics (min/max values per column)\n",
    "print(\"Row Group Statistics (first row group):\")\n",
    "rg = parquet_file.metadata.row_group(0)\n",
    "\n",
    "print(f\"  Total rows in this row group: {rg.num_rows:,}\")\n",
    "print(f\"  Total byte size: {rg.total_byte_size/1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n  Column Statistics:\")\n",
    "for i in range(min(5, rg.num_columns)):  # Show first 5 columns\n",
    "    col = rg.column(i)\n",
    "    print(f\"    {col.path_in_schema}:\")\n",
    "    if col.statistics:\n",
    "        stats = col.statistics\n",
    "        print(f\"      Min: {stats.min}\")\n",
    "        print(f\"      Max: {stats.max}\")\n",
    "        print(f\"      Null count: {stats.null_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. PyArrow Compute Functions\n",
    "\n",
    "Process data without converting to pandas (more memory efficient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read as PyArrow table\n",
    "table = pq.read_table(output_file)\n",
    "\n",
    "print(f\"PyArrow Table:\")\n",
    "print(f\"  Rows: {table.num_rows:,}\")\n",
    "print(f\"  Columns: {table.num_columns}\")\n",
    "\n",
    "# Compute statistics using PyArrow (without pandas)\n",
    "amount_col = table['amount']\n",
    "\n",
    "print(\"\\nColumn statistics (computed with PyArrow):\")\n",
    "print(f\"  Sum: ${pc.sum(amount_col).as_py():,.2f}\")\n",
    "print(f\"  Mean: ${pc.mean(amount_col).as_py():.2f}\")\n",
    "print(f\"  Min: ${pc.min(amount_col).as_py():.2f}\")\n",
    "print(f\"  Max: ${pc.max(amount_col).as_py():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter using PyArrow compute\n",
    "filtered_table = table.filter(\n",
    "    (pc.field('region') == 'North') & (pc.field('amount') > 200)\n",
    ")\n",
    "\n",
    "print(f\"Filtered table (North region, amount > 200):\")\n",
    "print(f\"  Rows: {filtered_table.num_rows:,}\")\n",
    "\n",
    "# Convert to pandas only at the end\n",
    "filtered_df = filtered_table.to_pandas()\n",
    "print(f\"\\nSample data:\")\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Best Practices Summary\n",
    "\n",
    "### Partitioning Strategy\n",
    "1. **Partition by columns frequently used in filters** (e.g., date, region)\n",
    "2. **Avoid high cardinality** - don't partition by columns with millions of unique values\n",
    "3. **Aim for partition sizes of 100MB-1GB** - not too small, not too large\n",
    "4. **Hierarchical partitioning**: year/month/day (coarse to fine)\n",
    "\n",
    "### Row Group Size\n",
    "1. **Default (128MB) is usually good** for most workloads\n",
    "2. **Smaller row groups (50MB)** = better filtering, more overhead\n",
    "3. **Larger row groups (256MB+)** = less overhead, coarser filtering\n",
    "\n",
    "### Compression\n",
    "1. **Snappy**: Default, good balance (fast compression/decompression)\n",
    "2. **Zstd**: Better compression ratio, moderate speed\n",
    "3. **Gzip**: Good compression, slower than Snappy/Zstd\n",
    "4. **None**: Only for temporary files or when speed is critical\n",
    "\n",
    "### Reading\n",
    "1. **Always use column selection** - only read what you need\n",
    "2. **Use filters/predicate pushdown** - filter during read, not after\n",
    "3. **Process in batches** for large files - use `iter_batches()`\n",
    "4. **Leverage partitioning** - read only relevant partitions\n",
    "\n",
    "### Writing\n",
    "1. **Use chunked writing** for large datasets - `ParquetWriter`\n",
    "2. **Set appropriate row group size** based on query patterns\n",
    "3. **Partition strategically** - balance between too many/too few partitions\n",
    "4. **Don't include partition columns in data** - they're stored in directory names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "print(\"Notebook complete!\")\n",
    "print(f\"\\nCreated files in: {output_dir}\")\n",
    "print(f\"Total disk usage: {sum(f.stat().st_size for f in output_dir.rglob('*.parquet'))/1024**2:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
