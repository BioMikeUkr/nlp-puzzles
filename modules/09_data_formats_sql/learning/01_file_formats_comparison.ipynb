{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Formats Comparison: CSV, JSON, Parquet\n",
    "\n",
    "This notebook compares the three most common data formats used in data engineering and ML pipelines:\n",
    "\n",
    "- **CSV**: Text-based, human-readable, widely supported\n",
    "- **JSON**: Flexible schema, hierarchical data, human-readable\n",
    "- **Parquet**: Columnar binary format, optimized for analytics, compressed\n",
    "\n",
    "We'll compare:\n",
    "1. File sizes\n",
    "2. Read/write performance\n",
    "3. Compression options\n",
    "4. Column pruning (reading subset of columns)\n",
    "5. Use cases and trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path('../fixtures/output')\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"pandas version: {pd.__version__}\")\n",
    "print(f\"pyarrow version: {pa.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Sample Dataset\n",
    "\n",
    "We'll create a realistic e-commerce dataset with various data types:\n",
    "- Numeric columns (integers, floats)\n",
    "- Text columns (strings)\n",
    "- Categorical data\n",
    "- Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate sample e-commerce data (100,000 rows)\n",
    "n_rows = 100_000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'order_id': range(1, n_rows + 1),\n",
    "    'customer_id': np.random.randint(1000, 10000, n_rows),\n",
    "    'product_name': np.random.choice(['Laptop', 'Phone', 'Tablet', 'Headphones', 'Mouse', 'Keyboard'], n_rows),\n",
    "    'category': np.random.choice(['Electronics', 'Accessories', 'Computers'], n_rows),\n",
    "    'price': np.round(np.random.uniform(10, 2000, n_rows), 2),\n",
    "    'quantity': np.random.randint(1, 10, n_rows),\n",
    "    'discount_percent': np.round(np.random.uniform(0, 30, n_rows), 1),\n",
    "    'order_date': pd.date_range('2023-01-01', periods=n_rows, freq='5min'),\n",
    "    'customer_email': [f'user{i}@example.com' for i in np.random.randint(1000, 10000, n_rows)],\n",
    "    'shipping_country': np.random.choice(['USA', 'UK', 'Canada', 'Germany', 'France'], n_rows),\n",
    "    'notes': np.random.choice(['Gift wrap requested', 'Express shipping', None, 'Standard delivery'], n_rows)\n",
    "})\n",
    "\n",
    "# Calculate total\n",
    "df['total'] = df['price'] * df['quantity'] * (1 - df['discount_percent'] / 100)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. File Size Comparison\n",
    "\n",
    "Let's save the data in different formats and compare file sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV (uncompressed)\n",
    "csv_path = output_dir / 'data.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "# CSV (gzip compressed)\n",
    "csv_gz_path = output_dir / 'data.csv.gz'\n",
    "df.to_csv(csv_gz_path, index=False, compression='gzip')\n",
    "\n",
    "# JSON (uncompressed)\n",
    "json_path = output_dir / 'data.json'\n",
    "df.to_json(json_path, orient='records', date_format='iso')\n",
    "\n",
    "# JSON (gzip compressed)\n",
    "json_gz_path = output_dir / 'data.json.gz'\n",
    "df.to_json(json_gz_path, orient='records', date_format='iso', compression='gzip')\n",
    "\n",
    "# Parquet (snappy - default compression)\n",
    "parquet_snappy_path = output_dir / 'data_snappy.parquet'\n",
    "df.to_parquet(parquet_snappy_path, compression='snappy', index=False)\n",
    "\n",
    "# Parquet (gzip compression)\n",
    "parquet_gzip_path = output_dir / 'data_gzip.parquet'\n",
    "df.to_parquet(parquet_gzip_path, compression='gzip', index=False)\n",
    "\n",
    "# Parquet (zstd compression - best compression ratio)\n",
    "parquet_zstd_path = output_dir / 'data_zstd.parquet'\n",
    "df.to_parquet(parquet_zstd_path, compression='zstd', index=False)\n",
    "\n",
    "# Parquet (uncompressed)\n",
    "parquet_none_path = output_dir / 'data_none.parquet'\n",
    "df.to_parquet(parquet_none_path, compression=None, index=False)\n",
    "\n",
    "print(\"Files saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare file sizes\n",
    "def get_file_size_mb(path):\n",
    "    return os.path.getsize(path) / 1024**2\n",
    "\n",
    "sizes = {\n",
    "    'CSV': get_file_size_mb(csv_path),\n",
    "    'CSV (gzip)': get_file_size_mb(csv_gz_path),\n",
    "    'JSON': get_file_size_mb(json_path),\n",
    "    'JSON (gzip)': get_file_size_mb(json_gz_path),\n",
    "    'Parquet (snappy)': get_file_size_mb(parquet_snappy_path),\n",
    "    'Parquet (gzip)': get_file_size_mb(parquet_gzip_path),\n",
    "    'Parquet (zstd)': get_file_size_mb(parquet_zstd_path),\n",
    "    'Parquet (none)': get_file_size_mb(parquet_none_path),\n",
    "}\n",
    "\n",
    "size_df = pd.DataFrame(list(sizes.items()), columns=['Format', 'Size (MB)'])\n",
    "size_df['Compression Ratio'] = size_df['Size (MB)'].iloc[0] / size_df['Size (MB)']\n",
    "size_df = size_df.sort_values('Size (MB)')\n",
    "\n",
    "print(\"File Size Comparison:\")\n",
    "print(size_df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(size_df['Format'], size_df['Size (MB)'])\n",
    "plt.xlabel('File Size (MB)')\n",
    "plt.title('File Format Size Comparison')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations:\n",
    "\n",
    "1. **Parquet with compression (zstd/gzip)** typically gives the smallest file size\n",
    "2. **CSV is very large** without compression due to text representation of numbers\n",
    "3. **JSON is even larger** due to additional syntax (keys, braces, quotes)\n",
    "4. **Snappy compression** (default for Parquet) balances speed and compression ratio\n",
    "5. **Zstd** provides best compression but slower write speeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read Performance Comparison\n",
    "\n",
    "Let's benchmark how long it takes to read each format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_read(func, path, runs=5):\n",
    "    \"\"\"Time a read operation over multiple runs.\"\"\"\n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        start = time.time()\n",
    "        func(path)\n",
    "        end = time.time()\n",
    "        times.append(end - start)\n",
    "    return np.mean(times), np.std(times)\n",
    "\n",
    "# Benchmark reads\n",
    "read_times = {}\n",
    "\n",
    "print(\"Benchmarking read performance...\\n\")\n",
    "\n",
    "# CSV\n",
    "mean_time, std_time = time_read(pd.read_csv, csv_path)\n",
    "read_times['CSV'] = mean_time\n",
    "print(f\"CSV: {mean_time:.4f}s ± {std_time:.4f}s\")\n",
    "\n",
    "# CSV (gzip)\n",
    "mean_time, std_time = time_read(pd.read_csv, csv_gz_path)\n",
    "read_times['CSV (gzip)'] = mean_time\n",
    "print(f\"CSV (gzip): {mean_time:.4f}s ± {std_time:.4f}s\")\n",
    "\n",
    "# JSON\n",
    "mean_time, std_time = time_read(lambda p: pd.read_json(p, orient='records'), json_path)\n",
    "read_times['JSON'] = mean_time\n",
    "print(f\"JSON: {mean_time:.4f}s ± {std_time:.4f}s\")\n",
    "\n",
    "# JSON (gzip)\n",
    "mean_time, std_time = time_read(lambda p: pd.read_json(p, orient='records', compression='gzip'), json_gz_path)\n",
    "read_times['JSON (gzip)'] = mean_time\n",
    "print(f\"JSON (gzip): {mean_time:.4f}s ± {std_time:.4f}s\")\n",
    "\n",
    "# Parquet (snappy)\n",
    "mean_time, std_time = time_read(pd.read_parquet, parquet_snappy_path)\n",
    "read_times['Parquet (snappy)'] = mean_time\n",
    "print(f\"Parquet (snappy): {mean_time:.4f}s ± {std_time:.4f}s\")\n",
    "\n",
    "# Parquet (gzip)\n",
    "mean_time, std_time = time_read(pd.read_parquet, parquet_gzip_path)\n",
    "read_times['Parquet (gzip)'] = mean_time\n",
    "print(f\"Parquet (gzip): {mean_time:.4f}s ± {std_time:.4f}s\")\n",
    "\n",
    "# Parquet (zstd)\n",
    "mean_time, std_time = time_read(pd.read_parquet, parquet_zstd_path)\n",
    "read_times['Parquet (zstd)'] = mean_time\n",
    "print(f\"Parquet (zstd): {mean_time:.4f}s ± {std_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize read performance\n",
    "perf_df = pd.DataFrame(list(read_times.items()), columns=['Format', 'Read Time (s)'])\n",
    "perf_df = perf_df.sort_values('Read Time (s)')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(perf_df['Format'], perf_df['Read Time (s)'])\n",
    "plt.xlabel('Read Time (seconds)')\n",
    "plt.title('File Format Read Performance Comparison')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRead Performance Ranking:\")\n",
    "print(perf_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations:\n",
    "\n",
    "1. **Parquet is typically fastest** for reading, especially with snappy compression\n",
    "2. **CSV is slower** due to text parsing and type inference\n",
    "3. **JSON is slowest** due to complex parsing overhead\n",
    "4. **Compression adds overhead** but is often worth it for network/disk I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_write(func, path, df, runs=3):\n",
    "    \"\"\"Time a write operation over multiple runs.\"\"\"\n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        start = time.time()\n",
    "        func(df, path)\n",
    "        end = time.time()\n",
    "        times.append(end - start)\n",
    "    return np.mean(times), np.std(times)\n",
    "\n",
    "write_times = {}\n",
    "\n",
    "print(\"Benchmarking write performance...\\n\")\n",
    "\n",
    "# CSV\n",
    "mean_time, std_time = time_write(lambda d, p: d.to_csv(p, index=False), csv_path, df)\n",
    "write_times['CSV'] = mean_time\n",
    "print(f\"CSV: {mean_time:.4f}s ± {std_time:.4f}s\")\n",
    "\n",
    "# CSV (gzip)\n",
    "mean_time, std_time = time_write(lambda d, p: d.to_csv(p, index=False, compression='gzip'), csv_gz_path, df)\n",
    "write_times['CSV (gzip)'] = mean_time\n",
    "print(f\"CSV (gzip): {mean_time:.4f}s ± {std_time:.4f}s\")\n",
    "\n",
    "# Parquet (snappy)\n",
    "mean_time, std_time = time_write(lambda d, p: d.to_parquet(p, compression='snappy', index=False), parquet_snappy_path, df)\n",
    "write_times['Parquet (snappy)'] = mean_time\n",
    "print(f\"Parquet (snappy): {mean_time:.4f}s ± {std_time:.4f}s\")\n",
    "\n",
    "# Parquet (gzip)\n",
    "mean_time, std_time = time_write(lambda d, p: d.to_parquet(p, compression='gzip', index=False), parquet_gzip_path, df)\n",
    "write_times['Parquet (gzip)'] = mean_time\n",
    "print(f\"Parquet (gzip): {mean_time:.4f}s ± {std_time:.4f}s\")\n",
    "\n",
    "# Parquet (zstd)\n",
    "mean_time, std_time = time_write(lambda d, p: d.to_parquet(p, compression='zstd', index=False), parquet_zstd_path, df)\n",
    "write_times['Parquet (zstd)'] = mean_time\n",
    "print(f\"Parquet (zstd): {mean_time:.4f}s ± {std_time:.4f}s\")\n",
    "\n",
    "# Visualize\n",
    "write_df = pd.DataFrame(list(write_times.items()), columns=['Format', 'Write Time (s)'])\n",
    "write_df = write_df.sort_values('Write Time (s)')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(write_df['Format'], write_df['Write Time (s)'])\n",
    "plt.xlabel('Write Time (seconds)')\n",
    "plt.title('File Format Write Performance Comparison')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Column Pruning with Parquet\n",
    "\n",
    "One of Parquet's biggest advantages is **column pruning**: reading only the columns you need.\n",
    "\n",
    "This is extremely efficient because Parquet stores data in a **columnar format**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all columns vs specific columns\n",
    "print(\"Reading ALL columns from Parquet:\")\n",
    "start = time.time()\n",
    "df_all = pd.read_parquet(parquet_snappy_path)\n",
    "time_all = time.time() - start\n",
    "print(f\"Time: {time_all:.4f}s\")\n",
    "print(f\"Shape: {df_all.shape}\")\n",
    "print(f\"Memory: {df_all.memory_usage(deep=True).sum() / 1024**2:.2f} MB\\n\")\n",
    "\n",
    "# Read only 3 columns\n",
    "print(\"Reading ONLY 3 columns (order_id, product_name, total):\")\n",
    "start = time.time()\n",
    "df_subset = pd.read_parquet(parquet_snappy_path, columns=['order_id', 'product_name', 'total'])\n",
    "time_subset = time.time() - start\n",
    "print(f\"Time: {time_subset:.4f}s\")\n",
    "print(f\"Shape: {df_subset.shape}\")\n",
    "print(f\"Memory: {df_subset.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\nSpeedup: {time_all / time_subset:.2f}x faster\")\n",
    "print(f\"Memory savings: {(1 - time_subset/time_all) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with CSV (no column pruning)\n",
    "\n",
    "CSV must read the entire file even if you only want specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV with usecols parameter\n",
    "print(\"CSV with usecols (still has to scan entire file):\")\n",
    "start = time.time()\n",
    "df_csv_subset = pd.read_csv(csv_path, usecols=['order_id', 'product_name', 'total'])\n",
    "time_csv_subset = time.time() - start\n",
    "print(f\"Time: {time_csv_subset:.4f}s\")\n",
    "print(f\"\\nParquet is {time_csv_subset / time_subset:.2f}x faster for column pruning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compression Options Deep Dive\n",
    "\n",
    "Let's examine the trade-offs between different compression algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "compression_comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Compression': 'None',\n",
    "        'Size (MB)': get_file_size_mb(parquet_none_path),\n",
    "        'Write Time': write_times.get('Parquet (none)', 0),\n",
    "        'Read Time': read_times.get('Parquet (none)', 0),\n",
    "    },\n",
    "    {\n",
    "        'Compression': 'Snappy',\n",
    "        'Size (MB)': get_file_size_mb(parquet_snappy_path),\n",
    "        'Write Time': write_times.get('Parquet (snappy)', 0),\n",
    "        'Read Time': read_times.get('Parquet (snappy)', 0),\n",
    "    },\n",
    "    {\n",
    "        'Compression': 'Gzip',\n",
    "        'Size (MB)': get_file_size_mb(parquet_gzip_path),\n",
    "        'Write Time': write_times.get('Parquet (gzip)', 0),\n",
    "        'Read Time': read_times.get('Parquet (gzip)', 0),\n",
    "    },\n",
    "    {\n",
    "        'Compression': 'Zstd',\n",
    "        'Size (MB)': get_file_size_mb(parquet_zstd_path),\n",
    "        'Write Time': write_times.get('Parquet (zstd)', 0),\n",
    "        'Read Time': read_times.get('Parquet (zstd)', 0),\n",
    "    },\n",
    "])\n",
    "\n",
    "print(\"Parquet Compression Algorithm Comparison:\\n\")\n",
    "print(compression_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compression Algorithm Recommendations:\n",
    "\n",
    "| Algorithm | Speed | Compression | Use Case |\n",
    "|-----------|-------|-------------|----------|\n",
    "| **None** | Fastest | No compression | Local processing, temporary files |\n",
    "| **Snappy** | Very fast | Moderate | **Default choice**, good balance |\n",
    "| **Gzip** | Slower | Good | Network transfer, long-term storage |\n",
    "| **Zstd** | Moderate | Best | Best compression ratio, modern choice |\n",
    "\n",
    "**General recommendation**: Use **Snappy** for most cases, **Zstd** for storage optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Schema and Type Preservation\n",
    "\n",
    "Parquet preserves data types perfectly, while CSV requires type inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original dtypes\n",
    "print(\"Original DataFrame dtypes:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Read from CSV (loses type information)\n",
    "df_csv = pd.read_csv(csv_path)\n",
    "print(\"After CSV round-trip (notice order_date becomes string):\")\n",
    "print(df_csv.dtypes)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Read from Parquet (preserves types)\n",
    "df_parquet = pd.read_parquet(parquet_snappy_path)\n",
    "print(\"After Parquet round-trip (types preserved):\")\n",
    "print(df_parquet.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet schema inspection with PyArrow\n",
    "parquet_file = pq.ParquetFile(parquet_snappy_path)\n",
    "print(\"Parquet Schema (stored in file metadata):\")\n",
    "print(parquet_file.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Recommendations\n",
    "\n",
    "### When to Use Each Format:\n",
    "\n",
    "#### CSV\n",
    "- Human-readable debugging\n",
    "- Interoperability with non-technical tools (Excel)\n",
    "- Simple data exchange\n",
    "- When schema changes frequently\n",
    "\n",
    "**Pros**: Universal support, human-readable  \n",
    "**Cons**: Large file size, slow parsing, no type preservation, no column pruning\n",
    "\n",
    "#### JSON\n",
    "- Hierarchical/nested data structures\n",
    "- Web APIs and microservices\n",
    "- Configuration files\n",
    "- Flexible schemas\n",
    "\n",
    "**Pros**: Flexible schema, handles nested data, human-readable  \n",
    "**Cons**: Largest file size, slowest to parse, verbose syntax\n",
    "\n",
    "#### Parquet\n",
    "- **Data lakes and analytics**\n",
    "- **ML feature stores**\n",
    "- **Large-scale data processing**\n",
    "- Long-term data storage\n",
    "\n",
    "**Pros**: Small size, fast reads, column pruning, type preservation, excellent compression  \n",
    "**Cons**: Binary format (not human-readable), requires special tools\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Use Parquet for analytics pipelines** - it's optimized for read-heavy workloads\n",
    "2. **Use CSV for human interaction** - debugging, data sharing with non-technical users\n",
    "3. **Use JSON for APIs and nested data** - web services, configuration\n",
    "4. **Default to Snappy compression** for Parquet - best speed/compression trade-off\n",
    "5. **Use Zstd for archival storage** - best compression ratio\n",
    "6. **Always specify columns when reading Parquet** - leverage column pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# File size comparison (select formats)\n",
    "formats = ['CSV', 'JSON', 'Parquet (snappy)', 'Parquet (zstd)']\n",
    "sizes_selected = [sizes[f] for f in formats]\n",
    "axes[0].bar(formats, sizes_selected)\n",
    "axes[0].set_ylabel('Size (MB)')\n",
    "axes[0].set_title('File Size Comparison')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Read performance\n",
    "read_selected = [read_times[f] for f in formats]\n",
    "axes[1].bar(formats, read_selected)\n",
    "axes[1].set_ylabel('Time (seconds)')\n",
    "axes[1].set_title('Read Performance')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Write performance\n",
    "write_selected = [write_times.get(f, 0) for f in formats if f in write_times]\n",
    "axes[2].bar([f for f in formats if f in write_times], write_selected)\n",
    "axes[2].set_ylabel('Time (seconds)')\n",
    "axes[2].set_title('Write Performance')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
