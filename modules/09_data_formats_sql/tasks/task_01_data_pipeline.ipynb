{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: ETL Pipeline - CSV to Parquet to SQLite\n",
    "\n",
    "## Scenario\n",
    "You need to build a data pipeline that:\n",
    "1. Loads ticket data from CSV\n",
    "2. Transforms and cleans the data\n",
    "3. Writes to compressed Parquet format\n",
    "4. Creates partitioned Parquet files for efficient querying\n",
    "5. Loads the data into a SQLite database\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand CSV vs Parquet trade-offs (size, speed, schema)\n",
    "- Work with Parquet compression and partitioning\n",
    "- Create SQLite databases from pandas DataFrames\n",
    "- Validate data integrity across formats\n",
    "\n",
    "## Dataset\n",
    "Support tickets CSV with columns:\n",
    "- `ticket_id`, `user_id`, `category`, `description`\n",
    "- `created_at`, `resolved_at`, `priority`, `status`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup (Provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directories\n",
    "OUTPUT_DIR = Path(\"../fixtures/output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "PARTITIONED_DIR = OUTPUT_DIR / \"partitioned_tickets\"\n",
    "PARTITIONED_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1.1: Load and Explore CSV Data\n",
    "\n",
    "Load the tickets CSV file and explore its structure.\n",
    "\n",
    "**File location:** `../fixtures/input/tickets.csv`\n",
    "\n",
    "**Tasks:**\n",
    "1. Load the CSV into a DataFrame called `df`\n",
    "2. Parse `created_at` and `resolved_at` as datetime columns\n",
    "3. Display basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Load CSV with datetime parsing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST - Do not modify\n",
    "assert 'df' in dir(), \"DataFrame 'df' not found\"\n",
    "assert len(df) == 50, f\"Expected 50 tickets, got {len(df)}\"\n",
    "assert 'created_at' in df.columns, \"Missing 'created_at' column\"\n",
    "assert pd.api.types.is_datetime64_any_dtype(df['created_at']), \"created_at should be datetime type\"\n",
    "assert pd.api.types.is_datetime64_any_dtype(df['resolved_at']), \"resolved_at should be datetime type\"\n",
    "\n",
    "print(\"✓ Task 1.1 PASSED!\")\n",
    "print(f\"\\nLoaded {len(df)} tickets\")\n",
    "print(f\"Date range: {df['created_at'].min()} to {df['created_at'].max()}\")\n",
    "print(f\"\\nCategories: {df['category'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1.2: Data Transformation\n",
    "\n",
    "Add calculated columns for analysis:\n",
    "1. `resolution_hours`: Hours between created_at and resolved_at\n",
    "2. `month`: Month from created_at (as string like '2024-01')\n",
    "3. `category_code`: Numeric code for category (Technical=1, Billing=2, Account=3)\n",
    "\n",
    "Store the result in `df_transformed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create transformed DataFrame with new columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST - Do not modify\n",
    "assert 'df_transformed' in dir(), \"DataFrame 'df_transformed' not found\"\n",
    "assert len(df_transformed) == 50, \"Transformed df should have same number of rows\"\n",
    "assert 'resolution_hours' in df_transformed.columns, \"Missing 'resolution_hours' column\"\n",
    "assert 'month' in df_transformed.columns, \"Missing 'month' column\"\n",
    "assert 'category_code' in df_transformed.columns, \"Missing 'category_code' column\"\n",
    "\n",
    "# Check resolution_hours calculation\n",
    "first_row_hours = (df.iloc[0]['resolved_at'] - df.iloc[0]['created_at']).total_seconds() / 3600\n",
    "assert abs(df_transformed.iloc[0]['resolution_hours'] - first_row_hours) < 0.01, \\\n",
    "    \"resolution_hours calculation incorrect\"\n",
    "\n",
    "# Check month format\n",
    "assert df_transformed['month'].iloc[0] in ['2024-01', '2024-02', '2024-03', '2024-04', '2024-05'], \\\n",
    "    \"Month should be in format YYYY-MM\"\n",
    "\n",
    "# Check category codes\n",
    "assert set(df_transformed['category_code'].unique()) <= {1, 2, 3}, \\\n",
    "    \"category_code should only contain 1, 2, or 3\"\n",
    "\n",
    "print(\"✓ Task 1.2 PASSED!\")\n",
    "print(f\"\\nAverage resolution time: {df_transformed['resolution_hours'].mean():.2f} hours\")\n",
    "print(f\"Months covered: {sorted(df_transformed['month'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1.3: Write to Parquet with Compression\n",
    "\n",
    "Write the transformed data to Parquet format with compression.\n",
    "\n",
    "**Tasks:**\n",
    "1. Write `df_transformed` to `../fixtures/output/tickets.parquet`\n",
    "2. Use `snappy` compression\n",
    "3. Compare file sizes between CSV and Parquet\n",
    "\n",
    "**Hint:** Use `df.to_parquet()` with `compression` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Write to Parquet with snappy compression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST - Do not modify\n",
    "parquet_path = OUTPUT_DIR / \"tickets.parquet\"\n",
    "assert parquet_path.exists(), \"Parquet file not created\"\n",
    "\n",
    "# Load back and verify\n",
    "df_loaded = pd.read_parquet(parquet_path)\n",
    "assert len(df_loaded) == 50, \"Parquet file should contain 50 rows\"\n",
    "assert list(df_loaded.columns) == list(df_transformed.columns), \"Columns don't match\"\n",
    "\n",
    "# Compare file sizes\n",
    "csv_size = os.path.getsize(\"../fixtures/input/tickets.csv\")\n",
    "parquet_size = os.path.getsize(parquet_path)\n",
    "\n",
    "print(\"✓ Task 1.3 PASSED!\")\n",
    "print(f\"\\nFile size comparison:\")\n",
    "print(f\"  CSV:     {csv_size:,} bytes\")\n",
    "print(f\"  Parquet: {parquet_size:,} bytes\")\n",
    "print(f\"  Reduction: {(1 - parquet_size/csv_size)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1.4: Create Partitioned Parquet Files\n",
    "\n",
    "Create partitioned Parquet files organized by month and category.\n",
    "\n",
    "**Structure:**\n",
    "```\n",
    "partitioned_tickets/\n",
    "  month=2024-01/\n",
    "    category=Technical/\n",
    "      data.parquet\n",
    "    category=Billing/\n",
    "      data.parquet\n",
    "  month=2024-02/\n",
    "    ...\n",
    "```\n",
    "\n",
    "**Hint:** Use `df.to_parquet()` with `partition_cols` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create partitioned Parquet files by month and category\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST - Do not modify\n",
    "# Check that partitioned directory has data\n",
    "partition_files = list(PARTITIONED_DIR.rglob(\"*.parquet\"))\n",
    "assert len(partition_files) > 0, \"No partitioned Parquet files created\"\n",
    "\n",
    "# Verify partition structure\n",
    "month_dirs = [d for d in PARTITIONED_DIR.iterdir() if d.is_dir() and d.name.startswith(\"month=\")]\n",
    "assert len(month_dirs) > 0, \"No month partitions found\"\n",
    "\n",
    "# Load back and verify data integrity\n",
    "df_partitioned = pd.read_parquet(PARTITIONED_DIR)\n",
    "assert len(df_partitioned) == 50, \"Partitioned data should have 50 rows\"\n",
    "\n",
    "# Verify we can filter by partition\n",
    "df_jan = pd.read_parquet(PARTITIONED_DIR, filters=[('month', '=', '2024-01')])\n",
    "assert len(df_jan) > 0, \"Should have January data\"\n",
    "assert all(df_jan['month'] == '2024-01'), \"Partition filter not working\"\n",
    "\n",
    "print(\"✓ Task 1.4 PASSED!\")\n",
    "print(f\"\\nCreated {len(partition_files)} partition files\")\n",
    "print(f\"Months: {sorted([d.name for d in month_dirs])}\")\n",
    "print(f\"January tickets: {len(df_jan)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1.5: Create SQLite Database\n",
    "\n",
    "Load the data into a SQLite database with proper schema.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create connection to `../fixtures/output/tickets.db`\n",
    "2. Write `df_transformed` to table named `tickets`\n",
    "3. Create an index on `user_id` for faster queries\n",
    "4. Create an index on `category` for filtering\n",
    "\n",
    "**Hint:** Use `df.to_sql()` and `conn.execute()` for indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create SQLite database and load data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST - Do not modify\n",
    "db_path = OUTPUT_DIR / \"tickets.db\"\n",
    "assert db_path.exists(), \"Database file not created\"\n",
    "\n",
    "# Verify table and data\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Check table exists\n",
    "tables = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table'\", conn)\n",
    "assert 'tickets' in tables['name'].values, \"Table 'tickets' not found\"\n",
    "\n",
    "# Check row count\n",
    "count = pd.read_sql(\"SELECT COUNT(*) as cnt FROM tickets\", conn)['cnt'].iloc[0]\n",
    "assert count == 50, f\"Expected 50 rows in database, got {count}\"\n",
    "\n",
    "# Check indexes exist\n",
    "indexes = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='index'\", conn)\n",
    "index_names = indexes['name'].tolist()\n",
    "has_user_index = any('user_id' in idx.lower() for idx in index_names)\n",
    "has_category_index = any('category' in idx.lower() for idx in index_names)\n",
    "\n",
    "assert has_user_index, \"Index on user_id not found\"\n",
    "assert has_category_index, \"Index on category not found\"\n",
    "\n",
    "# Verify data integrity\n",
    "sample = pd.read_sql(\"SELECT * FROM tickets LIMIT 5\", conn)\n",
    "assert len(sample) == 5, \"Could not query data\"\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"✓ Task 1.5 PASSED!\")\n",
    "print(f\"\\nDatabase created with:\")\n",
    "print(f\"  Rows: {count}\")\n",
    "print(f\"  Indexes: {[idx for idx in index_names if 'auto' not in idx.lower()]}\")\n",
    "print(f\"\\nSample query result:\")\n",
    "print(sample[['ticket_id', 'category', 'priority']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1.6: Validate Data Integrity\n",
    "\n",
    "Verify that data is consistent across all formats.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load data from Parquet and database\n",
    "2. Compare row counts\n",
    "3. Verify ticket_id values match\n",
    "4. Check that resolution_hours are consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Load from both sources and compare\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST - Do not modify\n",
    "parquet_data = pd.read_parquet(OUTPUT_DIR / \"tickets.parquet\")\n",
    "db_conn = sqlite3.connect(OUTPUT_DIR / \"tickets.db\")\n",
    "db_data = pd.read_sql(\"SELECT * FROM tickets\", db_conn)\n",
    "db_conn.close()\n",
    "\n",
    "# Compare row counts\n",
    "assert len(parquet_data) == len(db_data), \"Row counts don't match\"\n",
    "\n",
    "# Compare ticket IDs (sorted)\n",
    "parquet_ids = sorted(parquet_data['ticket_id'].tolist())\n",
    "db_ids = sorted(db_data['ticket_id'].tolist())\n",
    "assert parquet_ids == db_ids, \"Ticket IDs don't match\"\n",
    "\n",
    "# Compare resolution hours for first 10 tickets\n",
    "parquet_sorted = parquet_data.sort_values('ticket_id').reset_index(drop=True)\n",
    "db_sorted = db_data.sort_values('ticket_id').reset_index(drop=True)\n",
    "\n",
    "hours_match = all(\n",
    "    abs(parquet_sorted.loc[i, 'resolution_hours'] - db_sorted.loc[i, 'resolution_hours']) < 0.01\n",
    "    for i in range(10)\n",
    ")\n",
    "assert hours_match, \"Resolution hours don't match between formats\"\n",
    "\n",
    "print(\"✓ Task 1.6 PASSED!\")\n",
    "print(\"\\n✅ Data integrity verified across all formats!\")\n",
    "print(f\"\\nPipeline summary:\")\n",
    "print(f\"  CSV → Parquet: {len(parquet_data)} rows\")\n",
    "print(f\"  CSV → SQLite:  {len(db_data)} rows\")\n",
    "print(f\"  All formats consistent: ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Congratulations! You've built a complete ETL pipeline:\n",
    "\n",
    "✅ Loaded CSV data with proper datetime parsing\n",
    "✅ Transformed data with calculated columns\n",
    "✅ Wrote compressed Parquet files (smaller storage)\n",
    "✅ Created partitioned Parquet for efficient queries\n",
    "✅ Loaded data into SQLite with indexes\n",
    "✅ Validated data integrity across formats\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "**Format Comparison:**\n",
    "- **CSV**: Human-readable, universally compatible, larger size\n",
    "- **Parquet**: Columnar, compressed, faster queries, schema enforcement\n",
    "- **SQLite**: SQL queries, ACID transactions, indexes for fast lookups\n",
    "\n",
    "**Partitioning Benefits:**\n",
    "- Only read relevant data (partition pruning)\n",
    "- Parallel processing of partitions\n",
    "- Easier data management (delete old months)\n",
    "\n",
    "**When to use what:**\n",
    "- **CSV**: Sharing data, manual inspection, compatibility\n",
    "- **Parquet**: Analytics, data lakes, large datasets\n",
    "- **SQLite**: Application databases, complex queries, transactions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
