{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Task 01 Solution: Data Pipeline with Multiple Formats\n",
    "\n",
    "Complete solutions for building an ETL pipeline working with CSV, JSON, JSONL, and Parquet formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import jsonlines\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Task 1.1: Generate Sample E-commerce Transaction Data\n",
    "\n",
    "Create 100,000 transaction records with realistic patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Generate synthetic transaction data\n",
    "np.random.seed(42)\n",
    "\n",
    "n_transactions = 100_000\n",
    "\n",
    "# Generate data\n",
    "df = pd.DataFrame({\n",
    "    'transaction_id': range(1, n_transactions + 1),\n",
    "    'user_id': np.random.randint(1000, 50000, n_transactions),\n",
    "    'product_id': np.random.randint(1, 1000, n_transactions),\n",
    "    'category': np.random.choice(['Electronics', 'Clothing', 'Home', 'Books', 'Sports'], n_transactions),\n",
    "    'amount': np.random.uniform(10, 1000, n_transactions).round(2),\n",
    "    'quantity': np.random.randint(1, 10, n_transactions),\n",
    "    'timestamp': pd.date_range('2024-01-01', periods=n_transactions, freq='5min'),\n",
    "    'payment_method': np.random.choice(['credit_card', 'debit_card', 'paypal', 'crypto'], n_transactions),\n",
    "    'country': np.random.choice(['US', 'UK', 'CA', 'DE', 'FR', 'JP'], n_transactions)\n",
    "})\n",
    "\n",
    "# Add derived columns\n",
    "df['total_amount'] = df['amount'] * df['quantity']\n",
    "df['year'] = df['timestamp'].dt.year\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['day'] = df['timestamp'].dt.day\n",
    "\n",
    "print(f\"Generated {len(df):,} transactions\")\n",
    "print(f\"\\nDataFrame info:\")\n",
    "print(df.info())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Verify\n",
    "assert len(df) == 100_000, \"Should have 100k transactions\"\n",
    "assert df['amount'].min() >= 10, \"Amount should be >= 10\"\n",
    "assert df['amount'].max() <= 1000, \"Amount should be <= 1000\"\n",
    "print(\"\\n✅ Data generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Task 1.2: Save to CSV and Measure Size\n",
    "\n",
    "Export to CSV and check file size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Save to CSV\n",
    "csv_path = 'transactions.csv'\n",
    "\n",
    "start_time = time.time()\n",
    "df.to_csv(csv_path, index=False)\n",
    "csv_write_time = time.time() - start_time\n",
    "\n",
    "csv_size_mb = os.path.getsize(csv_path) / (1024**2)\n",
    "\n",
    "print(f\"CSV write time: {csv_write_time:.2f}s\")\n",
    "print(f\"CSV file size: {csv_size_mb:.2f} MB\")\n",
    "\n",
    "# Test read speed\n",
    "start_time = time.time()\n",
    "df_csv = pd.read_csv(csv_path)\n",
    "csv_read_time = time.time() - start_time\n",
    "\n",
    "print(f\"CSV read time: {csv_read_time:.2f}s\")\n",
    "\n",
    "# Verify\n",
    "assert os.path.exists(csv_path), \"CSV file should exist\"\n",
    "assert len(df_csv) == len(df), \"Should read same number of rows\"\n",
    "print(\"\\n✅ CSV operations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Task 1.3: Convert to Parquet with Different Compressions\n",
    "\n",
    "Compare Snappy, Gzip, and Zstd compression algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Benchmark different compressions\n",
    "results = []\n",
    "\n",
    "for compression in ['snappy', 'gzip', 'zstd']:\n",
    "    parquet_path = f'transactions_{compression}.parquet'\n",
    "    \n",
    "    # Write\n",
    "    start_time = time.time()\n",
    "    df.to_parquet(parquet_path, compression=compression, index=False)\n",
    "    write_time = time.time() - start_time\n",
    "    \n",
    "    # Read all columns\n",
    "    start_time = time.time()\n",
    "    df_read = pd.read_parquet(parquet_path)\n",
    "    read_all_time = time.time() - start_time\n",
    "    \n",
    "    # Read subset of columns (column pruning)\n",
    "    start_time = time.time()\n",
    "    df_subset = pd.read_parquet(parquet_path, columns=['user_id', 'amount', 'category'])\n",
    "    read_subset_time = time.time() - start_time\n",
    "    \n",
    "    # File size\n",
    "    file_size_mb = os.path.getsize(parquet_path) / (1024**2)\n",
    "    compression_ratio = csv_size_mb / file_size_mb\n",
    "    \n",
    "    results.append({\n",
    "        'compression': compression,\n",
    "        'write_time': f\"{write_time:.2f}s\",\n",
    "        'read_all_time': f\"{read_all_time:.2f}s\",\n",
    "        'read_subset_time': f\"{read_subset_time:.2f}s\",\n",
    "        'file_size_mb': f\"{file_size_mb:.2f} MB\",\n",
    "        'compression_ratio': f\"{compression_ratio:.2f}x\"\n",
    "    })\n",
    "\n",
    "# Display comparison\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nParquet Compression Benchmark:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nCSV baseline: {csv_size_mb:.2f} MB, read time: {csv_read_time:.2f}s\")\n",
    "\n",
    "# Verify\n",
    "assert len(df_read) == len(df), \"Should preserve all rows\"\n",
    "assert len(df_subset.columns) == 3, \"Subset should have 3 columns\"\n",
    "print(\"\\n✅ Parquet compression benchmark completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Task 1.4: Create Partitioned Parquet Dataset\n",
    "\n",
    "Partition by year and month for faster filtered queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Write partitioned Parquet\n",
    "partitioned_path = 'transactions_partitioned/'\n",
    "\n",
    "# Remove existing directory if present\n",
    "import shutil\n",
    "if os.path.exists(partitioned_path):\n",
    "    shutil.rmtree(partitioned_path)\n",
    "\n",
    "# Write with partitioning\n",
    "df.to_parquet(\n",
    "    partitioned_path,\n",
    "    partition_cols=['year', 'month'],\n",
    "    compression='snappy',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# List partition structure\n",
    "print(\"Partition structure:\")\n",
    "for root, dirs, files in os.walk(partitioned_path):\n",
    "    level = root.replace(partitioned_path, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    sub_indent = ' ' * 2 * (level + 1)\n",
    "    for file in files[:3]:  # Show first 3 files per directory\n",
    "        print(f\"{sub_indent}{file}\")\n",
    "    if len(files) > 3:\n",
    "        print(f\"{sub_indent}... and {len(files) - 3} more files\")\n",
    "\n",
    "# Test reading with filters (predicate pushdown)\n",
    "start_time = time.time()\n",
    "df_filtered = pd.read_parquet(\n",
    "    partitioned_path,\n",
    "    filters=[('year', '==', 2024), ('month', '==', 1)]\n",
    ")\n",
    "filter_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nFiltered read (year=2024, month=1): {filter_time:.3f}s\")\n",
    "print(f\"Rows returned: {len(df_filtered):,}\")\n",
    "\n",
    "# Verify\n",
    "assert os.path.exists(partitioned_path), \"Partitioned directory should exist\"\n",
    "assert len(df_filtered) > 0, \"Should return some rows\"\n",
    "assert df_filtered['year'].nunique() == 1, \"Should only contain year 2024\"\n",
    "assert df_filtered['month'].nunique() == 1, \"Should only contain month 1\"\n",
    "print(\"\\n✅ Partitioned Parquet created and tested!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Task 1.5: Work with JSONL Format\n",
    "\n",
    "Convert transaction data to JSONL for streaming processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Write to JSONL\n",
    "jsonl_path = 'transactions.jsonl'\n",
    "\n",
    "# Convert to JSONL (sample first 10k for reasonable file size)\n",
    "df_sample = df.head(10_000).copy()\n",
    "df_sample['timestamp'] = df_sample['timestamp'].astype(str)\n",
    "\n",
    "start_time = time.time()\n",
    "with jsonlines.open(jsonl_path, mode='w') as writer:\n",
    "    for _, row in df_sample.iterrows():\n",
    "        writer.write(row.to_dict())\n",
    "jsonl_write_time = time.time() - start_time\n",
    "\n",
    "jsonl_size_mb = os.path.getsize(jsonl_path) / (1024**2)\n",
    "\n",
    "print(f\"JSONL write time: {jsonl_write_time:.2f}s\")\n",
    "print(f\"JSONL file size: {jsonl_size_mb:.2f} MB\")\n",
    "\n",
    "# Read JSONL in streaming fashion\n",
    "start_time = time.time()\n",
    "records = []\n",
    "with jsonlines.open(jsonl_path) as reader:\n",
    "    for obj in reader:\n",
    "        records.append(obj)\n",
    "        if len(records) >= 1000:  # Stream first 1000\n",
    "            break\n",
    "jsonl_read_time = time.time() - start_time\n",
    "\n",
    "print(f\"JSONL streaming read (1000 records): {jsonl_read_time:.3f}s\")\n",
    "\n",
    "# Show sample record\n",
    "print(\"\\nSample JSONL record:\")\n",
    "print(json.dumps(records[0], indent=2))\n",
    "\n",
    "# Verify\n",
    "assert os.path.exists(jsonl_path), \"JSONL file should exist\"\n",
    "assert len(records) == 1000, \"Should read 1000 records\"\n",
    "assert 'transaction_id' in records[0], \"Should contain transaction_id\"\n",
    "print(\"\\n✅ JSONL operations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Task 1.6: Process Nested JSON\n",
    "\n",
    "Handle hierarchical JSON data with nested structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Create nested JSON structure\n",
    "nested_data = []\n",
    "\n",
    "for i in range(100):\n",
    "    transaction = {\n",
    "        'transaction_id': i + 1,\n",
    "        'user': {\n",
    "            'user_id': np.random.randint(1000, 5000),\n",
    "            'name': f\"User_{np.random.randint(1000, 5000)}\",\n",
    "            'email': f\"user{np.random.randint(1000, 5000)}@example.com\",\n",
    "            'address': {\n",
    "                'city': np.random.choice(['NYC', 'LA', 'Chicago', 'Houston']),\n",
    "                'country': np.random.choice(['US', 'UK', 'CA'])\n",
    "            }\n",
    "        },\n",
    "        'items': [\n",
    "            {\n",
    "                'product_id': np.random.randint(1, 100),\n",
    "                'quantity': np.random.randint(1, 5),\n",
    "                'price': round(np.random.uniform(10, 500), 2)\n",
    "            }\n",
    "            for _ in range(np.random.randint(1, 4))\n",
    "        ],\n",
    "        'payment': {\n",
    "            'method': np.random.choice(['credit_card', 'paypal']),\n",
    "            'status': 'completed'\n",
    "        },\n",
    "        'timestamp': '2024-01-01T10:00:00'\n",
    "    }\n",
    "    nested_data.append(transaction)\n",
    "\n",
    "# Save nested JSON\n",
    "nested_json_path = 'transactions_nested.json'\n",
    "with open(nested_json_path, 'w') as f:\n",
    "    json.dump(nested_data, f, indent=2)\n",
    "\n",
    "print(\"Sample nested transaction:\")\n",
    "print(json.dumps(nested_data[0], indent=2))\n",
    "\n",
    "# Solution: Flatten nested JSON using pandas\n",
    "df_nested = pd.json_normalize(\n",
    "    nested_data,\n",
    "    record_path='items',\n",
    "    meta=[\n",
    "        'transaction_id',\n",
    "        ['user', 'user_id'],\n",
    "        ['user', 'name'],\n",
    "        ['user', 'address', 'city'],\n",
    "        ['user', 'address', 'country'],\n",
    "        ['payment', 'method'],\n",
    "        'timestamp'\n",
    "    ],\n",
    "    meta_prefix='',\n",
    "    record_prefix='item_'\n",
    ")\n",
    "\n",
    "print(\"\\nFlattened DataFrame:\")\n",
    "print(df_nested.head())\n",
    "print(f\"\\nColumns: {df_nested.columns.tolist()}\")\n",
    "\n",
    "# Verify\n",
    "assert len(df_nested) > len(nested_data), \"Should have more rows (items expanded)\"\n",
    "assert 'user.user_id' in df_nested.columns, \"Should have flattened user columns\"\n",
    "assert 'item_product_id' in df_nested.columns, \"Should have item columns\"\n",
    "print(\"\\n✅ Nested JSON processed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Task 1.7: Chunked CSV to Parquet Conversion\n",
    "\n",
    "Process large CSV files in chunks to avoid memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Chunked conversion function\n",
    "def csv_to_parquet_chunked(csv_path, parquet_path, chunksize=10_000):\n",
    "    \"\"\"\n",
    "    Convert CSV to Parquet in chunks to avoid memory issues.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Input CSV file\n",
    "        parquet_path: Output Parquet file\n",
    "        chunksize: Rows per chunk\n",
    "    \"\"\"\n",
    "    writer = None\n",
    "    schema = None\n",
    "    total_rows = 0\n",
    "    \n",
    "    try:\n",
    "        # Read CSV in chunks\n",
    "        chunks = pd.read_csv(csv_path, chunksize=chunksize)\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Convert chunk to Arrow Table\n",
    "            table = pa.Table.from_pandas(chunk, preserve_index=False)\n",
    "            \n",
    "            if writer is None:\n",
    "                # Initialize writer on first chunk\n",
    "                schema = table.schema\n",
    "                writer = pq.ParquetWriter(\n",
    "                    parquet_path,\n",
    "                    schema,\n",
    "                    compression='snappy'\n",
    "                )\n",
    "            \n",
    "            # Write chunk\n",
    "            writer.write_table(table)\n",
    "            total_rows += len(chunk)\n",
    "            \n",
    "            if (i + 1) % 5 == 0:\n",
    "                print(f\"Processed {total_rows:,} rows...\")\n",
    "    \n",
    "    finally:\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "    \n",
    "    return total_rows\n",
    "\n",
    "# Test chunked conversion\n",
    "chunked_parquet_path = 'transactions_chunked.parquet'\n",
    "\n",
    "print(\"Converting CSV to Parquet in chunks...\")\n",
    "start_time = time.time()\n",
    "total_rows = csv_to_parquet_chunked(csv_path, chunked_parquet_path, chunksize=20_000)\n",
    "conversion_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nConversion completed in {conversion_time:.2f}s\")\n",
    "print(f\"Total rows processed: {total_rows:,}\")\n",
    "\n",
    "# Verify the output\n",
    "df_verify = pd.read_parquet(chunked_parquet_path)\n",
    "print(f\"Rows in output file: {len(df_verify):,}\")\n",
    "\n",
    "# Verify\n",
    "assert len(df_verify) == total_rows, \"Should preserve all rows\"\n",
    "assert len(df_verify) == len(df), \"Should match original data\"\n",
    "print(\"\\n✅ Chunked conversion successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Task 1.8: Format Comparison Summary\n",
    "\n",
    "Create a comprehensive comparison of all formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Compare all formats\n",
    "comparison = []\n",
    "\n",
    "# CSV\n",
    "comparison.append({\n",
    "    'Format': 'CSV',\n",
    "    'Size (MB)': f\"{csv_size_mb:.2f}\",\n",
    "    'Read Time': f\"{csv_read_time:.2f}s\",\n",
    "    'Write Time': f\"{csv_write_time:.2f}s\",\n",
    "    'Human Readable': 'Yes',\n",
    "    'Column Pruning': 'No',\n",
    "    'Compression': 'No'\n",
    "})\n",
    "\n",
    "# Parquet (Snappy)\n",
    "snappy_size = os.path.getsize('transactions_snappy.parquet') / (1024**2)\n",
    "comparison.append({\n",
    "    'Format': 'Parquet (Snappy)',\n",
    "    'Size (MB)': f\"{snappy_size:.2f}\",\n",
    "    'Read Time': results_df[results_df['compression'] == 'snappy']['read_all_time'].values[0],\n",
    "    'Write Time': results_df[results_df['compression'] == 'snappy']['write_time'].values[0],\n",
    "    'Human Readable': 'No',\n",
    "    'Column Pruning': 'Yes',\n",
    "    'Compression': 'Snappy'\n",
    "})\n",
    "\n",
    "# Parquet (Gzip)\n",
    "gzip_size = os.path.getsize('transactions_gzip.parquet') / (1024**2)\n",
    "comparison.append({\n",
    "    'Format': 'Parquet (Gzip)',\n",
    "    'Size (MB)': f\"{gzip_size:.2f}\",\n",
    "    'Read Time': results_df[results_df['compression'] == 'gzip']['read_all_time'].values[0],\n",
    "    'Write Time': results_df[results_df['compression'] == 'gzip']['write_time'].values[0],\n",
    "    'Human Readable': 'No',\n",
    "    'Column Pruning': 'Yes',\n",
    "    'Compression': 'Gzip'\n",
    "})\n",
    "\n",
    "# JSONL\n",
    "comparison.append({\n",
    "    'Format': 'JSONL',\n",
    "    'Size (MB)': f\"{jsonl_size_mb:.2f}\",\n",
    "    'Read Time': f\"{jsonl_read_time:.3f}s\",\n",
    "    'Write Time': f\"{jsonl_write_time:.2f}s\",\n",
    "    'Human Readable': 'Yes',\n",
    "    'Column Pruning': 'No',\n",
    "    'Compression': 'Optional'\n",
    "})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FILE FORMAT COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"1. Parquet (Snappy) is {csv_size_mb/snappy_size:.1f}x smaller than CSV\")\n",
    "print(f\"2. Parquet (Gzip) is {csv_size_mb/gzip_size:.1f}x smaller than CSV\")\n",
    "print(f\"3. Parquet supports column pruning (read only needed columns)\")\n",
    "print(f\"4. JSONL is best for streaming line-by-line processing\")\n",
    "print(f\"5. CSV is human-readable but inefficient for large data\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATIONS:\")\n",
    "print(\"=\"*80)\n",
    "print(\"• Use Parquet (Snappy) for ML training data (fast reads)\")\n",
    "print(\"• Use Parquet (Gzip) for archival/cold storage (max compression)\")\n",
    "print(\"• Use JSONL for streaming ETL pipelines\")\n",
    "print(\"• Use CSV only for small datasets or human review\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n✅ All tasks completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **CSV**: Simple but large and slow\n",
    "2. **Parquet**: Columnar, compressed, fast for analytics\n",
    "3. **Compression**: Snappy (fast) vs Gzip (small) vs Zstd (balanced)\n",
    "4. **Partitioning**: Faster filtered queries via predicate pushdown\n",
    "5. **JSONL**: Streaming processing for large files\n",
    "6. **Nested JSON**: Flattening hierarchical data\n",
    "7. **Chunked Processing**: Handle files larger than memory\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Choose format based on use case (size, speed, readability)\n",
    "- Parquet is best for ML workflows\n",
    "- Always benchmark before committing to a format\n",
    "- Use partitioning for frequently filtered columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
