{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Build FAISS Search System - SOLUTION\n",
    "\n",
    "Implement semantic search using FAISS and sentence-transformers.\n",
    "\n",
    "**Goals:**\n",
    "- Create FAISS index for ticket corpus\n",
    "- Implement search function\n",
    "- Compare Flat vs IVF vs HNSW performance\n",
    "- Implement metadata filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load ticket dataset from fixtures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tickets\n",
    "with open('../fixtures/input/tickets.json', 'r') as f:\n",
    "    tickets = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(tickets)} tickets\")\n",
    "print(f\"\\nSample ticket:\")\n",
    "print(json.dumps(tickets[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Generate Embeddings\n",
    "\n",
    "Use sentence-transformers to embed ticket titles and descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "# 1. Load sentence-transformers model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 2. Create list of texts (title + description for each ticket)\n",
    "texts = [f\"{ticket['title']}. {ticket['description']}\" for ticket in tickets]\n",
    "\n",
    "# 3. Generate embeddings with normalize_embeddings=True\n",
    "# 4. Convert to float32\n",
    "embeddings = model.encode(texts, normalize_embeddings=True, show_progress_bar=True)\n",
    "embeddings = embeddings.astype('float32')\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Embeddings dtype: {embeddings.dtype}\")\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert model is not None, \"Model not loaded\"\n",
    "assert len(texts) == len(tickets), f\"Expected {len(tickets)} texts, got {len(texts)}\"\n",
    "assert embeddings is not None, \"Embeddings not generated\"\n",
    "assert embeddings.shape == (len(tickets), 384), f\"Wrong shape: {embeddings.shape}\"\n",
    "assert embeddings.dtype == np.float32, f\"Wrong dtype: {embeddings.dtype}\"\n",
    "# Check normalization\n",
    "norms = np.linalg.norm(embeddings, axis=1)\n",
    "assert np.allclose(norms, 1.0, atol=1e-5), \"Embeddings not normalized\"\n",
    "print(\"✓ Task 1 passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Create FAISS Index\n",
    "\n",
    "Build IndexFlatIP and add embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "# 1. Create IndexFlatIP with correct dimension\n",
    "dimension = embeddings.shape[1]  # 384\n",
    "index_flat = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "# 2. Add embeddings to index\n",
    "index_flat.add(embeddings)\n",
    "\n",
    "print(f\"Index contains {index_flat.ntotal} vectors\")\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert index_flat is not None, \"Index not created\"\n",
    "assert index_flat.ntotal == len(tickets), f\"Expected {len(tickets)} vectors, got {index_flat.ntotal}\"\n",
    "assert index_flat.d == 384, f\"Wrong dimension: {index_flat.d}\"\n",
    "print(\"✓ Task 2 passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Implement Search Function\n",
    "\n",
    "Create function to search tickets by text query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "def search_tickets(query_text, k=5):\n",
    "    \"\"\"\n",
    "    Search for similar tickets.\n",
    "    \n",
    "    Args:\n",
    "        query_text: Text query\n",
    "        k: Number of results\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts with 'ticket', 'score' keys\n",
    "    \"\"\"\n",
    "    # 1. Encode query text with model (normalized, float32, 2D)\n",
    "    query_embedding = model.encode([query_text], normalize_embeddings=True).astype('float32')\n",
    "    \n",
    "    # 2. Search index\n",
    "    scores, indices = index_flat.search(query_embedding, k)\n",
    "    \n",
    "    # 3. Format results as list of dicts\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        results.append({\n",
    "            'ticket': tickets[idx],\n",
    "            'score': float(score)\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# TEST - Do not modify\n",
    "results = search_tickets(\"password reset issue\", k=3)\n",
    "assert len(results) == 3, f\"Expected 3 results, got {len(results)}\"\n",
    "assert 'ticket' in results[0], \"Missing 'ticket' key\"\n",
    "assert 'score' in results[0], \"Missing 'score' key\"\n",
    "assert isinstance(results[0]['score'], float), \"Score should be float\"\n",
    "# Scores should be descending\n",
    "scores = [r['score'] for r in results]\n",
    "assert scores == sorted(scores, reverse=True), \"Scores not sorted\"\n",
    "print(\"✓ Task 3 passed\")\n",
    "\n",
    "# Show results\n",
    "print(\"\\nSearch results for 'password reset issue':\")\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"{i+1}. [{result['score']:.3f}] {result['ticket']['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Build IVF Index\n",
    "\n",
    "Create and train IVF index for faster search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "# 1. Create quantizer (IndexFlatIP)\n",
    "quantizer = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "# 2. Create IndexIVFFlat with nlist=10\n",
    "nlist = 10\n",
    "index_ivf = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "# 3. Train on embeddings\n",
    "index_ivf.train(embeddings)\n",
    "\n",
    "# 4. Add embeddings\n",
    "index_ivf.add(embeddings)\n",
    "\n",
    "# 5. Set nprobe=5\n",
    "index_ivf.nprobe = 5\n",
    "\n",
    "print(f\"IVF Index trained: {index_ivf.is_trained}\")\n",
    "print(f\"IVF Index contains {index_ivf.ntotal} vectors\")\n",
    "print(f\"nlist: {index_ivf.nlist}, nprobe: {index_ivf.nprobe}\")\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert index_ivf is not None, \"IVF index not created\"\n",
    "assert index_ivf.is_trained, \"Index not trained\"\n",
    "assert index_ivf.ntotal == len(tickets), f\"Expected {len(tickets)} vectors\"\n",
    "assert index_ivf.nlist == 10, f\"Expected nlist=10, got {index_ivf.nlist}\"\n",
    "assert index_ivf.nprobe == 5, f\"Expected nprobe=5, got {index_ivf.nprobe}\"\n",
    "print(\"✓ Task 4 passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Measure Recall\n",
    "\n",
    "Compare IVF results to Flat (ground truth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "# Create test queries\n",
    "test_queries = [\n",
    "    \"cannot login\",\n",
    "    \"payment failed\",\n",
    "    \"slow performance\"\n",
    "]\n",
    "\n",
    "k = 10\n",
    "\n",
    "# 1. Encode test queries\n",
    "query_embeddings = model.encode(test_queries, normalize_embeddings=True).astype('float32')\n",
    "\n",
    "# 2. Search with both Flat and IVF indexes\n",
    "recalls = []\n",
    "\n",
    "for query_emb in query_embeddings:\n",
    "    query_emb_2d = query_emb.reshape(1, -1)\n",
    "    \n",
    "    # Ground truth from Flat index\n",
    "    _, flat_indices = index_flat.search(query_emb_2d, k)\n",
    "    flat_set = set(flat_indices[0])\n",
    "    \n",
    "    # Results from IVF index\n",
    "    _, ivf_indices = index_ivf.search(query_emb_2d, k)\n",
    "    ivf_set = set(ivf_indices[0])\n",
    "    \n",
    "    # 3. Calculate recall@10 for each query\n",
    "    recall = len(flat_set.intersection(ivf_set)) / k\n",
    "    recalls.append(recall)\n",
    "\n",
    "# 4. Calculate average recall\n",
    "avg_recall = np.mean(recalls)\n",
    "\n",
    "print(f\"Individual recalls: {[f'{r:.4f}' for r in recalls]}\")\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert avg_recall > 0, \"Recall not calculated\"\n",
    "assert avg_recall >= 0.8, f\"Recall too low: {avg_recall:.3f} (should be >0.8)\"\n",
    "print(f\"✓ Task 5 passed\")\n",
    "print(f\"Average Recall@10: {avg_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Build HNSW Index\n",
    "\n",
    "Create HNSW index and compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "# 1. Create IndexHNSWFlat with M=32\n",
    "M = 32\n",
    "index_hnsw = faiss.IndexHNSWFlat(dimension, M, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "# 2. Set efConstruction=200\n",
    "index_hnsw.hnsw.efConstruction = 200\n",
    "\n",
    "# 3. Add embeddings\n",
    "index_hnsw.add(embeddings)\n",
    "\n",
    "# 4. Set efSearch=64\n",
    "index_hnsw.hnsw.efSearch = 64\n",
    "\n",
    "print(f\"HNSW Index contains {index_hnsw.ntotal} vectors\")\n",
    "print(f\"M: {M}, efSearch: {index_hnsw.hnsw.efSearch}\")\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert index_hnsw is not None, \"HNSW index not created\"\n",
    "assert index_hnsw.ntotal == len(tickets), f\"Expected {len(tickets)} vectors\"\n",
    "assert index_hnsw.hnsw.efSearch == 64, f\"Expected efSearch=64\"\n",
    "print(\"✓ Task 6 passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Benchmark All Indexes\n",
    "\n",
    "Compare Flat, IVF, and HNSW on latency and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "# Prepare queries\n",
    "query_embeddings = model.encode(test_queries, normalize_embeddings=True).astype('float32')\n",
    "k = 10\n",
    "\n",
    "benchmark_results = {}\n",
    "\n",
    "# Benchmark Flat index (ground truth)\n",
    "start_time = time.time()\n",
    "flat_results = []\n",
    "for query_emb in query_embeddings:\n",
    "    query_emb_2d = query_emb.reshape(1, -1)\n",
    "    _, indices = index_flat.search(query_emb_2d, k)\n",
    "    flat_results.append(set(indices[0]))\n",
    "flat_time = (time.time() - start_time) * 1000 / len(test_queries)  # ms per query\n",
    "\n",
    "benchmark_results['Flat'] = {\n",
    "    'latency_ms': flat_time,\n",
    "    'recall': 1.0  # 100% recall against itself\n",
    "}\n",
    "\n",
    "# Benchmark IVF index\n",
    "start_time = time.time()\n",
    "ivf_recalls = []\n",
    "for i, query_emb in enumerate(query_embeddings):\n",
    "    query_emb_2d = query_emb.reshape(1, -1)\n",
    "    _, indices = index_ivf.search(query_emb_2d, k)\n",
    "    ivf_set = set(indices[0])\n",
    "    recall = len(flat_results[i].intersection(ivf_set)) / k\n",
    "    ivf_recalls.append(recall)\n",
    "ivf_time = (time.time() - start_time) * 1000 / len(test_queries)\n",
    "\n",
    "benchmark_results['IVF'] = {\n",
    "    'latency_ms': ivf_time,\n",
    "    'recall': np.mean(ivf_recalls)\n",
    "}\n",
    "\n",
    "# Benchmark HNSW index\n",
    "start_time = time.time()\n",
    "hnsw_recalls = []\n",
    "for i, query_emb in enumerate(query_embeddings):\n",
    "    query_emb_2d = query_emb.reshape(1, -1)\n",
    "    _, indices = index_hnsw.search(query_emb_2d, k)\n",
    "    hnsw_set = set(indices[0])\n",
    "    recall = len(flat_results[i].intersection(hnsw_set)) / k\n",
    "    hnsw_recalls.append(recall)\n",
    "hnsw_time = (time.time() - start_time) * 1000 / len(test_queries)\n",
    "\n",
    "benchmark_results['HNSW'] = {\n",
    "    'latency_ms': hnsw_time,\n",
    "    'recall': np.mean(hnsw_recalls)\n",
    "}\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert benchmark_results['Flat']['latency_ms'] > 0, \"Flat latency not measured\"\n",
    "assert benchmark_results['IVF']['latency_ms'] > 0, \"IVF latency not measured\"\n",
    "assert benchmark_results['HNSW']['latency_ms'] > 0, \"HNSW latency not measured\"\n",
    "assert benchmark_results['Flat']['recall'] == 1.0, \"Flat should have 100% recall\"\n",
    "assert benchmark_results['IVF']['recall'] >= 0.8, f\"IVF recall too low: {benchmark_results['IVF']['recall']}\"\n",
    "assert benchmark_results['HNSW']['recall'] >= 0.8, f\"HNSW recall too low: {benchmark_results['HNSW']['recall']}\"\n",
    "# IVF and HNSW should be faster than Flat\n",
    "assert benchmark_results['HNSW']['latency_ms'] < benchmark_results['Flat']['latency_ms'], \"IVF should be faster than Flat\"\n",
    "print(\"✓ Task 7 passed\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBenchmark Results:\")\n",
    "print(f\"{'Index':<10} {'Latency (ms)':<15} {'Recall@10':<12} {'Speedup':<10}\")\n",
    "print(\"-\" * 50)\n",
    "flat_latency = benchmark_results['Flat']['latency_ms']\n",
    "for name, metrics in benchmark_results.items():\n",
    "    speedup = flat_latency / metrics['latency_ms'] if metrics['latency_ms'] > 0 else 0\n",
    "    print(f\"{name:<10} {metrics['latency_ms']:<15.2f} {metrics['recall']:<12.4f} {speedup:<10.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Implement Metadata Filtering\n",
    "\n",
    "Search with category and status filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "def search_with_filter(query_text, k=5, category=None, status=None):\n",
    "    \"\"\"\n",
    "    Search with optional metadata filters.\n",
    "    \n",
    "    Args:\n",
    "        query_text: Query string\n",
    "        k: Number of results\n",
    "        category: Filter by category (optional)\n",
    "        status: Filter by status (optional)\n",
    "    \n",
    "    Returns:\n",
    "        List of filtered results\n",
    "    \"\"\"\n",
    "    # 1. Encode query\n",
    "    query_embedding = model.encode([query_text], normalize_embeddings=True).astype('float32')\n",
    "    \n",
    "    # 2. Search index (retrieve k*10 to account for filtering)\n",
    "    retrieve_k = min(k * 10, len(tickets))\n",
    "    scores, indices = index_flat.search(query_embedding, retrieve_k)\n",
    "    \n",
    "    # 3. Filter results by category and status\n",
    "    filtered_results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        ticket = tickets[idx]\n",
    "        \n",
    "        # Apply filters\n",
    "        if category is not None and ticket['category'] != category:\n",
    "            continue\n",
    "        if status is not None and ticket['status'] != status:\n",
    "            continue\n",
    "        \n",
    "        filtered_results.append({\n",
    "            'ticket': ticket,\n",
    "            'score': float(score)\n",
    "        })\n",
    "        \n",
    "        # 4. Return top k filtered results\n",
    "        if len(filtered_results) >= k:\n",
    "            break\n",
    "    \n",
    "    return filtered_results\n",
    "\n",
    "# TEST - Do not modify\n",
    "# Test category filter\n",
    "results_billing = search_with_filter(\"payment\", k=3, category=\"billing\")\n",
    "assert len(results_billing) > 0, \"No results with billing filter\"\n",
    "for r in results_billing:\n",
    "    assert r['ticket']['category'] == 'billing', f\"Wrong category: {r['ticket']['category']}\"\n",
    "\n",
    "# Test status filter\n",
    "results_open = search_with_filter(\"issue\", k=3, status=\"open\")\n",
    "assert len(results_open) > 0, \"No results with status filter\"\n",
    "for r in results_open:\n",
    "    assert r['ticket']['status'] == 'open', f\"Wrong status: {r['ticket']['status']}\"\n",
    "\n",
    "# Test combined filters\n",
    "results_combined = search_with_filter(\"problem\", k=2, category=\"technical\", status=\"open\")\n",
    "for r in results_combined:\n",
    "    assert r['ticket']['category'] == 'technical', \"Wrong category\"\n",
    "    assert r['ticket']['status'] == 'open', \"Wrong status\"\n",
    "\n",
    "print(\"✓ Task 8 passed\")\n",
    "\n",
    "# Show filtered results\n",
    "print(\"\\nFiltered search (category=billing):\")\n",
    "for i, r in enumerate(results_billing):\n",
    "    print(f\"{i+1}. [{r['score']:.3f}] {r['ticket']['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've successfully:\n",
    "- ✓ Generated embeddings with sentence-transformers\n",
    "- ✓ Built FAISS indexes (Flat, IVF, HNSW)\n",
    "- ✓ Measured performance and recall\n",
    "- ✓ Implemented metadata filtering\n",
    "\n",
    "**Next steps:**\n",
    "- Experiment with different index parameters\n",
    "- Try larger datasets\n",
    "- Combine FAISS with RAG in Module 6!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
