{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Build FAISS Search System\n",
    "\n",
    "Implement semantic search using FAISS and sentence-transformers.\n",
    "\n",
    "**Goals:**\n",
    "- Create FAISS index for ticket corpus\n",
    "- Implement search function\n",
    "- Compare Flat vs IVF vs HNSW performance\n",
    "- Implement metadata filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load ticket dataset from fixtures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tickets\n",
    "with open('../fixtures/input/tickets.json', 'r') as f:\n",
    "    tickets = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(tickets)} tickets\")\n",
    "print(f\"\\nSample ticket:\")\n",
    "print(json.dumps(tickets[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Generate Embeddings\n",
    "\n",
    "Use sentence-transformers to embed ticket titles and descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Load sentence-transformers model\n",
    "# 2. Create list of texts (title + description for each ticket)\n",
    "# 3. Generate embeddings with normalize_embeddings=True\n",
    "# 4. Convert to float32\n",
    "\n",
    "model = None  # TODO: Load model\n",
    "texts = None  # TODO: Create texts\n",
    "embeddings = None  # TODO: Generate embeddings\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert model is not None, \"Model not loaded\"\n",
    "assert len(texts) == len(tickets), f\"Expected {len(tickets)} texts, got {len(texts)}\"\n",
    "assert embeddings is not None, \"Embeddings not generated\"\n",
    "assert embeddings.shape == (len(tickets), 384), f\"Wrong shape: {embeddings.shape}\"\n",
    "assert embeddings.dtype == np.float32, f\"Wrong dtype: {embeddings.dtype}\"\n",
    "# Check normalization\n",
    "norms = np.linalg.norm(embeddings, axis=1)\n",
    "assert np.allclose(norms, 1.0, atol=1e-5), \"Embeddings not normalized\"\n",
    "print(\"✓ Task 1 passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Create FAISS Index\n",
    "\n",
    "Build IndexFlatIP and add embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Create IndexFlatIP with correct dimension\n",
    "# 2. Add embeddings to index\n",
    "\n",
    "index_flat = None  # TODO: Create index\n",
    "# TODO: Add embeddings\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert index_flat is not None, \"Index not created\"\n",
    "assert index_flat.ntotal == len(tickets), f\"Expected {len(tickets)} vectors, got {index_flat.ntotal}\"\n",
    "assert index_flat.d == 384, f\"Wrong dimension: {index_flat.d}\"\n",
    "print(\"✓ Task 2 passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Implement Search Function\n",
    "\n",
    "Create function to search tickets by text query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tickets(query_text, k=5):\n",
    "    \"\"\"\n",
    "    Search for similar tickets.\n",
    "\n",
    "    Args:\n",
    "        query_text: Text query\n",
    "        k: Number of results\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with 'ticket', 'score' keys\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # 1. Encode query text with model (normalized, float32, 2D)\n",
    "    # 2. Search index\n",
    "    # 3. Format results as list of dicts\n",
    "\n",
    "    return []  # TODO: Return results\n",
    "\n",
    "# TEST - Do not modify\n",
    "results = search_tickets(\"password reset issue\", k=3)\n",
    "assert len(results) == 3, f\"Expected 3 results, got {len(results)}\"\n",
    "assert 'ticket' in results[0], \"Missing 'ticket' key\"\n",
    "assert 'score' in results[0], \"Missing 'score' key\"\n",
    "assert isinstance(results[0]['score'], float), \"Score should be float\"\n",
    "# Scores should be descending\n",
    "scores = [r['score'] for r in results]\n",
    "assert scores == sorted(scores, reverse=True), \"Scores not sorted\"\n",
    "print(\"✓ Task 3 passed\")\n",
    "\n",
    "# Show results\n",
    "print(\"\\nSearch results for 'password reset issue':\")\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"{i+1}. [{result['score']:.3f}] {result['ticket']['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Build IVF Index\n",
    "\n",
    "Create and train IVF index for faster search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Create quantizer (IndexFlatIP)\n",
    "# 2. Create IndexIVFFlat with nlist=10\n",
    "# 3. Train on embeddings\n",
    "# 4. Add embeddings\n",
    "# 5. Set nprobe=5\n",
    "\n",
    "quantizer = None  # TODO: Create quantizer\n",
    "index_ivf = None  # TODO: Create IVF index\n",
    "# TODO: Train and add\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert index_ivf is not None, \"IVF index not created\"\n",
    "assert index_ivf.is_trained, \"Index not trained\"\n",
    "assert index_ivf.ntotal == len(tickets), f\"Expected {len(tickets)} vectors\"\n",
    "assert index_ivf.nlist == 10, f\"Expected nlist=10, got {index_ivf.nlist}\"\n",
    "assert index_ivf.nprobe == 5, f\"Expected nprobe=5, got {index_ivf.nprobe}\"\n",
    "print(\"✓ Task 4 passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Measure Recall\n",
    "\n",
    "Compare IVF results to Flat (ground truth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test queries\n",
    "test_queries = [\n",
    "    \"cannot login\",\n",
    "    \"payment failed\",\n",
    "    \"slow performance\"\n",
    "]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# 1. Encode test queries\n",
    "# 2. Search with both Flat and IVF indexes\n",
    "# 3. Calculate recall@10 for each query\n",
    "# 4. Calculate average recall\n",
    "\n",
    "avg_recall = 0.0  # TODO: Calculate average recall\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert avg_recall > 0, \"Recall not calculated\"\n",
    "assert avg_recall >= 0.8, f\"Recall too low: {avg_recall:.3f} (should be >0.8)\"\n",
    "print(f\"✓ Task 5 passed\")\n",
    "print(f\"Average Recall@10: {avg_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Build HNSW Index\n",
    "\n",
    "Create HNSW index and compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Create IndexHNSWFlat with M=32\n",
    "# 2. Set efConstruction=200\n",
    "# 3. Add embeddings\n",
    "# 4. Set efSearch=64\n",
    "\n",
    "index_hnsw = None  # TODO: Create HNSW index\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert index_hnsw is not None, \"HNSW index not created\"\n",
    "assert index_hnsw.ntotal == len(tickets), f\"Expected {len(tickets)} vectors\"\n",
    "assert index_hnsw.hnsw.efSearch == 64, f\"Expected efSearch=64\"\n",
    "print(\"✓ Task 6 passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Benchmark All Indexes\n",
    "\n",
    "Compare Flat, IVF, and HNSW on latency and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare queries\n",
    "query_embeddings = model.encode(test_queries, normalize_embeddings=True).astype('float32')\n",
    "k = 10\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# For each index (Flat, IVF, HNSW):\n",
    "# 1. Measure search time for all test queries\n",
    "# 2. Calculate average latency per query (in milliseconds)\n",
    "# 3. Calculate recall@10 vs Flat index\n",
    "# 4. Store in dictionary\n",
    "\n",
    "benchmark_results = {\n",
    "    'Flat': {'latency_ms': 0.0, 'recall': 0.0},\n",
    "    'IVF': {'latency_ms': 0.0, 'recall': 0.0},\n",
    "    'HNSW': {'latency_ms': 0.0, 'recall': 0.0},\n",
    "}\n",
    "\n",
    "# TODO: Implement benchmarking\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert benchmark_results['Flat']['latency_ms'] > 0, \"Flat latency not measured\"\n",
    "assert benchmark_results['IVF']['latency_ms'] > 0, \"IVF latency not measured\"\n",
    "assert benchmark_results['HNSW']['latency_ms'] > 0, \"HNSW latency not measured\"\n",
    "assert benchmark_results['Flat']['recall'] == 1.0, \"Flat should have 100% recall\"\n",
    "assert benchmark_results['IVF']['recall'] >= 0.8, f\"IVF recall too low: {benchmark_results['IVF']['recall']}\"\n",
    "assert benchmark_results['HNSW']['recall'] >= 0.8, f\"HNSW recall too low: {benchmark_results['HNSW']['recall']}\"\n",
    "# IVF and HNSW should be faster than Flat\n",
    "assert benchmark_results['HNSW']['latency_ms'] < benchmark_results['Flat']['latency_ms'], \"IVF should be faster than Flat\"\n",
    "print(\"✓ Task 7 passed\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBenchmark Results:\")\n",
    "print(f\"{'Index':<10} {'Latency (ms)':<15} {'Recall@10':<12} {'Speedup':<10}\")\n",
    "print(\"-\" * 50)\n",
    "flat_latency = benchmark_results['Flat']['latency_ms']\n",
    "for name, metrics in benchmark_results.items():\n",
    "    speedup = flat_latency / metrics['latency_ms'] if metrics['latency_ms'] > 0 else 0\n",
    "    print(f\"{name:<10} {metrics['latency_ms']:<15.2f} {metrics['recall']:<12.4f} {speedup:<10.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Implement Metadata Filtering\n",
    "\n",
    "Search with category and status filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_filter(query_text, k=5, category=None, status=None):\n",
    "    \"\"\"\n",
    "    Search with optional metadata filters.\n",
    "\n",
    "    Args:\n",
    "        query_text: Query string\n",
    "        k: Number of results\n",
    "        category: Filter by category (optional)\n",
    "        status: Filter by status (optional)\n",
    "\n",
    "    Returns:\n",
    "        List of filtered results\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # 1. Encode query\n",
    "    # 2. Search index (retrieve k*10 to account for filtering)\n",
    "    # 3. Filter results by category and status\n",
    "    # 4. Return top k filtered results\n",
    "\n",
    "    return []  # TODO: Implement\n",
    "\n",
    "# TEST - Do not modify\n",
    "# Test category filter\n",
    "results_billing = search_with_filter(\"payment\", k=3, category=\"billing\")\n",
    "assert len(results_billing) > 0, \"No results with billing filter\"\n",
    "for r in results_billing:\n",
    "    assert r['ticket']['category'] == 'billing', f\"Wrong category: {r['ticket']['category']}\"\n",
    "\n",
    "# Test status filter\n",
    "results_open = search_with_filter(\"issue\", k=3, status=\"open\")\n",
    "assert len(results_open) > 0, \"No results with status filter\"\n",
    "for r in results_open:\n",
    "    assert r['ticket']['status'] == 'open', f\"Wrong status: {r['ticket']['status']}\"\n",
    "\n",
    "# Test combined filters\n",
    "results_combined = search_with_filter(\"problem\", k=2, category=\"technical\", status=\"open\")\n",
    "for r in results_combined:\n",
    "    assert r['ticket']['category'] == 'technical', \"Wrong category\"\n",
    "    assert r['ticket']['status'] == 'open', \"Wrong status\"\n",
    "\n",
    "print(\"✓ Task 8 passed\")\n",
    "\n",
    "# Show filtered results\n",
    "print(\"\\nFiltered search (category=billing):\")\n",
    "for i, r in enumerate(results_billing):\n",
    "    print(f\"{i+1}. [{r['score']:.3f}] {r['ticket']['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've successfully:\n",
    "- ✓ Generated embeddings with sentence-transformers\n",
    "- ✓ Built FAISS indexes (Flat, IVF, HNSW)\n",
    "- ✓ Measured performance and recall\n",
    "- ✓ Implemented metadata filtering\n",
    "\n",
    "**Next steps:**\n",
    "- Experiment with different index parameters\n",
    "- Try larger datasets\n",
    "- Combine FAISS with RAG in Module 6!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
