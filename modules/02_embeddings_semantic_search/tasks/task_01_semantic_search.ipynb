{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Build a Semantic Search System\n",
    "\n",
    "## Scenario\n",
    "You have a collection of support ticket descriptions. Build a semantic search system that:\n",
    "1. Encodes all documents into embeddings\n",
    "2. Finds the most similar documents for a given query\n",
    "3. Returns results with similarity scores\n",
    "\n",
    "## Your Tasks:\n",
    "1. **Load and encode**: Load documents and create embeddings\n",
    "2. **Implement search**: Create a function to find top-k similar documents\n",
    "3. **Find duplicates**: Identify near-duplicate documents\n",
    "4. **Category clustering**: Group documents by semantic similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup (provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load documents\n",
    "with open('../fixtures/input/documents.json') as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "print(f\"\\nSample document:\")\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(f\"Model loaded. Embedding dimension: {model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1: Create Embeddings\n",
    "\n",
    "Extract text from documents and create normalized embeddings.\n",
    "\n",
    "Store:\n",
    "- `texts`: list of document texts\n",
    "- `embeddings`: numpy array of normalized embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Extract texts from documents\n",
    "# 2. Encode with normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST - Do not modify\n",
    "assert 'texts' in dir(), \"Variable 'texts' not found\"\n",
    "assert 'embeddings' in dir(), \"Variable 'embeddings' not found\"\n",
    "assert len(texts) == 20, f\"Expected 20 texts, got {len(texts)}\"\n",
    "assert embeddings.shape == (20, 384), f\"Expected shape (20, 384), got {embeddings.shape}\"\n",
    "\n",
    "# Check normalization\n",
    "norms = np.linalg.norm(embeddings, axis=1)\n",
    "assert np.allclose(norms, 1.0), \"Embeddings should be normalized (norm=1)\"\n",
    "\n",
    "print(\"Task 1 PASSED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Implement Semantic Search\n",
    "\n",
    "Create a function `search(query, top_k=3)` that:\n",
    "- Takes a query string\n",
    "- Returns top-k most similar documents with scores\n",
    "\n",
    "Return format: list of dicts with 'id', 'text', 'score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def search(query: str, top_k: int = 3):\n",
    "    \"\"\"\n",
    "    Find most similar documents to query.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query string\n",
    "        top_k: Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of dicts: [{'id': ..., 'text': ..., 'score': ...}, ...]\n",
    "    \"\"\"\n",
    "    pass  # Your implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST - Do not modify\n",
    "results = search(\"How to install Python on my computer?\", top_k=3)\n",
    "\n",
    "assert len(results) == 3, f\"Expected 3 results, got {len(results)}\"\n",
    "assert all('id' in r and 'text' in r and 'score' in r for r in results), \"Missing keys in results\"\n",
    "assert all(0 <= r['score'] <= 1 for r in results), \"Scores should be between 0 and 1\"\n",
    "assert results[0]['score'] >= results[1]['score'] >= results[2]['score'], \"Results should be sorted by score\"\n",
    "\n",
    "# Top result should be about Python installation\n",
    "assert 'python' in results[0]['text'].lower() or 'install' in results[0]['text'].lower(), \\\n",
    "    \"Top result should be about Python installation\"\n",
    "\n",
    "print(\"Task 2 PASSED!\")\n",
    "print(\"\\nTop 3 results:\")\n",
    "for r in results:\n",
    "    print(f\"  Score: {r['score']:.4f} | {r['text'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: Find Near-Duplicates\n",
    "\n",
    "Create a function `find_duplicates(threshold=0.85)` that finds document pairs with similarity above threshold.\n",
    "\n",
    "Return format: list of dicts with 'doc1_id', 'doc2_id', 'similarity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def find_duplicates(threshold: float = 0.85):\n",
    "    \"\"\"\n",
    "    Find document pairs with similarity above threshold.\n",
    "    \n",
    "    Args:\n",
    "        threshold: Minimum similarity to consider as duplicate\n",
    "        \n",
    "    Returns:\n",
    "        List of dicts: [{'doc1_id': ..., 'doc2_id': ..., 'similarity': ...}, ...]\n",
    "    \"\"\"\n",
    "    pass  # Your implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST - Do not modify\n",
    "duplicates = find_duplicates(threshold=0.85)\n",
    "\n",
    "assert isinstance(duplicates, list), \"Should return a list\"\n",
    "assert len(duplicates) > 0, \"Should find at least one duplicate pair\"\n",
    "assert all('doc1_id' in d and 'doc2_id' in d and 'similarity' in d for d in duplicates), \\\n",
    "    \"Missing keys in duplicate results\"\n",
    "assert all(d['similarity'] >= 0.85 for d in duplicates), \"All pairs should have similarity >= 0.85\"\n",
    "\n",
    "print(\"Task 3 PASSED!\")\n",
    "print(f\"\\nFound {len(duplicates)} duplicate pairs:\")\n",
    "for d in duplicates[:5]:  # Show first 5\n",
    "    print(f\"  {d['doc1_id']} <-> {d['doc2_id']}: {d['similarity']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4: Cluster Documents\n",
    "\n",
    "Create a function `cluster_documents(n_clusters=5)` that groups documents by semantic similarity.\n",
    "\n",
    "Return format: dict mapping cluster_id to list of document ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cluster_documents(n_clusters: int = 5):\n",
    "    \"\"\"\n",
    "    Cluster documents by semantic similarity.\n",
    "    \n",
    "    Args:\n",
    "        n_clusters: Number of clusters\n",
    "        \n",
    "    Returns:\n",
    "        Dict mapping cluster_id to list of document ids\n",
    "    \"\"\"\n",
    "    pass  # Your implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST - Do not modify\n",
    "clusters = cluster_documents(n_clusters=5)\n",
    "\n",
    "assert isinstance(clusters, dict), \"Should return a dict\"\n",
    "assert len(clusters) == 5, f\"Expected 5 clusters, got {len(clusters)}\"\n",
    "\n",
    "# All documents should be assigned\n",
    "all_ids = [doc_id for ids in clusters.values() for doc_id in ids]\n",
    "assert len(all_ids) == 20, f\"Expected 20 documents in clusters, got {len(all_ids)}\"\n",
    "\n",
    "print(\"Task 4 PASSED!\")\n",
    "print(\"\\nClusters:\")\n",
    "for cluster_id, doc_ids in clusters.items():\n",
    "    print(f\"\\nCluster {cluster_id} ({len(doc_ids)} docs):\")\n",
    "    for doc_id in doc_ids[:3]:  # Show first 3\n",
    "        doc = next(d for d in documents if d['id'] == doc_id)\n",
    "        print(f\"  - {doc['text'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Bonus: Visualize Embeddings\n",
    "\n",
    "Create a t-SNE visualization of document embeddings colored by category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (optional)\n",
    "# Use sklearn.manifold.TSNE to reduce to 2D\n",
    "# Color points by document category\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Expected Results\n",
    "\n",
    "After completing all tasks:\n",
    "- Task 1: 20 normalized embeddings of shape (20, 384)\n",
    "- Task 2: Search returns relevant Python installation docs for Python query\n",
    "- Task 3: Find similar document pairs (e.g., doc_001 and doc_002 about Python)\n",
    "- Task 4: Documents grouped by topic (Software, Network, Hardware, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
