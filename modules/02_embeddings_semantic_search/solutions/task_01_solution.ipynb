{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Build a Semantic Search System - SOLUTION\n",
    "\n",
    "## Scenario\n",
    "You have a collection of support ticket descriptions. Build a semantic search system that:\n",
    "1. Encodes all documents into embeddings\n",
    "2. Finds the most similar documents for a given query\n",
    "3. Returns results with similarity scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load documents\n",
    "with open('../fixtures/input/documents.json') as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "print(f\"\\nSample document:\")\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(f\"Model loaded. Embedding dimension: {model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1: Create Embeddings - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "# 1. Extract texts from documents\n",
    "texts = [doc['text'] for doc in documents]\n",
    "\n",
    "# 2. Encode with normalization\n",
    "embeddings = model.encode(\n",
    "    texts,\n",
    "    normalize_embeddings=True,  # Important for dot product = cosine\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"Texts: {len(texts)}\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Sample embedding norm: {np.linalg.norm(embeddings[0]):.4f} (should be 1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "assert 'texts' in dir(), \"Variable 'texts' not found\"\n",
    "assert 'embeddings' in dir(), \"Variable 'embeddings' not found\"\n",
    "assert len(texts) == 20, f\"Expected 20 texts, got {len(texts)}\"\n",
    "assert embeddings.shape == (20, 384), f\"Expected shape (20, 384), got {embeddings.shape}\"\n",
    "\n",
    "norms = np.linalg.norm(embeddings, axis=1)\n",
    "assert np.allclose(norms, 1.0), \"Embeddings should be normalized (norm=1)\"\n",
    "\n",
    "print(\"Task 1 PASSED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Implement Semantic Search - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "def search(query: str, top_k: int = 3):\n",
    "    \"\"\"\n",
    "    Find most similar documents to query.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query string\n",
    "        top_k: Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of dicts: [{'id': ..., 'text': ..., 'score': ...}, ...]\n",
    "    \"\"\"\n",
    "    # 1. Encode query (normalized)\n",
    "    query_embedding = model.encode(query, normalize_embeddings=True)\n",
    "    \n",
    "    # 2. Compute similarities (dot product since normalized)\n",
    "    similarities = np.dot(embeddings, query_embedding)\n",
    "    \n",
    "    # 3. Get top-k indices (sorted descending)\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    \n",
    "    # 4. Build results\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'id': documents[idx]['id'],\n",
    "            'text': documents[idx]['text'],\n",
    "            'score': float(similarities[idx])\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test search\n",
    "results = search(\"How to install Python on my computer?\")\n",
    "print(\"Search results:\")\n",
    "for r in results:\n",
    "    print(f\"  {r['score']:.4f} | {r['text'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "results = search(\"How to install Python on my computer?\", top_k=3)\n",
    "\n",
    "assert len(results) == 3, f\"Expected 3 results, got {len(results)}\"\n",
    "assert all('id' in r and 'text' in r and 'score' in r for r in results), \"Missing keys\"\n",
    "assert all(0 <= r['score'] <= 1 for r in results), \"Scores should be between 0 and 1\"\n",
    "assert results[0]['score'] >= results[1]['score'] >= results[2]['score'], \"Should be sorted\"\n",
    "assert 'python' in results[0]['text'].lower() or 'install' in results[0]['text'].lower()\n",
    "\n",
    "print(\"Task 2 PASSED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: Find Near-Duplicates - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "def find_duplicates(threshold: float = 0.85):\n",
    "    \"\"\"\n",
    "    Find document pairs with similarity above threshold.\n",
    "    \n",
    "    Args:\n",
    "        threshold: Minimum similarity to consider as duplicate\n",
    "        \n",
    "    Returns:\n",
    "        List of dicts: [{'doc1_id': ..., 'doc2_id': ..., 'similarity': ...}, ...]\n",
    "    \"\"\"\n",
    "    # Compute pairwise similarities\n",
    "    similarity_matrix = np.dot(embeddings, embeddings.T)\n",
    "    \n",
    "    duplicates = []\n",
    "    \n",
    "    # Check upper triangle only (avoid duplicates and self-comparison)\n",
    "    for i in range(len(documents)):\n",
    "        for j in range(i + 1, len(documents)):\n",
    "            sim = similarity_matrix[i, j]\n",
    "            if sim >= threshold:\n",
    "                duplicates.append({\n",
    "                    'doc1_id': documents[i]['id'],\n",
    "                    'doc2_id': documents[j]['id'],\n",
    "                    'similarity': float(sim)\n",
    "                })\n",
    "    \n",
    "    # Sort by similarity descending\n",
    "    duplicates.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    \n",
    "    return duplicates\n",
    "\n",
    "# Test\n",
    "duplicates = find_duplicates(threshold=0.85)\n",
    "print(f\"Found {len(duplicates)} duplicate pairs\")\n",
    "for d in duplicates[:5]:\n",
    "    doc1 = next(doc for doc in documents if doc['id'] == d['doc1_id'])\n",
    "    doc2 = next(doc for doc in documents if doc['id'] == d['doc2_id'])\n",
    "    print(f\"\\n{d['similarity']:.4f}:\")\n",
    "    print(f\"  1: {doc1['text'][:60]}...\")\n",
    "    print(f\"  2: {doc2['text'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "duplicates = find_duplicates(threshold=0.85)\n",
    "\n",
    "assert isinstance(duplicates, list), \"Should return a list\"\n",
    "assert len(duplicates) > 0, \"Should find at least one duplicate pair\"\n",
    "assert all('doc1_id' in d and 'doc2_id' in d and 'similarity' in d for d in duplicates)\n",
    "assert all(d['similarity'] >= 0.85 for d in duplicates)\n",
    "\n",
    "print(\"Task 3 PASSED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4: Cluster Documents - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cluster_documents(n_clusters: int = 5):\n",
    "    \"\"\"\n",
    "    Cluster documents by semantic similarity.\n",
    "    \n",
    "    Args:\n",
    "        n_clusters: Number of clusters\n",
    "        \n",
    "    Returns:\n",
    "        Dict mapping cluster_id to list of document ids\n",
    "    \"\"\"\n",
    "    # Fit K-means\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    # Group documents by cluster\n",
    "    clusters = {}\n",
    "    for idx, label in enumerate(labels):\n",
    "        label = int(label)  # Convert numpy int to Python int\n",
    "        if label not in clusters:\n",
    "            clusters[label] = []\n",
    "        clusters[label].append(documents[idx]['id'])\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "# Test\n",
    "clusters = cluster_documents(n_clusters=5)\n",
    "print(\"Clusters:\")\n",
    "for cluster_id, doc_ids in clusters.items():\n",
    "    print(f\"\\nCluster {cluster_id} ({len(doc_ids)} docs):\")\n",
    "    for doc_id in doc_ids:\n",
    "        doc = next(d for d in documents if d['id'] == doc_id)\n",
    "        print(f\"  [{doc['category']}] {doc['text'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "clusters = cluster_documents(n_clusters=5)\n",
    "\n",
    "assert isinstance(clusters, dict), \"Should return a dict\"\n",
    "assert len(clusters) == 5, f\"Expected 5 clusters, got {len(clusters)}\"\n",
    "\n",
    "all_ids = [doc_id for ids in clusters.values() for doc_id in ids]\n",
    "assert len(all_ids) == 20, f\"Expected 20 documents in clusters, got {len(all_ids)}\"\n",
    "\n",
    "print(\"Task 4 PASSED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Bonus: Visualize Embeddings - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS SOLUTION\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reduce to 2D\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Get categories for coloring\n",
    "categories = [doc['category'] for doc in documents]\n",
    "unique_categories = list(set(categories))\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(unique_categories)))\n",
    "category_colors = {cat: colors[i] for i, cat in enumerate(unique_categories)}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for cat in unique_categories:\n",
    "    mask = [c == cat for c in categories]\n",
    "    plt.scatter(\n",
    "        embeddings_2d[mask, 0],\n",
    "        embeddings_2d[mask, 1],\n",
    "        c=[category_colors[cat]],\n",
    "        label=cat,\n",
    "        alpha=0.7,\n",
    "        s=100\n",
    "    )\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Document Embeddings (t-SNE)')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "**Key techniques used:**\n",
    "\n",
    "1. **Embedding creation:**\n",
    "   - Use `normalize_embeddings=True` for efficient dot product\n",
    "   - Batch encode for efficiency\n",
    "\n",
    "2. **Semantic search:**\n",
    "   - Encode query with same normalization\n",
    "   - Use dot product (= cosine for normalized vectors)\n",
    "   - Use `np.argsort` for top-k\n",
    "\n",
    "3. **Duplicate detection:**\n",
    "   - Compute pairwise similarity matrix\n",
    "   - Only check upper triangle to avoid duplicates\n",
    "\n",
    "4. **Clustering:**\n",
    "   - K-means works on embedding space\n",
    "   - Documents with similar topics cluster together\n",
    "\n",
    "**Common pitfalls:**\n",
    "- Forgetting to normalize embeddings\n",
    "- Using cosine_similarity instead of dot product (slower for normalized vectors)\n",
    "- Not handling numpy types in JSON serialization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
