{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI API Basics\n",
    "\n",
    "Learn the fundamentals of working with OpenAI's API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "**Important:** You need an OpenAI API key to run this notebook.\n",
    "\n",
    "1. Get your API key from https://platform.openai.com/api-keys\n",
    "2. Set it in the cell below\n",
    "3. **Never commit API keys to git!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Method 1: Set API key directly (for learning)\n",
    "api_key = \"your-api-key-here\"  # Replace with your actual key\n",
    "\n",
    "# Method 2: Use environment variable (recommended for production)\n",
    "# api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "print(\"✓ OpenAI client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Chat Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is Python?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Messages\n",
    "\n",
    "System messages set the behavior/personality of the assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without system message\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain recursion\"}\n",
    "    ]\n",
    ")\n",
    "print(\"Without system message:\")\n",
    "print(response.choices[0].message.content[:200] + \"...\\n\")\n",
    "\n",
    "# With system message\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a computer science teacher explaining concepts to 10-year-olds. Use simple words and fun examples.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain recursion\"}\n",
    "    ]\n",
    ")\n",
    "print(\"With system message (ELI10):\")\n",
    "print(response.choices[0].message.content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Turn Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build conversation history\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful Python tutor.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's a list comprehension?\"}\n",
    "]\n",
    "\n",
    "# First response\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "assistant_message = response.choices[0].message.content\n",
    "print(\"Assistant:\", assistant_message[:150] + \"...\\n\")\n",
    "\n",
    "# Add to conversation history\n",
    "messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Can you show me an example?\"})\n",
    "\n",
    "# Continue conversation\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "print(\"Assistant:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect full response object\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    ")\n",
    "\n",
    "print(\"Response structure:\")\n",
    "print(f\"  ID: {response.id}\")\n",
    "print(f\"  Model: {response.model}\")\n",
    "print(f\"  Created: {response.created}\")\n",
    "print(f\"  \\nUsage:\")\n",
    "print(f\"    Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"    Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"    Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"  \\nContent: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Control generation with parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature: Controls randomness (0 = deterministic, 2 = creative)\n",
    "prompt = \"Write a tagline for a coffee shop\"\n",
    "\n",
    "print(\"Temperature = 0 (deterministic):\")\n",
    "for i in range(3):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    print(f\"  {i+1}. {response.choices[0].message.content}\")\n",
    "\n",
    "print(\"\\nTemperature = 1.5 (creative):\")\n",
    "for i in range(3):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=1.5\n",
    "    )\n",
    "    print(f\"  {i+1}. {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_tokens: Limit response length\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a long essay about Python\"}],\n",
    "    max_tokens=50  # Limit to ~50 tokens\n",
    ")\n",
    "\n",
    "print(\"Response (limited to 50 tokens):\")\n",
    "print(response.choices[0].message.content)\n",
    "print(f\"\\nActual tokens used: {response.usage.completion_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Count tokens in text\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "texts = [\n",
    "    \"Hello!\",\n",
    "    \"This is a longer sentence with more words.\",\n",
    "    \"tokenization converts text into numbers\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    tokens = encoding.encode(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"  Tokens: {tokens}\")\n",
    "    print(f\"  Count: {len(tokens)}\")\n",
    "    print(f\"  Decoded: {encoding.decode(tokens)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate cost before API call\n",
    "def estimate_cost(messages, max_tokens=500, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"Estimate API call cost\"\"\"\n",
    "\n",
    "    # Count input tokens\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    input_tokens = 0\n",
    "    for msg in messages:\n",
    "        input_tokens += len(encoding.encode(msg[\"content\"]))\n",
    "    input_tokens += 3 * len(messages)  # Message overhead\n",
    "\n",
    "    # Prices per 1M tokens\n",
    "    prices = {\n",
    "        \"gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.60},\n",
    "        \"gpt-4o\": {\"input\": 2.50, \"output\": 10.00}\n",
    "    }\n",
    "\n",
    "    input_cost = (input_tokens / 1_000_000) * prices[model][\"input\"]\n",
    "    output_cost = (max_tokens / 1_000_000) * prices[model][\"output\"]\n",
    "\n",
    "    return {\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"max_output_tokens\": max_tokens,\n",
    "        \"input_cost\": input_cost,\n",
    "        \"max_output_cost\": output_cost,\n",
    "        \"max_total_cost\": input_cost + output_cost\n",
    "    }\n",
    "\n",
    "# Example\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a detailed explanation of machine learning.\"}\n",
    "]\n",
    "\n",
    "cost = estimate_cost(messages, max_tokens=500)\n",
    "print(\"Cost Estimate:\")\n",
    "print(f\"  Input tokens: {cost['input_tokens']}\")\n",
    "print(f\"  Max output tokens: {cost['max_output_tokens']}\")\n",
    "print(f\"  Input cost: ${cost['input_cost']:.6f}\")\n",
    "print(f\"  Max output cost: ${cost['max_output_cost']:.6f}\")\n",
    "print(f\"  Max total cost: ${cost['max_total_cost']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import APIError, RateLimitError, APIConnectionError\n",
    "\n",
    "def safe_api_call(messages):\n",
    "    \"\"\"API call with error handling\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    except RateLimitError:\n",
    "        return \"Error: Rate limit exceeded. Please try again later.\"\n",
    "\n",
    "    except APIConnectionError:\n",
    "        return \"Error: Connection failed. Check your internet.\"\n",
    "\n",
    "    except APIError as e:\n",
    "        return f\"Error: API error - {str(e)}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: Unexpected error - {str(e)}\"\n",
    "\n",
    "# Test\n",
    "result = safe_api_call([{\"role\": \"user\", \"content\": \"Hello!\"}])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different models\n",
    "prompt = \"Explain quantum computing in one sentence.\"\n",
    "\n",
    "models = [\"gpt-4o-mini\", \"gpt-4o\"]\n",
    "\n",
    "for model in models:\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "    tokens = response.usage.total_tokens\n",
    "\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"  Response: {content}\")\n",
    "    print(f\"  Tokens: {tokens}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ Basic chat completions  \n",
    "✅ System messages for behavior control  \n",
    "✅ Multi-turn conversations  \n",
    "✅ Temperature and max_tokens parameters  \n",
    "✅ Token counting and cost estimation  \n",
    "✅ Error handling  \n",
    "✅ Model comparison\n",
    "\n",
    "**Next:** Learn about structured outputs with Pydantic!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
