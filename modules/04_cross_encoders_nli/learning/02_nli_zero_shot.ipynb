{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLI and Zero-Shot Classification\n",
    "\n",
    "Learn how to use Natural Language Inference for zero-shot text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from sentence_transformers import CrossEncoder\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is NLI?\n",
    "\n",
    "**Natural Language Inference** predicts the relationship between two texts:\n",
    "\n",
    "- **Entailment**: Hypothesis logically follows from premise\n",
    "- **Contradiction**: Hypothesis contradicts premise\n",
    "- **Neutral**: No clear relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NLI cross-encoder\n",
    "nli_model = CrossEncoder('cross-encoder/nli-deberta-v3-small')\n",
    "\n",
    "def predict_nli(premise, hypothesis):\n",
    "    \"\"\"Predict NLI relationship\"\"\"\n",
    "    scores = nli_model.predict([(premise, hypothesis)])\n",
    "    labels = ['contradiction', 'entailment', 'neutral']\n",
    "    # scores is array [contradiction_score, entailment_score, neutral_score]\n",
    "    max_idx = scores[0].argmax()\n",
    "    return labels[max_idx], scores[0][max_idx]\n",
    "\n",
    "# Example 1: Entailment\n",
    "premise = \"I have a dog named Max\"\n",
    "hypothesis = \"I have a pet\"\n",
    "label, score = predict_nli(premise, hypothesis)\n",
    "print(f\"Premise: {premise}\")\n",
    "print(f\"Hypothesis: {hypothesis}\")\n",
    "print(f\"Prediction: {label} ({score:.3f})\\n\")\n",
    "\n",
    "# Example 2: Contradiction\n",
    "premise = \"The weather is sunny\"\n",
    "hypothesis = \"It's raining outside\"\n",
    "label, score = predict_nli(premise, hypothesis)\n",
    "print(f\"Premise: {premise}\")\n",
    "print(f\"Hypothesis: {hypothesis}\")\n",
    "print(f\"Prediction: {label} ({score:.3f})\\n\")\n",
    "\n",
    "# Example 3: Neutral\n",
    "premise = \"I went to the store\"\n",
    "hypothesis = \"I bought milk\"\n",
    "label, score = predict_nli(premise, hypothesis)\n",
    "print(f\"Premise: {premise}\")\n",
    "print(f\"Hypothesis: {hypothesis}\")\n",
    "print(f\"Prediction: {label} ({score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Classification\n",
    "\n",
    "Convert classification to NLI:\n",
    "- **Premise** = Text to classify\n",
    "- **Hypothesis** = \"This text is about {label}\"\n",
    "- Check which hypothesis has highest **entailment** score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HuggingFace's zero-shot pipeline\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Example: Sentiment classification\n",
    "text = \"I absolutely love this product! It exceeded all my expectations.\"\n",
    "labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "result = classifier(text, candidate_labels=labels)\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print(\"\\nClassification:\")\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    print(f\"  {label}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How It Works Behind the Scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual zero-shot (what the pipeline does internally)\n",
    "text = \"The system crashed and all data was lost.\"\n",
    "labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "# Create hypothesis for each label\n",
    "template = \"This text expresses {} sentiment.\"\n",
    "\n",
    "scores = []\n",
    "for label in labels:\n",
    "    hypothesis = template.format(label)\n",
    "    # Predict entailment\n",
    "    result = nli_model.predict([(text, hypothesis)])\n",
    "    # Get entailment score (index 1)\n",
    "    entailment_score = result[0][1]\n",
    "    scores.append(entailment_score)\n",
    "    print(f\"{label}: {entailment_score:.3f}\")\n",
    "\n",
    "# Best label\n",
    "best_idx = np.argmax(scores)\n",
    "print(f\"\\nPredicted: {labels[best_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Label Classification\n",
    "\n",
    "Allow text to belong to multiple categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"URGENT: Payment system is down. All customer transactions failing.\"\n",
    "labels = [\"urgent\", \"technical\", \"payment\", \"customer_facing\", \"feature_request\"]\n",
    "\n",
    "# Multi-label classification\n",
    "result = classifier(\n",
    "    text,\n",
    "    candidate_labels=labels,\n",
    "    multi_label=True  # Allow multiple labels\n",
    ")\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print(\"\\nLabels (threshold > 0.5):\")\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    if score > 0.5:\n",
    "        print(f\"  ✓ {label}: {score:.3f}\")\n",
    "    else:\n",
    "        print(f\"  ✗ {label}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Hypothesis Templates\n",
    "\n",
    "Improve accuracy with better templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The meeting is scheduled for tomorrow at 2pm\"\n",
    "\n",
    "# Poor template (generic)\n",
    "result1 = classifier(\n",
    "    text,\n",
    "    candidate_labels=[\"scheduling\", \"reminder\", \"general\"],\n",
    "    hypothesis_template=\"This text is about {}.\"\n",
    ")\n",
    "\n",
    "print(\"Generic template:\")\n",
    "for label, score in zip(result1['labels'][:3], result1['scores'][:3]):\n",
    "    print(f\"  {label}: {score:.3f}\")\n",
    "\n",
    "# Better template (specific)\n",
    "result2 = classifier(\n",
    "    text,\n",
    "    candidate_labels=[\"scheduling\", \"reminder\", \"general\"],\n",
    "    hypothesis_template=\"This message is intended to {} something.\"\n",
    ")\n",
    "\n",
    "print(\"\\nSpecific template:\")\n",
    "for label, score in zip(result2['labels'][:3], result2['scores'][:3]):\n",
    "    print(f\"  {label}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cases\n",
    "\n",
    "### 1. Content Moderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = [\n",
    "    \"Great product, highly recommend!\",\n",
    "    \"This is spam! Buy now at spamsite.com\",\n",
    "    \"You're an idiot and I hate you\",\n",
    "    \"How do I reset my password?\"\n",
    "]\n",
    "\n",
    "moderation_labels = [\"appropriate\", \"spam\", \"toxic\", \"helpful\"]\n",
    "\n",
    "for comment in comments:\n",
    "    result = classifier(comment, candidate_labels=moderation_labels)\n",
    "    top_label = result['labels'][0]\n",
    "    top_score = result['scores'][0]\n",
    "    print(f\"[{top_label:12s} {top_score:.2f}] {comment[:40]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Ticket Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets = [\n",
    "    \"My credit card was charged twice for the same order\",\n",
    "    \"The app crashes when I try to upload images\",\n",
    "    \"Can you add dark mode to the settings?\",\n",
    "    \"I forgot my password and can't log in\"\n",
    "]\n",
    "\n",
    "teams = [\"billing\", \"technical\", \"product\", \"support\"]\n",
    "\n",
    "print(\"Ticket Routing:\")\n",
    "for ticket in tickets:\n",
    "    result = classifier(ticket, candidate_labels=teams)\n",
    "    assigned_team = result['labels'][0]\n",
    "    confidence = result['scores'][0]\n",
    "    print(f\"→ {assigned_team:10s} ({confidence:.2f}) | {ticket[:40]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Intent Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    \"What's the weather like today?\",\n",
    "    \"Order a pizza with extra cheese\",\n",
    "    \"Cancel my subscription\",\n",
    "    \"Tell me a joke\"\n",
    "]\n",
    "\n",
    "intents = [\"query\", \"command\", \"cancel\", \"entertainment\"]\n",
    "\n",
    "for msg in messages:\n",
    "    result = classifier(msg, candidate_labels=intents)\n",
    "    intent = result['labels'][0]\n",
    "    print(f\"[{intent:15s}] {msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "### 1. Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: Specific labels dominate generic ones\n",
    "text = \"The product is okay\"\n",
    "labels = [\"positive\", \"negative\", \"neutral\", \"product_review\"]\n",
    "\n",
    "result = classifier(text, candidate_labels=labels)\n",
    "print(\"⚠️  Notice how 'product_review' dominates:\")\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    print(f\"  {label}: {score:.3f}\")\n",
    "\n",
    "# Solution: Use hierarchical classification\n",
    "# First: Classify as product_review or not\n",
    "# Then: If product_review, classify sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Ambiguous Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambiguous text\n",
    "text = \"The system is running\"\n",
    "labels = [\"positive\", \"neutral\", \"status_update\"]\n",
    "\n",
    "result = classifier(text, candidate_labels=labels)\n",
    "print(\"Ambiguous classification:\")\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    print(f\"  {label}: {score:.3f}\")\n",
    "print(\"\\n⚠️  Close scores indicate ambiguity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sarcasm and Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sarcasm (hard for NLI)\n",
    "text = \"Oh great, another system outage. Just perfect!\"\n",
    "labels = [\"positive\", \"negative\"]\n",
    "\n",
    "result = classifier(text, candidate_labels=labels)\n",
    "print(\"Sarcasm test:\")\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    print(f\"  {label}: {score:.3f}\")\n",
    "\n",
    "# Most models struggle with sarcasm\n",
    "if result['labels'][0] == 'positive':\n",
    "    print(\"\\n⚠️  Model missed sarcasm (classified as positive)\")\n",
    "else:\n",
    "    print(\"\\n✓ Model caught sarcasm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "1. **Use specific hypothesis templates**\n",
    "   - \"This is a {} message\" > \"This is about {}\"\n",
    "\n",
    "2. **Start with 3-5 labels**\n",
    "   - Too many labels confuse the model\n",
    "   - Use hierarchical classification for more\n",
    "\n",
    "3. **Set thresholds for multi-label**\n",
    "   - Don't trust scores below 0.5-0.6\n",
    "\n",
    "4. **Validate on edge cases**\n",
    "   - Sarcasm, negation, ambiguity\n",
    "\n",
    "5. **Consider fine-tuning if you have data**\n",
    "   - Zero-shot is great for bootstrapping\n",
    "   - Fine-tuned models are more accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different models\n",
    "models = [\n",
    "    \"facebook/bart-large-mnli\",\n",
    "    \"cross-encoder/nli-deberta-v3-small\",\n",
    "]\n",
    "\n",
    "text = \"This movie was absolutely terrible\"\n",
    "labels = [\"positive\", \"negative\"]\n",
    "\n",
    "for model_name in models:\n",
    "    clf = pipeline(\"zero-shot-classification\", model=model_name)\n",
    "    result = clf(text, candidate_labels=labels)\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Predicted: {result['labels'][0]} ({result['scores'][0]:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ NLI models predict entailment/contradiction/neutral  \n",
    "✅ Zero-shot classification converts labels to hypotheses  \n",
    "✅ No training data needed  \n",
    "✅ Works across many domains  \n",
    "✅ Great for prototyping and bootstrapping  \n",
    "⚠️  May struggle with sarcasm and ambiguity  \n",
    "⚠️  Specific labels can dominate generic ones\n",
    "\n",
    "**Next:** Combine cross-encoders and NLI in production tasks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
