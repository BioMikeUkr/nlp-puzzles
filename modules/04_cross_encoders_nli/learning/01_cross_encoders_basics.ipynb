{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Encoders Basics\n",
    "\n",
    "Learn the fundamentals of cross-encoders for reranking and relevance scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-Encoder vs Cross-Encoder\n",
    "\n",
    "### Bi-Encoder (What we learned in Module 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bi-encoder\n",
    "bi_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "query = \"How to reset password?\"\n",
    "documents = [\n",
    "    \"To reset your password, go to Settings and click Reset Password.\",\n",
    "    \"Python tutorial for beginners: learn basic syntax.\",\n",
    "    \"Contact customer support at support@example.com\"\n",
    "]\n",
    "\n",
    "# Encode separately\n",
    "query_emb = bi_encoder.encode(query)\n",
    "doc_embs = bi_encoder.encode(documents)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarities = cosine_similarity([query_emb], doc_embs)[0]\n",
    "\n",
    "print(\"Bi-Encoder Scores:\")\n",
    "for i, (doc, score) in enumerate(zip(documents, similarities)):\n",
    "    print(f\"{i+1}. [{score:.3f}] {doc[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Encoder (More Accurate)\n",
    "\n",
    "Encodes query and document **together** for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cross-encoder\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# Create query-document pairs\n",
    "pairs = [[query, doc] for doc in documents]\n",
    "\n",
    "# Get relevance scores\n",
    "scores = cross_encoder.predict(pairs)\n",
    "\n",
    "print(\"\\nCross-Encoder Scores:\")\n",
    "for i, (doc, score) in enumerate(zip(documents, scores)):\n",
    "    print(f\"{i+1}. [{score:.3f}] {doc[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Differences\n",
    "\n",
    "| Aspect | Bi-Encoder | Cross-Encoder |\n",
    "|--------|------------|---------------|\n",
    "| **Encoding** | Separate | Together |\n",
    "| **Accuracy** | Good (85-90%) | Better (92-97%) |\n",
    "| **Speed** | Fast | Slow |\n",
    "| **Caching** | Can cache docs | Must recompute |\n",
    "| **Use Case** | Retrieval | Reranking |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bi-Encoder:\")\n",
    "print(\"  [CLS] query [SEP] → Embedding A (384 dims)\")\n",
    "print(\"  [CLS] document [SEP] → Embedding B (384 dims)\")\n",
    "print(\"  Score = cosine_sim(A, B)\")\n",
    "print(\"  No interaction between query and document tokens\\n\")\n",
    "\n",
    "print(\"Cross-Encoder:\")\n",
    "print(\"  [CLS] query [SEP] document [SEP] → Score\")\n",
    "print(\"  Full attention between all tokens\")\n",
    "print(\"  Direct relevance prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with more documents\n",
    "test_docs = documents * 50  # 150 documents\n",
    "query = \"password reset\"\n",
    "\n",
    "# Bi-encoder (with precomputed embeddings)\n",
    "start = time.time()\n",
    "query_emb = bi_encoder.encode(query)\n",
    "doc_embs = bi_encoder.encode(test_docs, show_progress_bar=False)\n",
    "similarities = cosine_similarity([query_emb], doc_embs)[0]\n",
    "bi_time = time.time() - start\n",
    "\n",
    "# Cross-encoder\n",
    "start = time.time()\n",
    "pairs = [[query, doc] for doc in test_docs]\n",
    "scores = cross_encoder.predict(pairs, show_progress_bar=False)\n",
    "cross_time = time.time() - start\n",
    "\n",
    "print(f\"Bi-Encoder: {bi_time:.3f}s\")\n",
    "print(f\"Cross-Encoder: {cross_time:.3f}s\")\n",
    "print(f\"Cross-encoder is {cross_time/bi_time:.1f}x slower\")\n",
    "\n",
    "# But if doc embeddings are precomputed:\n",
    "start = time.time()\n",
    "query_emb = bi_encoder.encode(query)\n",
    "similarities = cosine_similarity([query_emb], doc_embs)[0]\n",
    "bi_time_cached = time.time() - start\n",
    "\n",
    "print(f\"\\nBi-Encoder (cached docs): {bi_time_cached*1000:.1f}ms\")\n",
    "print(f\"Speedup: {cross_time/bi_time_cached:.0f}x faster with caching!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Comparison\n",
    "\n",
    "Let's test on tricky examples where bi-encoders struggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tricky example: semantic vs lexical match\n",
    "query = \"change password\"\n",
    "docs = [\n",
    "    \"To modify your account credentials, visit the settings page.\",  # Semantic match\n",
    "    \"Change password update reset modify credentials password.\",  # Keyword stuffing\n",
    "    \"The weather might change tomorrow. Remember your umbrella.\"   # False positive\n",
    "]\n",
    "\n",
    "# Bi-encoder\n",
    "query_emb = bi_encoder.encode(query)\n",
    "doc_embs = bi_encoder.encode(docs)\n",
    "bi_scores = cosine_similarity([query_emb], doc_embs)[0]\n",
    "\n",
    "# Cross-encoder\n",
    "pairs = [[query, doc] for doc in docs]\n",
    "cross_scores = cross_encoder.predict(pairs)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"\\nBi-Encoder Rankings:\")\n",
    "for idx in np.argsort(bi_scores)[::-1]:\n",
    "    print(f\"  [{bi_scores[idx]:.3f}] {docs[idx][:60]}...\")\n",
    "\n",
    "print(\"\\nCross-Encoder Rankings:\")\n",
    "for idx in np.argsort(cross_scores)[::-1]:\n",
    "    print(f\"  [{cross_scores[idx]:.3f}] {docs[idx][:60]}...\")\n",
    "\n",
    "print(\"\\n✓ Cross-encoder correctly ranks semantic match first!\")\n",
    "print(\"✗ Bi-encoder fooled by keyword stuffing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use Each\n",
    "\n",
    "### Use Bi-Encoder for:\n",
    "- First-stage retrieval (millions of candidates)\n",
    "- Real-time search (<50ms latency)\n",
    "- Similarity search with caching\n",
    "- Semantic search, duplicate detection\n",
    "\n",
    "### Use Cross-Encoder for:\n",
    "- Reranking top candidates (10-100)\n",
    "- High accuracy requirements\n",
    "- Question answering\n",
    "- When latency allows (>100ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Models\n",
    "\n",
    "### MS MARCO Models (Passage Ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"cross-encoder/ms-marco-TinyBERT-L-2-v2\",     # Fastest (14M params)\n",
    "    \"cross-encoder/ms-marco-MiniLM-L-6-v2\",       # Balanced (22M params)\n",
    "    \"cross-encoder/ms-marco-MiniLM-L-12-v2\",      # Accurate (33M params)\n",
    "]\n",
    "\n",
    "query = \"python tutorial\"\n",
    "doc = \"Learn Python programming from scratch with examples\"\n",
    "\n",
    "for model_name in models:\n",
    "    model = CrossEncoder(model_name)\n",
    "    score = model.predict([(query, doc)])[0]\n",
    "    print(f\"{model_name.split('/')[-1]}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process multiple queries efficiently\n",
    "queries = [\"password reset\", \"payment failed\", \"slow loading\"]\n",
    "docs = [\n",
    "    \"Reset your password in settings\",\n",
    "    \"Payment troubleshooting guide\",\n",
    "    \"Performance optimization tips\"\n",
    "]\n",
    "\n",
    "# Create all pairs\n",
    "all_pairs = []\n",
    "for query in queries:\n",
    "    for doc in docs:\n",
    "        all_pairs.append([query, doc])\n",
    "\n",
    "# Batch predict (much faster than individual calls)\n",
    "scores = cross_encoder.predict(all_pairs, batch_size=32)\n",
    "\n",
    "# Reshape results\n",
    "scores_matrix = np.array(scores).reshape(len(queries), len(docs))\n",
    "\n",
    "print(\"Relevance Matrix:\")\n",
    "print(f\"{'Query':<20} | {' | '.join([f'Doc{i+1}' for i in range(len(docs))])}\")\n",
    "print(\"-\" * 60)\n",
    "for i, query in enumerate(queries):\n",
    "    scores_str = ' | '.join([f'{s:.2f}' for s in scores_matrix[i]])\n",
    "    print(f\"{query:<20} | {scores_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Interpretation\n",
    "\n",
    "MS MARCO models output scores (not probabilities):\n",
    "- Scores range from negative to positive\n",
    "- Higher = more relevant\n",
    "- No fixed threshold (relative ranking matters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check score ranges\n",
    "test_pairs = [\n",
    "    [\"weather\", \"Today's weather forecast: sunny and warm\"],  # Highly relevant\n",
    "    [\"weather\", \"Climate change impacts global temperatures\"],  # Somewhat relevant\n",
    "    [\"weather\", \"Python programming tutorial for beginners\"],  # Not relevant\n",
    "]\n",
    "\n",
    "scores = cross_encoder.predict(test_pairs)\n",
    "\n",
    "print(\"Score Ranges:\")\n",
    "for pair, score in zip(test_pairs, scores):\n",
    "    relevance = \"High\" if score > 5 else \"Medium\" if score > 0 else \"Low\"\n",
    "    print(f\"Score: {score:6.2f} ({relevance:6s}) | {pair[1][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ Cross-encoders are more accurate than bi-encoders  \n",
    "✅ Use for reranking, not initial retrieval  \n",
    "✅ Process query and document together  \n",
    "✅ Slower but worth it for final ranking  \n",
    "✅ Choose model based on speed/accuracy tradeoff\n",
    "\n",
    "**Next:** Learn about retrieve-rerank pipelines in the next notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
