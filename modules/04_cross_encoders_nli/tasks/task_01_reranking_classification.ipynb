{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Reranking and Zero-Shot Classification\n",
    "\n",
    "Build a production-ready retrieve-rerank pipeline and zero-shot classifier.\n",
    "\n",
    "**Goals:**\n",
    "- Implement retrieve-rerank pipeline\n",
    "- Measure reranking improvement (MRR, NDCG)\n",
    "- Build zero-shot classifier\n",
    "- Handle multi-label classification\n",
    "- Test on edge cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load queries and documents\n",
    "with open('../fixtures/input/queries_documents.json', 'r') as f:\n",
    "    queries_data = json.load(f)\n",
    "\n",
    "# Load classification texts\n",
    "with open('../fixtures/input/classification_texts.json', 'r') as f:\n",
    "    classification_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(queries_data)} query sets\")\n",
    "print(f\"Loaded {len(classification_data)} classification examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Baseline Bi-Encoder Search\n",
    "\n",
    "Implement basic search using only bi-encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Load bi-encoder model\n",
    "# 2. For each query, rank documents by cosine similarity\n",
    "# 3. Return top-3 document IDs for each query\n",
    "\n",
    "bi_encoder = None  # TODO: Load model\n",
    "bi_encoder_results = {}  # TODO: Store results as {query_id: [doc_ids]}\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert bi_encoder is not None, \"Bi-encoder not loaded\"\n",
    "assert len(bi_encoder_results) == len(queries_data), \"Missing results\"\n",
    "for query_id, doc_ids in bi_encoder_results.items():\n",
    "    assert len(doc_ids) == 3, f\"Expected 3 results for {query_id}\"\n",
    "print(\"✓ Task 1 passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Cross-Encoder Reranking\n",
    "\n",
    "Rerank bi-encoder results with cross-encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Load cross-encoder model (ms-marco-MiniLM-L-6-v2)\n",
    "# 2. For each query, rerank all documents using cross-encoder\n",
    "# 3. Return top-3 document IDs for each query\n",
    "\n",
    "cross_encoder = None  # TODO: Load model\n",
    "cross_encoder_results = {}  # TODO: Store results as {query_id: [doc_ids]}\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert cross_encoder is not None, \"Cross-encoder not loaded\"\n",
    "assert len(cross_encoder_results) == len(queries_data), \"Missing results\"\n",
    "for query_id, doc_ids in cross_encoder_results.items():\n",
    "    assert len(doc_ids) == 3, f\"Expected 3 results for {query_id}\"\n",
    "print(\"✓ Task 2 passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Calculate MRR (Mean Reciprocal Rank)\n",
    "\n",
    "Measure ranking quality with MRR metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(results, ground_truth):\n",
    "    \"\"\"\n",
    "    Calculate Mean Reciprocal Rank\n",
    "    \n",
    "    Args:\n",
    "        results: Dict of {query_id: [ranked_doc_ids]}\n",
    "        ground_truth: Dict of {query_id: [relevant_doc_ids]}\n",
    "    \n",
    "    Returns:\n",
    "        float: MRR score\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # 1. For each query, find rank of first relevant document\n",
    "    # 2. Calculate reciprocal rank (1/rank)\n",
    "    # 3. Return mean of reciprocal ranks\n",
    "    \n",
    "    return 0.0  # TODO: Calculate MRR\n",
    "\n",
    "# Prepare ground truth\n",
    "ground_truth = {q['query_id']: q['relevant_docs'] for q in queries_data}\n",
    "\n",
    "# Calculate MRR for both methods\n",
    "mrr_bi_encoder = calculate_mrr(bi_encoder_results, ground_truth)\n",
    "mrr_cross_encoder = calculate_mrr(cross_encoder_results, ground_truth)\n",
    "\n",
    "print(f\"Bi-Encoder MRR: {mrr_bi_encoder:.3f}\")\n",
    "print(f\"Cross-Encoder MRR: {mrr_cross_encoder:.3f}\")\n",
    "print(f\"Improvement: {(mrr_cross_encoder - mrr_bi_encoder) / mrr_bi_encoder * 100:.1f}%\")\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert mrr_bi_encoder > 0, \"Bi-encoder MRR not calculated\"\n",
    "assert mrr_cross_encoder > 0, \"Cross-encoder MRR not calculated\"\n",
    "assert mrr_cross_encoder >= mrr_bi_encoder, \"Cross-encoder should improve MRR\"\n",
    "print(\"✓ Task 3 passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Calculate NDCG@3\n",
    "\n",
    "Measure ranking quality with NDCG metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ndcg_at_k(results, ground_truth, k=3):\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain at K\n",
    "    \n",
    "    Args:\n",
    "        results: Dict of {query_id: [ranked_doc_ids]}\n",
    "        ground_truth: Dict of {query_id: [relevant_doc_ids]}\n",
    "        k: Cutoff rank\n",
    "    \n",
    "    Returns:\n",
    "        float: NDCG@k score\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # 1. For each query, create relevance vector (1 if relevant, 0 if not)\n",
    "    # 2. Calculate DCG = sum(rel_i / log2(i+1)) for i=1..k\n",
    "    # 3. Calculate IDCG (DCG of perfect ranking)\n",
    "    # 4. NDCG = DCG / IDCG\n",
    "    # 5. Return mean NDCG across queries\n",
    "    \n",
    "    return 0.0  # TODO: Calculate NDCG\n",
    "\n",
    "# Calculate NDCG for both methods\n",
    "ndcg_bi_encoder = calculate_ndcg_at_k(bi_encoder_results, ground_truth, k=3)\n",
    "ndcg_cross_encoder = calculate_ndcg_at_k(cross_encoder_results, ground_truth, k=3)\n",
    "\n",
    "print(f\"Bi-Encoder NDCG@3: {ndcg_bi_encoder:.3f}\")\n",
    "print(f\"Cross-Encoder NDCG@3: {ndcg_cross_encoder:.3f}\")\n",
    "print(f\"Improvement: {(ndcg_cross_encoder - ndcg_bi_encoder) / ndcg_bi_encoder * 100:.1f}%\")\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert ndcg_bi_encoder > 0, \"Bi-encoder NDCG not calculated\"\n",
    "assert ndcg_cross_encoder > 0, \"Cross-encoder NDCG not calculated\"\n",
    "assert ndcg_cross_encoder >= ndcg_bi_encoder, \"Cross-encoder should improve NDCG\"\n",
    "print(\"✓ Task 4 passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Zero-Shot Classification\n",
    "\n",
    "Classify texts without training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Load zero-shot classification pipeline\n",
    "# 2. For each text in classification_data, predict top label\n",
    "# 3. Store results as {text_id: predicted_label}\n",
    "\n",
    "zero_shot_classifier = None  # TODO: Load pipeline\n",
    "classification_results = {}  # TODO: Store predictions\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert zero_shot_classifier is not None, \"Classifier not loaded\"\n",
    "assert len(classification_results) == len(classification_data), \"Missing predictions\"\n",
    "print(\"✓ Task 5 passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Calculate Classification Accuracy\n",
    "\n",
    "Measure zero-shot accuracy against ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Compare predictions to true_labels (first label)\n",
    "# 2. Calculate accuracy\n",
    "\n",
    "accuracy = 0.0  # TODO: Calculate accuracy\n",
    "\n",
    "print(f\"Zero-shot Accuracy: {accuracy:.1%}\")\n",
    "\n",
    "# Show predictions\n",
    "for item in classification_data[:5]:\n",
    "    text_id = item['text_id']\n",
    "    predicted = classification_results[text_id]\n",
    "    actual = item['true_labels'][0]\n",
    "    match = \"✓\" if predicted == actual else \"✗\"\n",
    "    print(f\"{match} {text_id}: predicted={predicted}, actual={actual}\")\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert accuracy > 0, \"Accuracy not calculated\"\n",
    "assert accuracy >= 0.5, f\"Accuracy too low: {accuracy:.1%}\"\n",
    "print(\"✓ Task 6 passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Multi-Label Classification\n",
    "\n",
    "Handle texts with multiple labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. For texts with multiple true_labels, use multi_label=True\n",
    "# 2. Predict all labels with score > threshold (0.5)\n",
    "# 3. Calculate F1 score for multi-label predictions\n",
    "\n",
    "def calculate_multilabel_f1(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "    Calculate F1 for multi-label classification\n",
    "    \n",
    "    Args:\n",
    "        predictions: Dict of {text_id: [predicted_labels]}\n",
    "        ground_truth: Dict of {text_id: [true_labels]}\n",
    "    \n",
    "    Returns:\n",
    "        float: Average F1 score\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # For each text:\n",
    "    # - Calculate precision = TP / (TP + FP)\n",
    "    # - Calculate recall = TP / (TP + FN)\n",
    "    # - F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return 0.0  # TODO: Calculate F1\n",
    "\n",
    "multilabel_predictions = {}  # TODO: Predict multiple labels\n",
    "multilabel_ground_truth = {item['text_id']: item['true_labels'] \n",
    "                           for item in classification_data}\n",
    "\n",
    "f1_score = calculate_multilabel_f1(multilabel_predictions, multilabel_ground_truth)\n",
    "\n",
    "print(f\"Multi-label F1: {f1_score:.3f}\")\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert len(multilabel_predictions) == len(classification_data), \"Missing predictions\"\n",
    "assert f1_score > 0, \"F1 not calculated\"\n",
    "assert f1_score >= 0.5, f\"F1 too low: {f1_score:.3f}\"\n",
    "print(\"✓ Task 7 passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Handle Edge Cases\n",
    "\n",
    "Test on challenging examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load edge cases\n",
    "with open('../fixtures/edge_cases/test_cases.json', 'r') as f:\n",
    "    edge_cases = json.load(f)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# 1. Test reranking edge cases\n",
    "# 2. Test classification edge cases\n",
    "# 3. Identify which cases the models handle well/poorly\n",
    "\n",
    "reranking_edge_results = {}  # TODO: Test reranking edge cases\n",
    "classification_edge_results = {}  # TODO: Test classification edge cases\n",
    "\n",
    "# Analyze results\n",
    "print(\"Reranking Edge Cases:\")\n",
    "for case in edge_cases['reranking_edge_cases']:\n",
    "    case_name = case['case']\n",
    "    if case_name in reranking_edge_results:\n",
    "        print(f\"  {case_name}: {reranking_edge_results[case_name]}\")\n",
    "\n",
    "print(\"\\nClassification Edge Cases:\")\n",
    "for case in edge_cases['classification_edge_cases']:\n",
    "    case_name = case['case']\n",
    "    if case_name in classification_edge_results:\n",
    "        predicted = classification_edge_results[case_name]\n",
    "        expected = case.get('expected', 'N/A')\n",
    "        match = \"✓\" if predicted == expected else \"✗\"\n",
    "        print(f\"  {match} {case_name}: predicted={predicted}, expected={expected}\")\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert len(reranking_edge_results) > 0, \"No reranking edge cases tested\"\n",
    "assert len(classification_edge_results) > 0, \"No classification edge cases tested\"\n",
    "print(\"\\n✓ Task 8 passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've successfully:\n",
    "- ✓ Built retrieve-rerank pipeline\n",
    "- ✓ Measured improvement with MRR and NDCG\n",
    "- ✓ Implemented zero-shot classification\n",
    "- ✓ Handled multi-label scenarios\n",
    "- ✓ Tested edge cases\n",
    "\n",
    "**Next steps:**\n",
    "- Try different cross-encoder models\n",
    "- Experiment with hypothesis templates\n",
    "- Combine with FAISS for production pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
