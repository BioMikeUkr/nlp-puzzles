{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Reranking and Zero-Shot Classification - SOLUTION\n",
    "\n",
    "Build a production-ready retrieve-rerank pipeline and zero-shot classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load queries and documents\n",
    "with open('../fixtures/input/queries_documents.json', 'r') as f:\n",
    "    queries_data = json.load(f)\n",
    "\n",
    "# Load classification texts\n",
    "with open('../fixtures/input/classification_texts.json', 'r') as f:\n",
    "    classification_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(queries_data)} query sets\")\n",
    "print(f\"Loaded {len(classification_data)} classification examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Baseline Bi-Encoder Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "# 1. Load bi-encoder model\n",
    "bi_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 2. For each query, rank documents by cosine similarity\n",
    "bi_encoder_results = {}\n",
    "\n",
    "for item in queries_data:\n",
    "    query_id = item['query_id']\n",
    "    query = item['query']\n",
    "    documents = item['documents']\n",
    "    \n",
    "    # Encode query and documents\n",
    "    query_emb = bi_encoder.encode(query)\n",
    "    doc_texts = [doc['text'] for doc in documents]\n",
    "    doc_embs = bi_encoder.encode(doc_texts)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = cosine_similarity([query_emb], doc_embs)[0]\n",
    "    \n",
    "    # Get top-3 document IDs\n",
    "    top_indices = np.argsort(similarities)[::-1][:3]\n",
    "    top_doc_ids = [documents[idx]['doc_id'] for idx in top_indices]\n",
    "    \n",
    "    bi_encoder_results[query_id] = top_doc_ids\n",
    "\n",
    "print(\"Bi-encoder results:\")\n",
    "for query_id, doc_ids in list(bi_encoder_results.items())[:2]:\n",
    "    print(f\"  {query_id}: {doc_ids}\")\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert bi_encoder is not None, \"Bi-encoder not loaded\"\n",
    "assert len(bi_encoder_results) == len(queries_data), \"Missing results\"\n",
    "for query_id, doc_ids in bi_encoder_results.items():\n",
    "    assert len(doc_ids) == 3, f\"Expected 3 results for {query_id}\"\n",
    "print(\"✓ Task 1 passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Cross-Encoder Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "# 1. Load cross-encoder model\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# 2. For each query, rerank all documents\n",
    "cross_encoder_results = {}\n",
    "\n",
    "for item in queries_data:\n",
    "    query_id = item['query_id']\n",
    "    query = item['query']\n",
    "    documents = item['documents']\n",
    "    \n",
    "    # Create query-document pairs\n",
    "    doc_texts = [doc['text'] for doc in documents]\n",
    "    pairs = [[query, doc_text] for doc_text in doc_texts]\n",
    "    \n",
    "    # Get cross-encoder scores\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # Get top-3 document IDs\n",
    "    top_indices = np.argsort(scores)[::-1][:3]\n",
    "    top_doc_ids = [documents[idx]['doc_id'] for idx in top_indices]\n",
    "    \n",
    "    cross_encoder_results[query_id] = top_doc_ids\n",
    "\n",
    "print(\"Cross-encoder results:\")\n",
    "for query_id, doc_ids in list(cross_encoder_results.items())[:2]:\n",
    "    print(f\"  {query_id}: {doc_ids}\")\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert cross_encoder is not None, \"Cross-encoder not loaded\"\n",
    "assert len(cross_encoder_results) == len(queries_data), \"Missing results\"\n",
    "for query_id, doc_ids in cross_encoder_results.items():\n",
    "    assert len(doc_ids) == 3, f\"Expected 3 results for {query_id}\"\n",
    "print(\"✓ Task 2 passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Calculate MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "def calculate_mrr(results, ground_truth):\n",
    "    \"\"\"\n",
    "    Calculate Mean Reciprocal Rank\n",
    "    \"\"\"\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    for query_id, ranked_docs in results.items():\n",
    "        relevant_docs = ground_truth[query_id]\n",
    "        \n",
    "        # Find rank of first relevant document\n",
    "        for rank, doc_id in enumerate(ranked_docs, start=1):\n",
    "            if doc_id in relevant_docs:\n",
    "                reciprocal_ranks.append(1.0 / rank)\n",
    "                break\n",
    "        else:\n",
    "            # No relevant document found\n",
    "            reciprocal_ranks.append(0.0)\n",
    "    \n",
    "    return np.mean(reciprocal_ranks)\n",
    "\n",
    "# Prepare ground truth\n",
    "ground_truth = {q['query_id']: q['relevant_docs'] for q in queries_data}\n",
    "\n",
    "# Calculate MRR for both methods\n",
    "mrr_bi_encoder = calculate_mrr(bi_encoder_results, ground_truth)\n",
    "mrr_cross_encoder = calculate_mrr(cross_encoder_results, ground_truth)\n",
    "\n",
    "print(f\"Bi-Encoder MRR: {mrr_bi_encoder:.3f}\")\n",
    "print(f\"Cross-Encoder MRR: {mrr_cross_encoder:.3f}\")\n",
    "improvement = (mrr_cross_encoder - mrr_bi_encoder) / mrr_bi_encoder * 100\n",
    "print(f\"Improvement: {improvement:.1f}%\")\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert mrr_bi_encoder > 0, \"Bi-encoder MRR not calculated\"\n",
    "assert mrr_cross_encoder > 0, \"Cross-encoder MRR not calculated\"\n",
    "assert mrr_cross_encoder >= mrr_bi_encoder, \"Cross-encoder should improve MRR\"\n",
    "print(\"✓ Task 3 passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Calculate NDCG@3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "def calculate_ndcg_at_k(results, ground_truth, k=3):\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain at K\n",
    "    \"\"\"\n",
    "    ndcg_scores = []\n",
    "    \n",
    "    for query_id, ranked_docs in results.items():\n",
    "        relevant_docs = set(ground_truth[query_id])\n",
    "        \n",
    "        # Create relevance vector (1 if relevant, 0 if not)\n",
    "        relevances = [1 if doc_id in relevant_docs else 0 \n",
    "                     for doc_id in ranked_docs[:k]]\n",
    "        \n",
    "        # Calculate DCG\n",
    "        dcg = 0.0\n",
    "        for i, rel in enumerate(relevances, start=1):\n",
    "            dcg += rel / np.log2(i + 1)\n",
    "        \n",
    "        # Calculate IDCG (perfect ranking)\n",
    "        ideal_relevances = sorted(relevances, reverse=True)\n",
    "        idcg = 0.0\n",
    "        for i, rel in enumerate(ideal_relevances, start=1):\n",
    "            idcg += rel / np.log2(i + 1)\n",
    "        \n",
    "        # NDCG\n",
    "        if idcg > 0:\n",
    "            ndcg_scores.append(dcg / idcg)\n",
    "        else:\n",
    "            ndcg_scores.append(0.0)\n",
    "    \n",
    "    return np.mean(ndcg_scores)\n",
    "\n",
    "# Calculate NDCG for both methods\n",
    "ndcg_bi_encoder = calculate_ndcg_at_k(bi_encoder_results, ground_truth, k=3)\n",
    "ndcg_cross_encoder = calculate_ndcg_at_k(cross_encoder_results, ground_truth, k=3)\n",
    "\n",
    "print(f\"Bi-Encoder NDCG@3: {ndcg_bi_encoder:.3f}\")\n",
    "print(f\"Cross-Encoder NDCG@3: {ndcg_cross_encoder:.3f}\")\n",
    "improvement = (ndcg_cross_encoder - ndcg_bi_encoder) / ndcg_bi_encoder * 100\n",
    "print(f\"Improvement: {improvement:.1f}%\")\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert ndcg_bi_encoder > 0, \"Bi-encoder NDCG not calculated\"\n",
    "assert ndcg_cross_encoder > 0, \"Cross-encoder NDCG not calculated\"\n",
    "assert ndcg_cross_encoder >= ndcg_bi_encoder, \"Cross-encoder should improve NDCG\"\n",
    "print(\"✓ Task 4 passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Zero-Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "# 1. Load zero-shot classification pipeline\n",
    "zero_shot_classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# 2. For each text, predict top label\n",
    "classification_results = {}\n",
    "\n",
    "for item in classification_data:\n",
    "    text_id = item['text_id']\n",
    "    text = item['text']\n",
    "    candidate_labels = item['candidate_labels']\n",
    "    \n",
    "    # Predict\n",
    "    result = zero_shot_classifier(text, candidate_labels=candidate_labels)\n",
    "    \n",
    "    # Store top prediction\n",
    "    classification_results[text_id] = result['labels'][0]\n",
    "\n",
    "print(\"Sample predictions:\")\n",
    "for text_id, label in list(classification_results.items())[:3]:\n",
    "    print(f\"  {text_id}: {label}\")\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert zero_shot_classifier is not None, \"Classifier not loaded\"\n",
    "assert len(classification_results) == len(classification_data), \"Missing predictions\"\n",
    "print(\"✓ Task 5 passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Calculate Classification Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "# Compare predictions to true_labels (first label)\n",
    "correct = 0\n",
    "total = len(classification_data)\n",
    "\n",
    "for item in classification_data:\n",
    "    text_id = item['text_id']\n",
    "    predicted = classification_results[text_id]\n",
    "    # Check if predicted matches any true label\n",
    "    if predicted in item['true_labels']:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f\"Zero-shot Accuracy: {accuracy:.1%}\")\n",
    "\n",
    "# Show predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "for item in classification_data[:5]:\n",
    "    text_id = item['text_id']\n",
    "    predicted = classification_results[text_id]\n",
    "    actual = item['true_labels'][0]\n",
    "    match = \"✓\" if predicted in item['true_labels'] else \"✗\"\n",
    "    print(f\"{match} {text_id}: predicted={predicted}, actual={actual}\")\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert accuracy > 0, \"Accuracy not calculated\"\n",
    "assert accuracy >= 0.5, f\"Accuracy too low: {accuracy:.1%}\"\n",
    "print(\"✓ Task 6 passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Multi-Label Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "def calculate_multilabel_f1(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "    Calculate F1 for multi-label classification\n",
    "    \"\"\"\n",
    "    f1_scores = []\n",
    "    \n",
    "    for text_id, pred_labels in predictions.items():\n",
    "        true_labels = set(ground_truth[text_id])\n",
    "        pred_labels = set(pred_labels)\n",
    "        \n",
    "        # Calculate TP, FP, FN\n",
    "        tp = len(true_labels & pred_labels)\n",
    "        fp = len(pred_labels - true_labels)\n",
    "        fn = len(true_labels - pred_labels)\n",
    "        \n",
    "        # Calculate precision and recall\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        \n",
    "        # Calculate F1\n",
    "        if precision + recall > 0:\n",
    "            f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        else:\n",
    "            f1 = 0.0\n",
    "        \n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "# Predict multiple labels with threshold\n",
    "multilabel_predictions = {}\n",
    "threshold = 0.5\n",
    "\n",
    "for item in classification_data:\n",
    "    text_id = item['text_id']\n",
    "    text = item['text']\n",
    "    candidate_labels = item['candidate_labels']\n",
    "    \n",
    "    # Multi-label prediction\n",
    "    result = zero_shot_classifier(\n",
    "        text,\n",
    "        candidate_labels=candidate_labels,\n",
    "        multi_label=True\n",
    "    )\n",
    "    \n",
    "    # Filter by threshold\n",
    "    predicted_labels = [\n",
    "        label for label, score in zip(result['labels'], result['scores'])\n",
    "        if score >= threshold\n",
    "    ]\n",
    "    \n",
    "    multilabel_predictions[text_id] = predicted_labels\n",
    "\n",
    "multilabel_ground_truth = {item['text_id']: item['true_labels'] \n",
    "                           for item in classification_data}\n",
    "\n",
    "f1_score = calculate_multilabel_f1(multilabel_predictions, multilabel_ground_truth)\n",
    "\n",
    "print(f\"Multi-label F1: {f1_score:.3f}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\nSample multi-label predictions:\")\n",
    "for item in classification_data[:3]:\n",
    "    text_id = item['text_id']\n",
    "    predicted = multilabel_predictions[text_id]\n",
    "    actual = item['true_labels']\n",
    "    print(f\"{text_id}:\")\n",
    "    print(f\"  Predicted: {predicted}\")\n",
    "    print(f\"  Actual: {actual}\")\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert len(multilabel_predictions) == len(classification_data), \"Missing predictions\"\n",
    "assert f1_score > 0, \"F1 not calculated\"\n",
    "assert f1_score >= 0.5, f\"F1 too low: {f1_score:.3f}\"\n",
    "print(\"✓ Task 7 passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Handle Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "# Load edge cases\n",
    "with open('../fixtures/edge_cases/test_cases.json', 'r') as f:\n",
    "    edge_cases = json.load(f)\n",
    "\n",
    "# Test reranking edge cases\n",
    "reranking_edge_results = {}\n",
    "\n",
    "print(\"Testing Reranking Edge Cases:\\n\")\n",
    "for case in edge_cases['reranking_edge_cases']:\n",
    "    case_name = case['case']\n",
    "    query = case['query']\n",
    "    documents = case['documents']\n",
    "    \n",
    "    # Rank with cross-encoder\n",
    "    pairs = [[query, doc] for doc in documents]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # Get rankings\n",
    "    ranked_indices = np.argsort(scores)[::-1]\n",
    "    ranked_docs = [documents[i] for i in ranked_indices]\n",
    "    \n",
    "    reranking_edge_results[case_name] = ranked_docs[0]  # Top doc\n",
    "    \n",
    "    print(f\"{case_name}:\")\n",
    "    print(f\"  Challenge: {case['challenge']}\")\n",
    "    print(f\"  Top result: {ranked_docs[0][:60]}...\")\n",
    "    print(f\"  Score: {scores[ranked_indices[0]]:.3f}\\n\")\n",
    "\n",
    "# Test classification edge cases\n",
    "classification_edge_results = {}\n",
    "\n",
    "print(\"\\nTesting Classification Edge Cases:\\n\")\n",
    "for case in edge_cases['classification_edge_cases']:\n",
    "    case_name = case['case']\n",
    "    text = case['text']\n",
    "    labels = case['labels']\n",
    "    expected = case.get('expected', 'N/A')\n",
    "    \n",
    "    # Classify\n",
    "    result = zero_shot_classifier(text, candidate_labels=labels)\n",
    "    predicted = result['labels'][0]\n",
    "    confidence = result['scores'][0]\n",
    "    \n",
    "    classification_edge_results[case_name] = predicted\n",
    "    \n",
    "    match = \"✓\" if predicted == expected else \"✗\"\n",
    "    print(f\"{match} {case_name}:\")\n",
    "    print(f\"  Challenge: {case['challenge']}\")\n",
    "    print(f\"  Text: {text[:60]}...\")\n",
    "    print(f\"  Predicted: {predicted} ({confidence:.2f})\")\n",
    "    print(f\"  Expected: {expected}\\n\")\n",
    "\n",
    "# TEST - Do not modify\n",
    "assert len(reranking_edge_results) > 0, \"No reranking edge cases tested\"\n",
    "assert len(classification_edge_results) > 0, \"No classification edge cases tested\"\n",
    "print(\"✓ Task 8 passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've successfully:\n",
    "- ✓ Built retrieve-rerank pipeline\n",
    "- ✓ Measured improvement with MRR and NDCG\n",
    "- ✓ Implemented zero-shot classification\n",
    "- ✓ Handled multi-label scenarios\n",
    "- ✓ Tested edge cases\n",
    "\n",
    "**Key insights:**\n",
    "- Cross-encoders improve ranking quality by 15-30%\n",
    "- MRR and NDCG capture different aspects of ranking quality\n",
    "- Zero-shot works well for clear categories\n",
    "- Struggles with sarcasm, ambiguity, and negation\n",
    "- Multi-label needs careful threshold tuning\n",
    "\n",
    "**Next:** Apply these techniques in RAG pipelines (Module 5)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
