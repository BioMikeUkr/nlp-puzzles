{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning with Contrastive Loss\n",
    "\n",
    "Learn to use contrastive loss (InfoNCE) for learning discriminative embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Contrastive Loss?\n",
    "\n",
    "Contrastive loss (also called InfoNCE or NT-Xent) learns by:\n",
    "- Pulling similar pairs together\n",
    "- Pushing dissimilar pairs apart\n",
    "\n",
    "**Key difference from Triplet Loss:**\n",
    "- Triplet: Uses explicit anchor-positive-negative\n",
    "- Contrastive: Uses pairs with similarity labels (0 or 1)\n",
    "\n",
    "```\n",
    "Pair 1: [\"How to reset password?\", \"Click forgot password\", label=1.0] â† Similar\n",
    "Pair 2: [\"How to reset password?\", \"Pricing starts at $29\", label=0.0] â† Dissimilar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load hard pairs from extended dataset (500 examples)\nwith open('../fixtures/input/training_pairs_hard.json', 'r') as f:\n    pairs_data = json.load(f)\n\nprint(f\"Loaded {len(pairs_data)} hard training examples\\n\")\n\n# Show example\nexample = pairs_data[0]\nprint(\"Training example structure:\")\nprint(f\"  Query: {example['query']}\")\nprint(f\"  Positive: {example['positive']}\")\nprint(f\"  Negative: {example['negative']}\")\n\nprint(\"\\nðŸ’¡ Hard negatives are semantically similar but not relevant\")\nprint(\"   This makes the model learn fine-grained distinctions!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Contrastive Pairs\n",
    "\n",
    "Create both positive pairs (label=1.0) and negative pairs (label=0.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create InputExample objects with labels\n",
    "train_examples = []\n",
    "\n",
    "for item in pairs_data:\n",
    "    # Positive pair (label = 1.0)\n",
    "    train_examples.append(\n",
    "        InputExample(texts=[item['query'], item['positive']], label=1.0)\n",
    "    )\n",
    "    \n",
    "    # Negative pair (label = 0.0)\n",
    "    train_examples.append(\n",
    "        InputExample(texts=[item['query'], item['negative']], label=0.0)\n",
    "    )\n",
    "\n",
    "print(f\"Created {len(train_examples)} training pairs\")\n",
    "print(f\"  Positive pairs: {len(train_examples) // 2}\")\n",
    "print(f\"  Negative pairs: {len(train_examples) // 2}\")\n",
    "print(f\"\\nExample format:\")\n",
    "print(f\"  Texts: {train_examples[0].texts}\")\n",
    "print(f\"  Label: {train_examples[0].label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 80/20\n",
    "split_idx = int(0.8 * len(train_examples))\n",
    "train_data = train_examples[:split_idx]\n",
    "val_data = train_examples[split_idx:]\n",
    "\n",
    "print(f\"Training pairs: {len(train_data)}\")\n",
    "print(f\"Validation pairs: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    shuffle=True,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "print(f\"DataLoader created:\")\n",
    "print(f\"  Batch size: 16\")\n",
    "print(f\"  Number of batches: {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Contrastive Loss (OnlineContrastiveLoss)\n",
    "\n",
    "**Parameters:**\n",
    "- `distance_metric`: Cosine or Euclidean\n",
    "- `margin`: Minimum distance for negative pairs\n",
    "- `size_average`: Average loss over batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Define contrastive loss\n",
    "train_loss = losses.OnlineContrastiveLoss(\n",
    "    model=model,\n",
    "    distance_metric=losses.SiameseDistanceMetric.COSINE_DISTANCE,\n",
    "    margin=0.5  # Negative pairs should be at least 0.5 apart\n",
    ")\n",
    "\n",
    "print(\"âœ“ Loss function: OnlineContrastiveLoss\")\n",
    "print(\"  Distance metric: Cosine\")\n",
    "print(\"  Margin: 0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare validation data\n",
    "val_sentences1 = [ex.texts[0] for ex in val_data]\n",
    "val_sentences2 = [ex.texts[1] for ex in val_data]\n",
    "val_scores = [ex.label for ex in val_data]\n",
    "\n",
    "evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1=val_sentences1,\n",
    "    sentences2=val_sentences2,\n",
    "    scores=val_scores,\n",
    "    name='contrastive_eval'\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Evaluator created with {len(val_data)} validation pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune with Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training configuration\nnum_epochs = 3  # Reduced for 500 examples - more data needs fewer epochs\nwarmup_steps = int(0.1 * len(train_dataloader) * num_epochs)\neval_steps = max(50, len(train_dataloader) // 4)  # Evaluate 4 times per epoch\n\nprint(f\"Training configuration:\")\nprint(f\"  Total examples: {len(train_data)} ({len(train_data)//2} positive + {len(train_data)//2} negative)\")\nprint(f\"  Hard negatives: âœ“ Challenging examples\")\nprint(f\"  Epochs: {num_epochs}\")\nprint(f\"  Warmup steps: {warmup_steps}\")\nprint(f\"  Evaluation frequency: every {eval_steps} steps\")\nprint(f\"  Total steps: {len(train_dataloader) * num_epochs}\")\n\n# Evaluate before training\nprint(\"\\n\" + \"=\"*60)\nprint(\"INITIAL EVALUATION (before training)\")\nprint(\"=\"*60)\ninitial_result = evaluator(model, output_path='../output/contrastive_finetuned_model')\n# EmbeddingSimilarityEvaluator returns dict like {'contrastive_eval_cosine_spearman': 0.82}\nif isinstance(initial_result, dict):\n    # Get spearman correlation value\n    if 'cosine_spearman' in str(initial_result):\n        initial_score = [v for k, v in initial_result.items() if 'spearman' in k][0]\n    else:\n        initial_score = list(initial_result.values())[0]\nelse:\n    initial_score = initial_result\nprint(f\"Initial Spearman Correlation: {initial_score:.4f}\")\nprint(f\"  (1.0 = perfect, 0.0 = no correlation, -1.0 = inverse)\")\nprint(\"=\"*60 + \"\\n\")\n\n# Fine-tune\nprint(\"Starting fine-tuning with contrastive loss on HARD examples...\")\nprint(\"Watch for evaluation outputs below:\\n\")\n\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    epochs=num_epochs,\n    warmup_steps=warmup_steps,\n    evaluator=evaluator,\n    evaluation_steps=eval_steps,  # Evaluate more frequently\n    output_path='../output/contrastive_finetuned_model',\n    show_progress_bar=True,\n    save_best_model=True,  # Only save best model\n    use_amp=False,  # Disable mixed precision\n    checkpoint_save_steps=10000,  # Don't save intermediate checkpoints\n    checkpoint_save_total_limit=1,  # Keep only best model\n    optimizer_params={'lr': 2e-5}  # Learning rate for better convergence\n)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"âœ“ Fine-tuning complete!\")\nprint(\"=\"*60)\n\n# Load and display metrics\nimport pandas as pd\nimport os\n\n# NOTE: sentence-transformers saves evaluation results to 'eval/' subfolder\n# with prefix from evaluator class name (e.g., 'similarity_evaluation_')\nmetrics_file = '../output/contrastive_finetuned_model/eval/similarity_evaluation_contrastive_eval_results.csv'\nif os.path.exists(metrics_file):\n    print(\"\\n\" + \"=\"*60)\n    print(\"TRAINING METRICS SUMMARY\")\n    print(\"=\"*60)\n    metrics_df = pd.read_csv(metrics_file)\n    \n    # Filter out pre-training evaluation (epoch=-1, steps=-1)\n    metrics_df = metrics_df[(metrics_df['epoch'] >= 0) & (metrics_df['steps'] >= 0)]\n    \n    # Show first and last few rows\n    print(\"\\nFirst evaluations:\")\n    print(metrics_df.head(3).to_string(index=False))\n    if len(metrics_df) > 6:\n        print(\"\\n...\")\n        print(\"\\nFinal evaluations:\")\n        print(metrics_df.tail(3).to_string(index=False))\n    \n    print(\"\\n\" + \"=\"*60)\n    # EmbeddingSimilarityEvaluator tracks multiple metrics\n    if 'cosine_spearman' in metrics_df.columns:\n        metric_col = 'cosine_spearman'\n    elif 'spearman' in metrics_df.columns:\n        metric_col = 'spearman'\n    else:\n        metric_col = metrics_df.columns[-1]\n    \n    best_score = metrics_df[metric_col].max()\n    best_step = metrics_df.loc[metrics_df[metric_col].idxmax(), 'steps']\n    improvement = best_score - initial_score\n    improvement_pct = (improvement / abs(initial_score) * 100) if initial_score != 0 else 0\n    \n    print(f\"ðŸ“Š RESULTS:\")\n    print(f\"  Initial Score:  {initial_score:.4f}\")\n    print(f\"  Best Score:     {best_score:.4f} (at step {int(best_step)})\")\n    print(f\"  Improvement:    +{improvement:.4f} ({improvement_pct:+.1f}%)\")\n    print(\"=\"*60)\n    \n    # Note about training loss\n    print(\"\\nðŸ’¡ NOTE: 'No log' in Training/Validation Loss columns is NORMAL.\")\n    print(\"   sentence-transformers logs evaluation metrics (spearman)\")\n    print(\"   but doesn't log raw loss values in the progress bar.\")\n    print(\"   The evaluation metrics are saved to CSV and shown above.\")\n    \n    if improvement > 0.1:\n        print(\"\\nâœ… Model successfully improved on hard negatives!\")\n    elif improvement > 0:\n        print(\"\\nâš ï¸  Small improvement - hard negatives are challenging\")\n    else:\n        print(\"\\nâš ï¸  No improvement - may need more epochs or different hyperparameters\")\nelse:\n    print(\"\\nâš ï¸  Metrics file not found at:\")\n    print(f\"   {metrics_file}\")\n    print(\"   Check that the evaluator and output_path are correctly configured\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Visualize Training Progress",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Plot training progress\nimport pandas as pd\nimport os\n\nmetrics_file = '../output/contrastive_finetuned_model/eval/similarity_evaluation_contrastive_eval_results.csv'\nif os.path.exists(metrics_file):\n    metrics_df = pd.read_csv(metrics_file)\n    \n    # Filter out pre-training evaluation (epoch=-1, steps=-1)\n    metrics_df = metrics_df[(metrics_df['epoch'] >= 0) & (metrics_df['steps'] >= 0)]\n    \n    # Determine which metric column to use\n    metric_col = None\n    if 'cosine_spearman' in metrics_df.columns:\n        metric_col = 'cosine_spearman'\n        metric_name = 'Spearman Correlation (Cosine)'\n    elif 'spearman' in metrics_df.columns:\n        metric_col = 'spearman'\n        metric_name = 'Spearman Correlation'\n    \n    if metric_col:\n        fig, ax = plt.subplots(figsize=(12, 5))\n        \n        ax.plot(metrics_df['steps'], metrics_df[metric_col], marker='o', linewidth=2, markersize=6)\n        ax.axhline(initial_score, color='red', linestyle='--', label=f'Initial: {initial_score:.4f}')\n        ax.set_xlabel('Training Steps', fontsize=12)\n        ax.set_ylabel(metric_name, fontsize=12)\n        ax.set_title('Contrastive Loss Training Progress', fontsize=14, fontweight='bold')\n        ax.grid(True, alpha=0.3)\n        ax.legend()\n        \n        # Annotate best score\n        best_idx = metrics_df[metric_col].idxmax()\n        best_step = metrics_df.loc[best_idx, 'steps']\n        best_score = metrics_df.loc[best_idx, metric_col]\n        ax.annotate(f'Best: {best_score:.4f}', \n                    xy=(best_step, best_score),\n                    xytext=(10, -20),\n                    textcoords='offset points',\n                    bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.7),\n                    arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n        \n        plt.tight_layout()\n        plt.show()\n        \n        print(f\"\\n{metric_name} improved from {initial_score:.4f} to {best_score:.4f}\")\n        print(f\"Total improvement: +{(best_score - initial_score):.4f} ({((best_score - initial_score) / abs(initial_score) * 100):.1f}%)\")\n    else:\n        print(\"Metric columns found:\", metrics_df.columns.tolist())\nelse:\n    print(\"âš ï¸  No metrics file found. Training may not have completed or evaluator didn't run.\")\n    print(f\"   Expected path: {metrics_file}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Generic vs Fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "generic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "finetuned_model = SentenceTransformer('../output/contrastive_finetuned_model')\n",
    "\n",
    "# Test on validation examples\n",
    "test_examples = val_data[:20]\n",
    "\n",
    "generic_correct = 0\n",
    "finetuned_correct = 0\n",
    "\n",
    "for ex in test_examples:\n",
    "    # Generic\n",
    "    emb1 = generic_model.encode(ex.texts[0])\n",
    "    emb2 = generic_model.encode(ex.texts[1])\n",
    "    gen_sim = cosine_similarity([emb1], [emb2])[0][0]\n",
    "    \n",
    "    # Fine-tuned\n",
    "    emb1 = finetuned_model.encode(ex.texts[0])\n",
    "    emb2 = finetuned_model.encode(ex.texts[1])\n",
    "    ft_sim = cosine_similarity([emb1], [emb2])[0][0]\n",
    "    \n",
    "    # Check if prediction matches label\n",
    "    # Positive pairs (label=1.0) should have similarity > 0.5\n",
    "    # Negative pairs (label=0.0) should have similarity < 0.5\n",
    "    threshold = 0.5\n",
    "    \n",
    "    if ex.label == 1.0:\n",
    "        if gen_sim > threshold:\n",
    "            generic_correct += 1\n",
    "        if ft_sim > threshold:\n",
    "            finetuned_correct += 1\n",
    "    else:\n",
    "        if gen_sim < threshold:\n",
    "            generic_correct += 1\n",
    "        if ft_sim < threshold:\n",
    "            finetuned_correct += 1\n",
    "\n",
    "print(f\"Classification accuracy (threshold=0.5):\")\n",
    "print(f\"  Generic model: {generic_correct}/{len(test_examples)} = {100*generic_correct/len(test_examples):.1f}%\")\n",
    "print(f\"  Fine-tuned model: {finetuned_correct}/{len(test_examples)} = {100*finetuned_correct/len(test_examples):.1f}%\")\n",
    "print(f\"\\nâœ“ Improvement: +{finetuned_correct - generic_correct} correct predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Similarity Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect similarities for positive and negative pairs\n",
    "gen_pos_sims, gen_neg_sims = [], []\n",
    "ft_pos_sims, ft_neg_sims = [], []\n",
    "\n",
    "for ex in val_data[:50]:\n",
    "    # Generic\n",
    "    emb1 = generic_model.encode(ex.texts[0])\n",
    "    emb2 = generic_model.encode(ex.texts[1])\n",
    "    gen_sim = cosine_similarity([emb1], [emb2])[0][0]\n",
    "    \n",
    "    # Fine-tuned\n",
    "    emb1 = finetuned_model.encode(ex.texts[0])\n",
    "    emb2 = finetuned_model.encode(ex.texts[1])\n",
    "    ft_sim = cosine_similarity([emb1], [emb2])[0][0]\n",
    "    \n",
    "    if ex.label == 1.0:\n",
    "        gen_pos_sims.append(gen_sim)\n",
    "        ft_pos_sims.append(ft_sim)\n",
    "    else:\n",
    "        gen_neg_sims.append(gen_sim)\n",
    "        ft_neg_sims.append(ft_sim)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Generic model\n",
    "axes[0].hist(gen_pos_sims, alpha=0.6, label='Positive pairs', bins=15, color='green')\n",
    "axes[0].hist(gen_neg_sims, alpha=0.6, label='Negative pairs', bins=15, color='red')\n",
    "axes[0].axvline(0.5, color='black', linestyle='--', label='Threshold')\n",
    "axes[0].set_xlabel('Similarity')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Generic Model')\n",
    "axes[0].legend()\n",
    "\n",
    "# Fine-tuned model\n",
    "axes[1].hist(ft_pos_sims, alpha=0.6, label='Positive pairs', bins=15, color='green')\n",
    "axes[1].hist(ft_neg_sims, alpha=0.6, label='Negative pairs', bins=15, color='red')\n",
    "axes[1].axvline(0.5, color='black', linestyle='--', label='Threshold')\n",
    "axes[1].set_xlabel('Similarity')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Fine-tuned Model (Contrastive Loss)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Better separation = Less overlap between green and red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Similarity Statistics:\\n\")\n",
    "\n",
    "print(\"Generic Model:\")\n",
    "print(f\"  Positive pairs: Î¼={np.mean(gen_pos_sims):.3f}, Ïƒ={np.std(gen_pos_sims):.3f}\")\n",
    "print(f\"  Negative pairs: Î¼={np.mean(gen_neg_sims):.3f}, Ïƒ={np.std(gen_neg_sims):.3f}\")\n",
    "print(f\"  Separation: {np.mean(gen_pos_sims) - np.mean(gen_neg_sims):.3f}\")\n",
    "\n",
    "print(\"\\nFine-tuned Model:\")\n",
    "print(f\"  Positive pairs: Î¼={np.mean(ft_pos_sims):.3f}, Ïƒ={np.std(ft_pos_sims):.3f}\")\n",
    "print(f\"  Negative pairs: Î¼={np.mean(ft_neg_sims):.3f}, Ïƒ={np.std(ft_neg_sims):.3f}\")\n",
    "print(f\"  Separation: {np.mean(ft_pos_sims) - np.mean(ft_neg_sims):.3f}\")\n",
    "\n",
    "improvement = (\n",
    "    (np.mean(ft_pos_sims) - np.mean(ft_neg_sims)) - \n",
    "    (np.mean(gen_pos_sims) - np.mean(gen_neg_sims))\n",
    ")\n",
    "print(f\"\\nâœ“ Separation improvement: +{improvement:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Real Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    {\n",
    "        \"query\": \"forgot my login credentials\",\n",
    "        \"docs\": [\n",
    "            \"Reset password from account settings page\",\n",
    "            \"Contact support for billing questions\",\n",
    "            \"API documentation available online\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"what does BERT do?\",\n",
    "        \"docs\": [\n",
    "            \"BERT is bidirectional transformer for NLP\",\n",
    "            \"Database backup runs nightly at midnight\",\n",
    "            \"Professional plan includes 25 users\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Query Ranking Test:\\n\")\n",
    "\n",
    "for test in test_queries:\n",
    "    print(f\"Query: {test['query']}\")\n",
    "    \n",
    "    # Generic rankings\n",
    "    q_emb = generic_model.encode(test['query'])\n",
    "    doc_embs = generic_model.encode(test['docs'])\n",
    "    gen_sims = [cosine_similarity([q_emb], [d])[0][0] for d in doc_embs]\n",
    "    gen_ranking = np.argsort(gen_sims)[::-1]\n",
    "    \n",
    "    # Fine-tuned rankings\n",
    "    q_emb = finetuned_model.encode(test['query'])\n",
    "    doc_embs = finetuned_model.encode(test['docs'])\n",
    "    ft_sims = [cosine_similarity([q_emb], [d])[0][0] for d in doc_embs]\n",
    "    ft_ranking = np.argsort(ft_sims)[::-1]\n",
    "    \n",
    "    print(\"\\n  Generic Model Ranking:\")\n",
    "    for rank, idx in enumerate(gen_ranking, 1):\n",
    "        print(f\"    {rank}. [{gen_sims[idx]:.3f}] {test['docs'][idx][:50]}...\")\n",
    "    \n",
    "    print(\"\\n  Fine-tuned Model Ranking:\")\n",
    "    for rank, idx in enumerate(ft_ranking, 1):\n",
    "        print(f\"    {rank}. [{ft_sims[idx]:.3f}] {test['docs'][idx][:50]}...\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "âœ… Loaded 100 examples and created 200 pairs (positive + negative)  \n",
    "âœ… Used OnlineContrastiveLoss with 0.5 margin  \n",
    "âœ… Achieved better separation between similar and dissimilar  \n",
    "âœ… Improved classification accuracy  \n",
    "âœ… Better ranking of relevant documents  \n",
    "\n",
    "**Key learnings:**\n",
    "- Contrastive loss works with labeled pairs\n",
    "- Simpler than triplet loss (no need for explicit negatives)\n",
    "- Margin controls minimum distance for dissimilar pairs\n",
    "- Works well with 200+ pairs (100 positive + 100 negative)\n",
    "- Better separation in similarity distributions\n",
    "\n",
    "**Contrastive vs Triplet:**\n",
    "- **Contrastive**: Binary labels (similar/dissimilar), simpler data prep\n",
    "- **Triplet**: Explicit triplets, more fine-grained control\n",
    "- Both improve over generic models\n",
    "\n",
    "**Next:** Learn full training with HuggingFace Transformers Trainer API!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}