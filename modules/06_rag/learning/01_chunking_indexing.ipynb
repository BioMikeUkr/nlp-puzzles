{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Chunking and Indexing\n",
    "\n",
    "Learn how to chunk documents and build searchable indexes for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample documents\n",
    "with open('../fixtures/input/documents.json', 'r') as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "print(f\"\\nFirst document:\")\n",
    "print(f\"  Source: {documents[0]['source']}\")\n",
    "print(f\"  Length: {len(documents[0]['content'])} chars\")\n",
    "print(f\"  Content preview: {documents[0]['content'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking Strategies\n",
    "\n",
    "### 1. Fixed-Size Chunks (Naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple fixed-size chunking\n",
    "def fixed_size_chunk(text, size=500):\n",
    "    \"\"\"Split text into fixed-size chunks\"\"\"\n",
    "    return [text[i:i+size] for i in range(0, len(text), size)]\n",
    "\n",
    "# Test on first document\n",
    "text = documents[0]['content']\n",
    "chunks = fixed_size_chunk(text, size=300)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks\\n\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1} ({len(chunk)} chars):\")\n",
    "    print(f\"  {chunk[:100]}...\\n\")\n",
    "\n",
    "print(\"⚠️  Problem: Can split mid-sentence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Recursive Character Splitting (Better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smart chunking with RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,  # Overlap to preserve context\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Try these in order\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks\\n\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1} ({len(chunk)} chars):\")\n",
    "    print(f\"  {chunk[:100]}...\\n\")\n",
    "\n",
    "print(\"✓ Splits at natural boundaries\")\n",
    "print(\"✓ Overlap preserves context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk Overlap Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show overlap between consecutive chunks\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "sample_text = \"Python is a programming language. It was created by Guido van Rossum. Python emphasizes code readability. It has a large standard library.\"\n",
    "\n",
    "chunks = splitter.split_text(sample_text)\n",
    "\n",
    "print(\"Chunks with overlap:\\n\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: '{chunk}'\")\n",
    "\n",
    "    if i < len(chunks) - 1:\n",
    "        # Find overlap\n",
    "        next_chunk = chunks[i+1]\n",
    "        overlap = \"\"\n",
    "        for j in range(1, min(len(chunk), len(next_chunk))):\n",
    "            if chunk[-j:] == next_chunk[:j]:\n",
    "                overlap = chunk[-j:]\n",
    "\n",
    "        if overlap:\n",
    "            print(f\"  Overlap with next: '{overlap[:50]}...'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk All Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk all documents and track metadata\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=120\n",
    ")\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "for doc in documents:\n",
    "    doc_chunks = splitter.split_text(doc['content'])\n",
    "\n",
    "    for i, chunk_text in enumerate(doc_chunks):\n",
    "        all_chunks.append({\n",
    "            'text': chunk_text,\n",
    "            'source': doc['source'],\n",
    "            'doc_id': doc['doc_id'],\n",
    "            'chunk_id': i,\n",
    "            'metadata': doc['metadata']\n",
    "        })\n",
    "\n",
    "print(f\"Total chunks: {len(all_chunks)}\")\n",
    "print(f\"\\nChunks per document:\")\n",
    "for doc in documents:\n",
    "    doc_chunk_count = sum(1 for c in all_chunks if c['doc_id'] == doc['doc_id'])\n",
    "    print(f\"  {doc['source']}: {doc_chunk_count} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Embed all chunks\n",
    "chunk_texts = [c['text'] for c in all_chunks]\n",
    "embeddings = embed_model.encode(\n",
    "    chunk_texts,\n",
    "    normalize_embeddings=True,\n",
    "    show_progress_bar=True\n",
    ").astype('float32')\n",
    "\n",
    "print(f\"\\nEmbeddings shape: {embeddings.shape}\")\n",
    "print(f\"Dimension: {embeddings.shape[1]}\")\n",
    "print(f\"dtype: {embeddings.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner product for normalized vectors\n",
    "\n",
    "# Add embeddings\n",
    "index.add(embeddings)\n",
    "\n",
    "print(f\"✓ Index created\")\n",
    "print(f\"  Total vectors: {index.ntotal}\")\n",
    "print(f\"  Dimension: {index.d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str, k: int = 5):\n",
    "    \"\"\"Search for relevant chunks\"\"\"\n",
    "    # Embed query\n",
    "    query_emb = embed_model.encode(\n",
    "        query,\n",
    "        normalize_embeddings=True\n",
    "    ).astype('float32').reshape(1, -1)\n",
    "\n",
    "    # Search\n",
    "    scores, indices = index.search(query_emb, k)\n",
    "\n",
    "    # Format results\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        chunk = all_chunks[idx]\n",
    "        results.append({\n",
    "            'text': chunk['text'],\n",
    "            'source': chunk['source'],\n",
    "            'score': float(score),\n",
    "            'metadata': chunk['metadata']\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test queries\n",
    "queries = [\n",
    "    \"What is Python?\",\n",
    "    \"How much does the Professional plan cost?\",\n",
    "    \"What was Q4 revenue?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    results = search(query, k=3)\n",
    "\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. [{result['score']:.3f}] {result['source']}\")\n",
    "        print(f\"   {result['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Chunk Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Analyze chunk length distribution\n",
    "chunk_lengths = [len(c['text']) for c in all_chunks]\n",
    "\n",
    "print(f\"Chunk length statistics:\")\n",
    "print(f\"  Min: {min(chunk_lengths)} chars\")\n",
    "print(f\"  Max: {max(chunk_lengths)} chars\")\n",
    "print(f\"  Mean: {np.mean(chunk_lengths):.0f} chars\")\n",
    "print(f\"  Median: {np.median(chunk_lengths):.0f} chars\")\n",
    "\n",
    "# Histogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(chunk_lengths, bins=20, edgecolor='black')\n",
    "plt.xlabel('Chunk Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Chunk Lengths')\n",
    "plt.axvline(600, color='r', linestyle='--', label='Target size')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('../output', exist_ok=True)\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(index, '../output/documents.index')\n",
    "\n",
    "# Save chunks metadata\n",
    "with open('../output/chunks.json', 'w') as f:\n",
    "    json.dump(all_chunks, f, indent=2)\n",
    "\n",
    "print(\"✓ Saved index and chunks\")\n",
    "print(f\"  Index: ../output/documents.index\")\n",
    "print(f\"  Chunks: ../output/chunks.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ Learned chunking strategies  \n",
    "✅ Understood chunk overlap importance  \n",
    "✅ Generated embeddings for chunks  \n",
    "✅ Built FAISS index  \n",
    "✅ Tested basic retrieval  \n",
    "✅ Saved index for reuse\n",
    "\n",
    "**Key takeaways:**\n",
    "- Use RecursiveCharacterTextSplitter for semantic boundaries\n",
    "- 20% overlap prevents context loss\n",
    "- Track metadata (source, chunk_id) for citations\n",
    "- Normalize embeddings for cosine similarity\n",
    "\n",
    "**Next:** Learn RAG generation with LLMs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
