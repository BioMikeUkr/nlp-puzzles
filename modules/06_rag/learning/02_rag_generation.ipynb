{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Generation with LLMs\n",
    "\n",
    "Learn to combine retrieval with LLM generation for question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import json\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "**Important:** Set your OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "api_key = \"your-api-key-here\"\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Load models\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Load index and chunks from previous notebook\n",
    "index = faiss.read_index('../output/documents.index')\n",
    "with open('../output/chunks.json', 'r') as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {index.ntotal} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query: str, k: int = 5):\n",
    "    \"\"\"Retrieve relevant chunks\"\"\"\n",
    "    query_emb = embed_model.encode(\n",
    "        query,\n",
    "        normalize_embeddings=True\n",
    "    ).astype('float32').reshape(1, -1)\n",
    "\n",
    "    scores, indices = index.search(query_emb, k)\n",
    "\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        results.append({\n",
    "            **chunks[idx],\n",
    "            'score': float(score)\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "def rag(question: str, k: int = 5):\n",
    "    \"\"\"RAG: Retrieve + Generate\"\"\"\n",
    "\n",
    "    # 1. Retrieve relevant chunks\n",
    "    relevant_chunks = retrieve(question, k=k)\n",
    "\n",
    "    # 2. Build context\n",
    "    context = \"\\n\\n\".join([c['text'] for c in relevant_chunks])\n",
    "\n",
    "    # 3. Generate answer\n",
    "    prompt = f\"\"\"Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on provided context.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test\n",
    "question = \"What is Python?\"\n",
    "answer = rag(question)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_citations(question: str, k: int = 5):\n",
    "    \"\"\"RAG with source tracking\"\"\"\n",
    "\n",
    "    # Retrieve\n",
    "    relevant_chunks = retrieve(question, k=k)\n",
    "\n",
    "    # Build numbered context\n",
    "    context_parts = []\n",
    "    sources = []\n",
    "\n",
    "    for i, chunk in enumerate(relevant_chunks, 1):\n",
    "        context_parts.append(f\"[{i}] {chunk['text']}\")\n",
    "        sources.append({\n",
    "            'id': i,\n",
    "            'source': chunk['source'],\n",
    "            'score': chunk['score']\n",
    "        })\n",
    "\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    # Generate with citation instruction\n",
    "    prompt = f\"\"\"Answer the question based on the context below.\n",
    "Cite sources using [1], [2], etc.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Answer questions with citations.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'answer': response.choices[0].message.content,\n",
    "        'sources': sources\n",
    "    }\n",
    "\n",
    "# Test\n",
    "result = rag_with_citations(\"How much does the Professional plan cost?\")\n",
    "\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"\\nSources:\")\n",
    "for source in result['sources']:\n",
    "    print(f\"  [{source['id']}] {source['source']} (score: {source['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token-Aware Context Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context_with_limit(chunks, max_tokens=3000):\n",
    "    \"\"\"Build context respecting token limit\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "    context_parts = []\n",
    "    sources = []\n",
    "    total_tokens = 0\n",
    "\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        chunk_text = f\"[{i}] {chunk['text']}\"\n",
    "        chunk_tokens = len(encoding.encode(chunk_text))\n",
    "\n",
    "        if total_tokens + chunk_tokens > max_tokens:\n",
    "            print(f\"⚠️  Stopped at chunk {i-1} (token limit reached)\")\n",
    "            break\n",
    "\n",
    "        context_parts.append(chunk_text)\n",
    "        sources.append({'id': i, 'source': chunk['source']})\n",
    "        total_tokens += chunk_tokens\n",
    "\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    print(f\"✓ Built context: {total_tokens} tokens, {len(sources)} chunks\")\n",
    "\n",
    "    return context, sources\n",
    "\n",
    "# Test\n",
    "relevant_chunks = retrieve(\"What is machine learning?\", k=10)\n",
    "context, sources = build_context_with_limit(relevant_chunks, max_tokens=500)\n",
    "\n",
    "print(f\"\\nContext:\\n{context}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare: With vs Without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What was our Q4 2023 revenue?\"\n",
    "\n",
    "# Without RAG (LLM only)\n",
    "print(\"WITHOUT RAG:\")\n",
    "print(\"=\"*60)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# With RAG\n",
    "print(\"\\n\\nWITH RAG:\")\n",
    "print(\"=\"*60)\n",
    "answer = rag(question)\n",
    "print(answer)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RAG provides accurate, up-to-date information!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling 'Not in Context'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_strict(question: str, k: int = 5):\n",
    "    \"\"\"RAG that only answers from context\"\"\"\n",
    "\n",
    "    relevant_chunks = retrieve(question, k=k)\n",
    "    context = \"\\n\\n\".join([c['text'] for c in relevant_chunks])\n",
    "\n",
    "    prompt = f\"\"\"Answer ONLY based on the context below.\n",
    "If the answer is not in the context, respond with: \"I don't have that information in the provided context.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Answer only from provided context.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test with question NOT in documents\n",
    "question = \"What is the weather today?\"\n",
    "answer = rag_strict(question)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(\"\\n✓ Correctly refuses to hallucinate!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Multiple Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test queries\n",
    "with open('../fixtures/input/test_queries.json', 'r') as f:\n",
    "    test_queries = json.load(f)\n",
    "\n",
    "# Test RAG on all queries\n",
    "for query_data in test_queries[:5]:  # First 5\n",
    "    question = query_data['query']\n",
    "\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    result = rag_with_citations(question, k=3)\n",
    "\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"\\nSources: {[s['source'] for s in result['sources']]}\")\n",
    "    print(f\"Expected docs: {query_data['expected_doc_ids']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ Built basic RAG pipeline  \n",
    "✅ Added citation tracking  \n",
    "✅ Implemented token-aware context  \n",
    "✅ Handled non-answerable questions  \n",
    "✅ Compared with/without RAG\n",
    "\n",
    "**Key patterns:**\n",
    "- Retrieve → Build Context → Generate\n",
    "- Always check token limits\n",
    "- Number chunks for citations\n",
    "- Instruct LLM to stay in context\n",
    "- Use temperature=0 for factual answers\n",
    "\n",
    "**Next:** Build production RAG in tasks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
