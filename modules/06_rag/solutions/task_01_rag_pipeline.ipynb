{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Build RAG Pipeline - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import faiss\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"your-api-key-here\"\n",
    "client = OpenAI(api_key=api_key)\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "with open('../fixtures/input/documents.json', 'r') as f:\n",
    "    documents = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Chunk Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=120\n",
    ")\n",
    "\n",
    "all_chunks = []\n",
    "for doc in documents:\n",
    "    doc_chunks = splitter.split_text(doc['content'])\n",
    "    for i, chunk_text in enumerate(doc_chunks):\n",
    "        all_chunks.append({\n",
    "            'text': chunk_text,\n",
    "            'source': doc['source'],\n",
    "            'doc_id': doc['doc_id'],\n",
    "            'chunk_id': i\n",
    "        })\n",
    "\n",
    "print(f\"✓ Created {len(all_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Build FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "chunk_texts = [c['text'] for c in all_chunks]\n",
    "embeddings = embed_model.encode(\n",
    "    chunk_texts,\n",
    "    normalize_embeddings=True,\n",
    "    show_progress_bar=True\n",
    ").astype('float32')\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "print(f\"✓ Indexed {index.ntotal} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Implement Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def retrieve(query: str, k: int = 5):\n",
    "    query_emb = embed_model.encode(\n",
    "        query,\n",
    "        normalize_embeddings=True\n",
    "    ).astype('float32').reshape(1, -1)\n",
    "\n",
    "    scores, indices = index.search(query_emb, k)\n",
    "\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        results.append({\n",
    "            **all_chunks[idx],\n",
    "            'score': float(score)\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "print(\"✓ Retrieve function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Implement RAG with Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def rag(question: str, k: int = 5):\n",
    "    # Retrieve\n",
    "    chunks = retrieve(question, k=k)\n",
    "\n",
    "    # Build context with citations\n",
    "    context_parts = []\n",
    "    sources = []\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        context_parts.append(f\"[{i}] {chunk['text']}\")\n",
    "        sources.append({\n",
    "            'id': i,\n",
    "            'source': chunk['source'],\n",
    "            'doc_id': chunk['doc_id']\n",
    "        })\n",
    "\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    # Generate\n",
    "    prompt = f\"\"\"Answer based on context. Cite sources using [1], [2], etc.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Answer with citations.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'answer': response.choices[0].message.content,\n",
    "        'sources': sources\n",
    "    }\n",
    "\n",
    "print(\"✓ RAG function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Test on Multiple Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "with open('../fixtures/input/test_queries.json', 'r') as f:\n",
    "    test_queries = json.load(f)\n",
    "\n",
    "correct = 0\n",
    "for query_data in test_queries:\n",
    "    result = rag(query_data['query'], k=3)\n",
    "    retrieved_doc_ids = [s['doc_id'] for s in result['sources']]\n",
    "    expected = query_data['expected_doc_ids']\n",
    "\n",
    "    if any(doc_id in retrieved_doc_ids for doc_id in expected):\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / len(test_queries)\n",
    "print(f\"✓ Accuracy: {accuracy:.1%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
