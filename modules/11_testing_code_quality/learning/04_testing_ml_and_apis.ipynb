{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19cb9d4f",
   "metadata": {},
   "source": [
    "# 04 — Testing ML Pipelines & APIs\n",
    "\n",
    "This notebook covers:\n",
    "- Testing ML pipelines with reproducible seeds\n",
    "- FastAPI TestClient (with lifespan!)\n",
    "- Mocking ML model predictions\n",
    "- Coverage reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5465e1f9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d86339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys, textwrap, tempfile, pathlib, os\n",
    "\n",
    "FIXTURES = os.path.join(os.path.dirname(os.path.abspath(\".\")),\n",
    "                        \"11_testing_code_quality\", \"fixtures\", \"input\")\n",
    "# Fallback for running from within the module dir\n",
    "if not os.path.exists(FIXTURES):\n",
    "    FIXTURES = os.path.join(os.path.abspath(\".\"), \"fixtures\", \"input\")\n",
    "if not os.path.exists(FIXTURES):\n",
    "    FIXTURES = os.path.join(os.path.abspath(\"..\"), \"fixtures\", \"input\")\n",
    "\n",
    "print(f\"Fixtures dir: {FIXTURES}\")\n",
    "assert os.path.exists(FIXTURES), f\"Fixtures not found at {FIXTURES}\"\n",
    "\n",
    "sys.path.insert(0, FIXTURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09888dd",
   "metadata": {},
   "source": [
    "## 1. Testing the Sample Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79ad4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sample_module import clean_text, tokenize, count_words, extract_emails, is_palindrome\n",
    "\n",
    "# Direct testing — no pytest needed for exploration\n",
    "assert clean_text(\"  hello   world  \") == \"hello world\"\n",
    "assert tokenize(\"Hello World\") == [\"hello\", \"world\"]\n",
    "assert tokenize(\"\") == []\n",
    "assert count_words(\"the cat sat on the mat\") == {\"the\": 2, \"cat\": 1, \"sat\": 1, \"on\": 1, \"mat\": 1}\n",
    "assert extract_emails(\"email me at user@example.com or admin@test.org\") == [\"user@example.com\", \"admin@test.org\"]\n",
    "assert is_palindrome(\"A man, a plan, a canal: Panama\") == True\n",
    "assert is_palindrome(\"hello\") == False\n",
    "\n",
    "print(\"All sample_module functions work correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce46ceb",
   "metadata": {},
   "source": [
    "## 2. Testing ML Pipeline — Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c0fa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sample_ml_pipeline import (\n",
    "    create_pipeline, train_pipeline, predict, evaluate,\n",
    "    SAMPLE_TEXTS, SAMPLE_LABELS, preprocess_texts\n",
    ")\n",
    "\n",
    "# Train with fixed seed — results should be reproducible\n",
    "np.random.seed(42)\n",
    "pipe1 = create_pipeline(max_features=100)\n",
    "train_pipeline(pipe1, SAMPLE_TEXTS, SAMPLE_LABELS)\n",
    "preds1 = predict(pipe1, SAMPLE_TEXTS)\n",
    "\n",
    "np.random.seed(42)\n",
    "pipe2 = create_pipeline(max_features=100)\n",
    "train_pipeline(pipe2, SAMPLE_TEXTS, SAMPLE_LABELS)\n",
    "preds2 = predict(pipe2, SAMPLE_TEXTS)\n",
    "\n",
    "assert np.array_equal(preds1, preds2), \"Same seed should give same predictions\"\n",
    "print(f\"Predictions (run 1): {preds1}\")\n",
    "print(f\"Predictions (run 2): {preds2}\")\n",
    "print(\"Reproducibility confirmed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d32749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate metrics\n",
    "metrics = evaluate(SAMPLE_LABELS, preds1.tolist())\n",
    "print(f\"Metrics: {metrics}\")\n",
    "assert 0 <= metrics[\"accuracy\"] <= 1\n",
    "assert 0 <= metrics[\"f1_macro\"] <= 1\n",
    "print(\"Metrics are in valid range.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a75fbe1",
   "metadata": {},
   "source": [
    "## 3. Testing FastAPI with TestClient\n",
    "\n",
    "**Important**: Starlette 0.50+ requires context manager for lifespan events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1910a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi.testclient import TestClient\n",
    "from sample_fastapi_app import app\n",
    "\n",
    "# CORRECT: context manager triggers lifespan (model loads)\n",
    "with TestClient(app) as client:\n",
    "    # Health check\n",
    "    resp = client.get(\"/health\")\n",
    "    assert resp.status_code == 200\n",
    "    data = resp.json()\n",
    "    assert data[\"status\"] == \"ok\"\n",
    "    assert data[\"model_loaded\"] is True\n",
    "    print(f\"Health: {data}\")\n",
    "\n",
    "    # Predict\n",
    "    resp = client.post(\"/predict\", json={\"text\": \"This is great and amazing\"})\n",
    "    assert resp.status_code == 200\n",
    "    pred = resp.json()\n",
    "    assert pred[\"label\"] in (\"positive\", \"negative\", \"neutral\")\n",
    "    assert 0 <= pred[\"score\"] <= 1\n",
    "    print(f\"Predict: {pred}\")\n",
    "\n",
    "    # Batch predict\n",
    "    resp = client.post(\"/predict/batch\", json={\"texts\": [\"good\", \"bad\", \"ok\"]})\n",
    "    assert resp.status_code == 200\n",
    "    batch = resp.json()\n",
    "    assert len(batch[\"predictions\"]) == 3\n",
    "    print(f\"Batch: {len(batch['predictions'])} predictions\")\n",
    "\n",
    "print(\"\\nAll API tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d61e650",
   "metadata": {},
   "source": [
    "## 4. Testing Validation Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a733c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with TestClient(app) as client:\n",
    "    # Empty text — should fail validation (min_length=1)\n",
    "    resp = client.post(\"/predict\", json={\"text\": \"\"})\n",
    "    assert resp.status_code == 422\n",
    "    print(f\"Empty text -> {resp.status_code} (validation error)\")\n",
    "\n",
    "    # Missing field\n",
    "    resp = client.post(\"/predict\", json={})\n",
    "    assert resp.status_code == 422\n",
    "    print(f\"Missing field -> {resp.status_code}\")\n",
    "\n",
    "    # Wrong type\n",
    "    resp = client.post(\"/predict\", json={\"text\": 123})\n",
    "    assert resp.status_code == 422\n",
    "    print(f\"Wrong type -> {resp.status_code}\")\n",
    "\n",
    "print(\"\\nValidation error handling works correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8091ab",
   "metadata": {},
   "source": [
    "## 5. Mocking the ML Model in API Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfd39d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import MagicMock\n",
    "from sample_fastapi_app import ml_models\n",
    "\n",
    "with TestClient(app) as client:\n",
    "    # Replace the real model with a mock\n",
    "    mock_model = MagicMock()\n",
    "    mock_model.predict.return_value = (\"positive\", 0.99)\n",
    "    ml_models[\"sentiment\"] = mock_model\n",
    "\n",
    "    resp = client.post(\"/predict\", json={\"text\": \"anything\"})\n",
    "    assert resp.status_code == 200\n",
    "    assert resp.json()[\"label\"] == \"positive\"\n",
    "    assert resp.json()[\"score\"] == 0.99\n",
    "\n",
    "    # Verify the mock was called\n",
    "    mock_model.predict.assert_called_once_with(\"anything\")\n",
    "\n",
    "print(\"Mock model works — we control predictions in tests!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11c0764",
   "metadata": {},
   "source": [
    "## 6. Running pytest with Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33885c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a proper test file and run with coverage\n",
    "test_content = textwrap.dedent(f'''\n",
    "import sys\n",
    "sys.path.insert(0, \"{FIXTURES}\")\n",
    "from sample_module import clean_text, tokenize, extract_emails\n",
    "\n",
    "def test_clean():\n",
    "    assert clean_text(\"  hi  there  \") == \"hi there\"\n",
    "\n",
    "def test_tokenize():\n",
    "    assert tokenize(\"Hello World\") == [\"hello\", \"world\"]\n",
    "\n",
    "def test_tokenize_empty():\n",
    "    assert tokenize(\"\") == []\n",
    "\n",
    "def test_emails():\n",
    "    assert extract_emails(\"a@b.com\") == [\"a@b.com\"]\n",
    "    assert extract_emails(\"no email\") == []\n",
    "''')\n",
    "\n",
    "with tempfile.TemporaryDirectory() as td:\n",
    "    p = pathlib.Path(td) / \"test_cov.py\"\n",
    "    p.write_text(test_content)\n",
    "    cmd = [\n",
    "        sys.executable, \"-m\", \"pytest\", str(p), \"-v\",\n",
    "        f\"--cov={FIXTURES}/sample_module.py\",  # not a package, but works for demo\n",
    "        \"--cov-report=term-missing\",\n",
    "        \"--tb=short\", \"--no-header\",\n",
    "    ]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)\n",
    "    print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ebd08",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Reproducibility**: always set `np.random.seed()` for ML tests\n",
    "2. **TestClient context manager**: required for lifespan events (Starlette 0.50+)\n",
    "3. **Mock ML models** to control predictions and speed up tests\n",
    "4. **Test validation errors** (422) — they're part of your API contract\n",
    "5. **Coverage**: `--cov-report=term-missing` shows exactly which lines need tests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
