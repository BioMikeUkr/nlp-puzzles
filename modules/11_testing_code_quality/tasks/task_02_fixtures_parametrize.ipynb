{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72274b86",
   "metadata": {},
   "source": [
    "# Task 02 — Fixtures & Parametrize\n",
    "\n",
    "Practice writing fixtures, using `conftest.py`, and advanced parametrize patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57c926e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cae158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys, os, textwrap, tempfile, pathlib\n",
    "\n",
    "FIXTURES = os.path.abspath(os.path.join(\"..\", \"fixtures\", \"input\"))\n",
    "if not os.path.exists(FIXTURES):\n",
    "    FIXTURES = os.path.abspath(os.path.join(\"fixtures\", \"input\"))\n",
    "sys.path.insert(0, FIXTURES)\n",
    "\n",
    "def run_pytest(test_code: str, extra_files: dict[str, str] | None = None, extra_args: list[str] | None = None):\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        td_path = pathlib.Path(td)\n",
    "        p = td_path / \"test_tmp.py\"\n",
    "        p.write_text(textwrap.dedent(test_code))\n",
    "        if extra_files:\n",
    "            for name, content in extra_files.items():\n",
    "                (td_path / name).write_text(textwrap.dedent(content))\n",
    "        cmd = [sys.executable, \"-m\", \"pytest\", str(td), \"-v\", \"--tb=short\", \"--no-header\"]\n",
    "        if extra_args:\n",
    "            cmd.extend(extra_args)\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)\n",
    "        print(result.stdout + result.stderr)\n",
    "        return result.returncode\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01126f0b",
   "metadata": {},
   "source": [
    "## Task 2.1: Create Fixtures for Text Processing\n",
    "\n",
    "Write a `conftest.py` with these fixtures:\n",
    "- `sample_texts` — returns a list of 5 diverse text strings\n",
    "- `empty_text` — returns \"\"\n",
    "- `text_with_emails` — returns a string containing 2+ email addresses\n",
    "\n",
    "Then write tests that use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1507500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "conftest_code = f'''\n",
    "import sys, pytest\n",
    "sys.path.insert(0, \"{FIXTURES}\")\n",
    "\n",
    "# @pytest.fixture\n",
    "# def sample_texts():\n",
    "#     ...\n",
    "'''\n",
    "\n",
    "test_code = f'''\n",
    "import sys\n",
    "sys.path.insert(0, \"{FIXTURES}\")\n",
    "from sample_module import tokenize, extract_emails\n",
    "\n",
    "# def test_tokenize_samples(sample_texts):\n",
    "#     for text in sample_texts:\n",
    "#         tokens = tokenize(text)\n",
    "#         assert isinstance(tokens, list)\n",
    "#\n",
    "# def test_extract_from_email_text(text_with_emails):\n",
    "#     emails = extract_emails(text_with_emails)\n",
    "#     assert len(emails) >= 2\n",
    "'''\n",
    "\n",
    "# TEST — Do not modify\n",
    "rc = run_pytest(test_code, extra_files={\"conftest.py\": conftest_code})\n",
    "assert rc == 0, \"Tests should pass\"\n",
    "print(\"Task 2.1 passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ad53ab",
   "metadata": {},
   "source": [
    "## Task 2.2: Parametrize ML Preprocessing\n",
    "\n",
    "Use `@pytest.mark.parametrize` to test `preprocess_texts` from `sample_ml_pipeline.py` with different inputs.\n",
    "\n",
    "Test cases should include:\n",
    "- Normal text → lowercased and stripped\n",
    "- Text with URLs → URLs removed\n",
    "- Text with extra whitespace → normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2596003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "test_preprocess = f'''\n",
    "import sys, pytest\n",
    "sys.path.insert(0, \"{FIXTURES}\")\n",
    "from sample_ml_pipeline import preprocess_texts\n",
    "\n",
    "# @pytest.mark.parametrize(\"texts, expected\", [\n",
    "#     ([\"Hello World\"], [\"hello world\"]),\n",
    "#     ([\"Check https://example.com out\"], [\"check out\"]),\n",
    "#     ...\n",
    "# ])\n",
    "# def test_preprocess(texts, expected):\n",
    "#     assert preprocess_texts(texts) == expected\n",
    "'''\n",
    "\n",
    "# TEST — Do not modify\n",
    "rc = run_pytest(test_preprocess)\n",
    "assert rc == 0, \"Tests should pass\"\n",
    "print(\"Task 2.2 passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e307cfd6",
   "metadata": {},
   "source": [
    "## Task 2.3: Use `tmp_path` for File-Based Tests\n",
    "\n",
    "Write a test that:\n",
    "1. Creates a CSV file in `tmp_path` with text data\n",
    "2. Reads it back with pandas\n",
    "3. Applies `clean_text` to each row\n",
    "4. Asserts the results are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96666cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "test_file = f'''\n",
    "import sys\n",
    "sys.path.insert(0, \"{FIXTURES}\")\n",
    "import pandas as pd\n",
    "from sample_module import clean_text\n",
    "\n",
    "# def test_csv_processing(tmp_path):\n",
    "#     # 1. Create CSV\n",
    "#     csv_path = tmp_path / \"data.csv\"\n",
    "#     csv_path.write_text(\"text\" + chr(10) + \"  hello   world  \" + chr(10) + \"  foo   bar  \" + chr(10))\n",
    "#     # 2. Read with pandas\n",
    "#     df = pd.read_csv(csv_path)\n",
    "#     # 3. Apply clean_text\n",
    "#     df[\"cleaned\"] = df[\"text\"].apply(clean_text)\n",
    "#     # 4. Assert\n",
    "#     assert df[\"cleaned\"].tolist() == [\"hello world\", \"foo bar\"]\n",
    "'''\n",
    "\n",
    "# TEST — Do not modify\n",
    "rc = run_pytest(test_file)\n",
    "assert rc == 0, \"Tests should pass\"\n",
    "print(\"Task 2.3 passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcd1227",
   "metadata": {},
   "source": [
    "## Task 2.4: Fixture with Yield (Setup + Teardown)\n",
    "\n",
    "Write a fixture that:\n",
    "1. Creates a trained ML pipeline (setup)\n",
    "2. Yields it\n",
    "3. Logs \"pipeline cleaned up\" to a list (teardown)\n",
    "\n",
    "Then write a test using this fixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eb961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "test_yield = f'''\n",
    "import sys, pytest, numpy as np\n",
    "sys.path.insert(0, \"{FIXTURES}\")\n",
    "from sample_ml_pipeline import create_pipeline, train_pipeline, predict, SAMPLE_TEXTS, SAMPLE_LABELS\n",
    "\n",
    "cleanup_log = []\n",
    "\n",
    "# @pytest.fixture\n",
    "# def trained_pipe():\n",
    "#     np.random.seed(42)\n",
    "#     pipe = create_pipeline(max_features=100)\n",
    "#     train_pipeline(pipe, SAMPLE_TEXTS, SAMPLE_LABELS)\n",
    "#     yield pipe\n",
    "#     cleanup_log.append(\"pipeline cleaned up\")\n",
    "\n",
    "# def test_predict(trained_pipe):\n",
    "#     preds = predict(trained_pipe, [\"great product\"])\n",
    "#     assert len(preds) == 1\n",
    "\n",
    "# def test_cleanup_happened():\n",
    "#     assert \"pipeline cleaned up\" in cleanup_log\n",
    "'''\n",
    "\n",
    "# TEST — Do not modify\n",
    "rc = run_pytest(test_yield)\n",
    "assert rc == 0, \"Tests should pass\"\n",
    "print(\"Task 2.4 passed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
