{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01918a87",
   "metadata": {},
   "source": [
    "# Solution â€” Task 02: Fixtures & Parametrize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef840cd7",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63003d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys, os, textwrap, tempfile, pathlib\n",
    "\n",
    "FIXTURES = os.path.abspath(os.path.join(\"..\", \"fixtures\", \"input\"))\n",
    "if not os.path.exists(FIXTURES):\n",
    "    FIXTURES = os.path.abspath(os.path.join(\"fixtures\", \"input\"))\n",
    "sys.path.insert(0, FIXTURES)\n",
    "\n",
    "def run_pytest(test_code: str, extra_files: dict[str, str] | None = None, extra_args: list[str] | None = None):\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        td_path = pathlib.Path(td)\n",
    "        p = td_path / \"test_tmp.py\"\n",
    "        p.write_text(textwrap.dedent(test_code))\n",
    "        if extra_files:\n",
    "            for name, content in extra_files.items():\n",
    "                (td_path / name).write_text(textwrap.dedent(content))\n",
    "        cmd = [sys.executable, \"-m\", \"pytest\", str(td), \"-v\", \"--tb=short\", \"--no-header\"]\n",
    "        if extra_args:\n",
    "            cmd.extend(extra_args)\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)\n",
    "        print(result.stdout + result.stderr)\n",
    "        return result.returncode\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b2af70",
   "metadata": {},
   "source": [
    "## Solution 2.1: Fixtures in conftest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5791df",
   "metadata": {},
   "outputs": [],
   "source": [
    "conftest_code = f'''\n",
    "import sys, pytest\n",
    "sys.path.insert(0, \"{FIXTURES}\")\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_texts():\n",
    "    return [\n",
    "        \"Hello world\",\n",
    "        \"  extra   spaces  \",\n",
    "        \"UPPERCASE TEXT\",\n",
    "        \"hello\\\\tworld\\\\nfoo\",\n",
    "        \"simple\",\n",
    "    ]\n",
    "\n",
    "@pytest.fixture\n",
    "def empty_text():\n",
    "    return \"\"\n",
    "\n",
    "@pytest.fixture\n",
    "def text_with_emails():\n",
    "    return \"Contact alice@example.com and bob@test.org for info.\"\n",
    "'''\n",
    "\n",
    "test_code = f'''\n",
    "import sys\n",
    "sys.path.insert(0, \"{FIXTURES}\")\n",
    "from sample_module import tokenize, extract_emails, clean_text\n",
    "\n",
    "def test_tokenize_samples(sample_texts):\n",
    "    for text in sample_texts:\n",
    "        tokens = tokenize(text)\n",
    "        assert isinstance(tokens, list)\n",
    "        if text.strip():\n",
    "            assert len(tokens) > 0\n",
    "\n",
    "def test_clean_empty(empty_text):\n",
    "    assert clean_text(empty_text) == \"\"\n",
    "\n",
    "def test_extract_from_email_text(text_with_emails):\n",
    "    emails = extract_emails(text_with_emails)\n",
    "    assert len(emails) >= 2\n",
    "    assert \"alice@example.com\" in emails\n",
    "    assert \"bob@test.org\" in emails\n",
    "'''\n",
    "\n",
    "rc = run_pytest(test_code, extra_files={\"conftest.py\": conftest_code})\n",
    "assert rc == 0, \"Tests should pass\"\n",
    "print(\"Task 2.1 passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6914a872",
   "metadata": {},
   "source": [
    "## Solution 2.2: Parametrize ML Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccfaf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preprocess = f'''\n",
    "import sys, pytest\n",
    "sys.path.insert(0, \"{FIXTURES}\")\n",
    "from sample_ml_pipeline import preprocess_texts\n",
    "\n",
    "@pytest.mark.parametrize(\"texts, expected\", [\n",
    "    ([\"Hello World\"], [\"hello world\"]),\n",
    "    ([\"Check https://example.com out\"], [\"check out\"]),\n",
    "    ([\"  extra   spaces  \"], [\"extra spaces\"]),\n",
    "    ([\"UPPER case Text\"], [\"upper case text\"]),\n",
    "    ([\"no change\"], [\"no change\"]),\n",
    "])\n",
    "def test_preprocess(texts, expected):\n",
    "    assert preprocess_texts(texts) == expected\n",
    "\n",
    "def test_preprocess_multiple():\n",
    "    texts = [\"Hello\", \"  World  \"]\n",
    "    result = preprocess_texts(texts)\n",
    "    assert result == [\"hello\", \"world\"]\n",
    "'''\n",
    "\n",
    "rc = run_pytest(test_preprocess)\n",
    "assert rc == 0, \"Tests should pass\"\n",
    "print(\"Task 2.2 passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd059170",
   "metadata": {},
   "source": [
    "## Solution 2.3: `tmp_path` for File-Based Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fdb25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = f'''\n",
    "import sys\n",
    "sys.path.insert(0, \"{FIXTURES}\")\n",
    "import pandas as pd\n",
    "from sample_module import clean_text\n",
    "\n",
    "def test_csv_processing(tmp_path):\n",
    "    csv_path = tmp_path / \"data.csv\"\n",
    "    csv_path.write_text(\"text\" + chr(10) + \"  hello   world  \" + chr(10) + \"  foo   bar  \" + chr(10))\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"cleaned\"] = df[\"text\"].apply(clean_text)\n",
    "    assert df[\"cleaned\"].tolist() == [\"hello world\", \"foo bar\"]\n",
    "\n",
    "def test_json_roundtrip(tmp_path):\n",
    "    import json\n",
    "    data = {{\"texts\": [\"hello\", \"world\"], \"labels\": [1, 0]}}\n",
    "    json_path = tmp_path / \"data.json\"\n",
    "    json_path.write_text(json.dumps(data))\n",
    "    loaded = json.loads(json_path.read_text())\n",
    "    assert loaded == data\n",
    "'''\n",
    "\n",
    "rc = run_pytest(test_file)\n",
    "assert rc == 0, \"Tests should pass\"\n",
    "print(\"Task 2.3 passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77495ab7",
   "metadata": {},
   "source": [
    "## Solution 2.4: Fixture with Yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21834073",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_yield = f'''\n",
    "import sys, pytest, numpy as np\n",
    "sys.path.insert(0, \"{FIXTURES}\")\n",
    "from sample_ml_pipeline import create_pipeline, train_pipeline, predict, SAMPLE_TEXTS, SAMPLE_LABELS\n",
    "\n",
    "cleanup_log = []\n",
    "\n",
    "@pytest.fixture\n",
    "def trained_pipe():\n",
    "    np.random.seed(42)\n",
    "    pipe = create_pipeline(max_features=100)\n",
    "    train_pipeline(pipe, SAMPLE_TEXTS, SAMPLE_LABELS)\n",
    "    yield pipe\n",
    "    cleanup_log.append(\"pipeline cleaned up\")\n",
    "\n",
    "def test_predict(trained_pipe):\n",
    "    preds = predict(trained_pipe, [\"great product\"])\n",
    "    assert len(preds) == 1\n",
    "\n",
    "def test_cleanup_happened():\n",
    "    assert \"pipeline cleaned up\" in cleanup_log\n",
    "'''\n",
    "\n",
    "rc = run_pytest(test_yield)\n",
    "assert rc == 0, \"Tests should pass\"\n",
    "print(\"Task 2.4 passed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
